{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#__ANM-NCPOP-Templete__\n",
        "\n",
        "Welcome to the repository for testing ANM-NCPOP! It focuses on Causal Model Discovery Problems in Learning Joint Multiple Dynamical Systems via Non-Commutative Polynomial Optimization(NCPOP).\n",
        "\n",
        "# **ANM-NCPOP Process**\n",
        "Define a function for solving the NCPO problems with given standard deviations of process noise and observtion noise, length of  estimation data and required relaxation level.\n",
        "\n",
        "* Generate the artificial data or standardize observational data.\n",
        "* Learn the Causal Structure beneath the observation data.\n",
        "* Visualize the comparison of estimated/true graphs using a heatmap.\n",
        "* Calculate Metrics.\n",
        "\n",
        "Please follow the instructions for modelling ANM-NCPOP causality discovery\n",
        "tool for both real data input and synthetic data generation.\n",
        "\n",
        "## Synthetic_Data_Generation\n",
        "* Weighted random DAG is produced according to num_nodes and num_edges. Test raw datasets is the time series generating from weighted random DAG and SEM type.\n",
        "\n",
        "## Real_Data_Standardization\n",
        "* For multiple features time series data, all time series are saved as a (Feature_num, Sample_num, Time) three dimensions array.\n",
        "\n",
        "# **Output Data Form**\n",
        "* Causality Data can be stored as NumPy array x(F features, S smples and T timesets) and matrix y(F features, F features) under DATASETS.\n",
        "\n",
        "* Metrics and heatmaps are saved under SUMMARY.\n",
        "\n",
        "# **File_Save_Flow**\n",
        "       \n",
        "                                                -- Datasets\n",
        "                                               |\n",
        "                                               |            \n",
        "                          Save_Path -- Results  -- Summary  -- Summary_Details\n",
        "                                  |            |           |\n",
        "                                  |            |            -- MetricsDAG\n",
        "                                  |             -- Heatmap -- Summary_Table\n",
        "                                  |\n",
        "                                    -- Input_Data"
      ],
      "metadata": {
        "id": "kq1OSo5X0whe"
      },
      "id": "kq1OSo5X0whe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Step 1: Get start__"
      ],
      "metadata": {
        "id": "3dlj27Nr5HbP"
      },
      "id": "3dlj27Nr5HbP"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/NCPOP/Causal_Models_Learning/Test/\")\n",
        "# os.chdir(\"/content/drive/MyDrive/Colab Notebooks/NCPOP/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxG01ITDDrxJ",
        "outputId": "b8ac7430-1860-46a3-e5e7-338b86d1ef61"
      },
      "id": "QxG01ITDDrxJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTHONPATH=\"$PYTHONPATH:/content/drive/MyDrive/Colab Notebooks/NCPOP-Colab Notebooks-data\"\n",
        "import os\n",
        "os.environ['MOSEKLM_LICENSE_FILE']=\"/content/drive/MyDrive/Colab Notebooks/NCPOP/\""
      ],
      "metadata": {
        "id": "YvsgA49ewu23"
      },
      "id": "YvsgA49ewu23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a349920",
        "outputId": "74285e07-fedd-42e1-b01e-f745446f59c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mosek\n",
            "  Downloading Mosek-10.2.1-cp37-abi3-manylinux2014_x86_64.whl (15.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mosek) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mosek, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed mosek-10.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting ncpol2sdpa\n",
            "  Downloading ncpol2sdpa-1.12.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sympy>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from ncpol2sdpa) (1.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ncpol2sdpa) (1.25.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=0.7.2->ncpol2sdpa) (1.3.0)\n",
            "Building wheels for collected packages: ncpol2sdpa\n",
            "  Building wheel for ncpol2sdpa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ncpol2sdpa: filename=ncpol2sdpa-1.12.2-py3-none-any.whl size=70795 sha256=10d6308011b83bbe8497a95c799d5db818ac0dc36170203b4c6dbd5cfc9cc38b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/d7/fe/ab61f3bf30a350feab3bb4dccd63932d56cbbd32b9ec0d94fa\n",
            "Successfully built ncpol2sdpa\n",
            "Installing collected packages: ncpol2sdpa\n",
            "Successfully installed ncpol2sdpa-1.12.2\n",
            "Collecting gcastle==1.0.3rc2\n",
            "  Downloading gcastle-1.0.3rc2-py3-none-any.whl (187 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (3.3)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->gcastle==1.0.3rc2) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->gcastle==1.0.3rc2) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.1->gcastle==1.0.3rc2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.1->gcastle==1.0.3rc2) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.16.0)\n",
            "Installing collected packages: gcastle\n",
            "Successfully installed gcastle-1.0.3rc2\n"
          ]
        }
      ],
      "source": [
        "### for colab ###\n",
        "# To execute the notebook directly in colab make sure your MOSEK license file is in one the locations\n",
        "#\n",
        "# /content/mosek.lic   or   /root/mosek/mosek.lic\n",
        "#\n",
        "# inside this notebook's internal filesystem.\n",
        "# Install MOSEK and ncpol2sdpa if not already installed\n",
        "!pip install mosek torch\n",
        "!pip install ncpol2sdpa\n",
        "# !pip install networkx\n",
        "!pip install gcastle==1.0.3rc2"
      ],
      "id": "8a349920"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 1.1: Real_Data_Standardization__"
      ],
      "metadata": {
        "id": "ZEPql9cD8h19"
      },
      "id": "ZEPql9cD8h19"
    },
    {
      "cell_type": "code",
      "source": [
        "# from Data_Standardization import*\n",
        "from pickle import TRUE\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "from itertools import combinations\n",
        "import urllib\n",
        "import hashlib\n",
        "from urllib.error import URLError\n",
        "USER_AGENT = \"gcastle/dataset\"\n",
        "\n",
        "def _check_exist(root, filename, files):\n",
        "    path_exist = os.path.join(root, filename.split('.')[0])\n",
        "    processed_folder_exists = os.path.exists(path_exist)\n",
        "    if not processed_folder_exists:\n",
        "        return False\n",
        "\n",
        "    return all(\n",
        "        _check_integrity(os.path.join(path_exist, file)) for file in files\n",
        "    )\n",
        "\n",
        "def _read_data(root, filename, files):\n",
        "    path_exist = os.path.join(root, filename.split('.')[0])\n",
        "\n",
        "    result = []\n",
        "    for file in files:\n",
        "        if file.split('.')[-1] == 'csv':\n",
        "            file_path = os.path.join(path_exist, file)\n",
        "            result.append(pd.read_csv(file_path))\n",
        "        elif file.split('.')[-1] == 'npy':\n",
        "            file_path = os.path.join(path_exist, file)\n",
        "            result.append(np.load(file_path))\n",
        "\n",
        "    if len(result) == 2:\n",
        "        result.append(None)\n",
        "\n",
        "    return result\n",
        "\n",
        "def _check_integrity(fpath, md5=None):\n",
        "    if not os.path.isfile(fpath):\n",
        "        return False\n",
        "    if md5 is None:\n",
        "        return True\n",
        "\n",
        "    md5f = hashlib.md5()\n",
        "    with open(fpath, 'rb') as f:\n",
        "        md5f.update(f.read())\n",
        "\n",
        "    return md5 == md5f.hexdigest()\n",
        "\n",
        "\n",
        "def _download(root, url, filename, md5):\n",
        "    \"\"\"Download the datasets if it doesn't exist already.\"\"\"\n",
        "\n",
        "    os.makedirs(root, exist_ok=True)\n",
        "\n",
        "    # download files\n",
        "    for mirror in url:\n",
        "        filepath = \"{}{}\".format(mirror, filename)\n",
        "        savegz = os.path.join(root, filename)\n",
        "        try:\n",
        "            print(\"Downloading {}\".format(filepath))\n",
        "            response = urllib.request.urlopen( \\\n",
        "                urllib.request.Request( \\\n",
        "                    filepath, headers={\"User-Agent\": USER_AGENT}))\n",
        "            with open(savegz, \"wb\") as fh:\n",
        "                fh.write(response.read())\n",
        "\n",
        "            tar = tarfile.open(savegz)\n",
        "            names = tar.getnames()\n",
        "            for name in names:\n",
        "                tar.extract(name, path=root)\n",
        "            tar.close()\n",
        "        except URLError as error:\n",
        "            print(\"Failed to download (trying next):\\n{}\".format(error))\n",
        "            continue\n",
        "        break\n",
        "    else:\n",
        "        raise RuntimeError(\"Error downloading {}\".format(filename))\n",
        "\n",
        "    # check integrity of downloaded file\n",
        "    if not _check_integrity(savegz, md5):\n",
        "        raise RuntimeError(\"File not found or corrupted.\")\n",
        "\n",
        "def load_dataset(name='IID_Test', root=None, download=False):\n",
        "    \"\"\"\n",
        "    A function for loading some well-known datasets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    name: class, default='IID_Test'\n",
        "        Dataset name, independent and identically distributed (IID),\n",
        "        Topological Hawkes Process (THP) and real datasets.\n",
        "    root: str\n",
        "        Root directory in which the dataset will be saved.\n",
        "    download: bool\n",
        "        If true, downloads the dataset from the internet and\n",
        "        puts it in root directory. If dataset is already downloaded, it is not\n",
        "        downloaded again.\n",
        "\n",
        "    Return\n",
        "    ------\n",
        "    out: tuple\n",
        "        true_graph_matrix: numpy.matrix\n",
        "            adjacency matrix for the target causal graph.\n",
        "        topology_matrix: numpy.matrix\n",
        "            adjacency matrix for the topology.\n",
        "        data: pandas.core.frame.DataFrame\n",
        "            standard trainning dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    if name not in DataSetRegistry.meta.keys():\n",
        "        raise ValueError('The dataset {} has not been registered, you can use'\n",
        "                         ' ''castle.datasets.__builtin_dataset__'' to get registered '\n",
        "                         'dataset list'.format(name))\n",
        "    loader = DataSetRegistry.meta.get(name)()\n",
        "    loader.load(root, download)\n",
        "    return loader.data, loader.true_graph_matrix, loader.topology_matrix\n",
        "\n",
        "\n",
        "class BuiltinDataSet(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self._data = None\n",
        "        self._true_graph_matrix = None\n",
        "        self._topology_matrix = None\n",
        "\n",
        "    def load(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "        return self._data\n",
        "\n",
        "    @property\n",
        "    def true_graph_matrix(self):\n",
        "        return self._true_graph_matrix\n",
        "\n",
        "    @property\n",
        "    def topology_matrix(self):\n",
        "        return self._topology_matrix\n",
        "\n",
        "class RealDataSet(BuiltinDataSet):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.url = None\n",
        "        self.tar_file = None\n",
        "        self.md5 = None\n",
        "        self.file_list = None\n",
        "\n",
        "    def load(self, root=None, download=False):\n",
        "\n",
        "        if root is None:\n",
        "            root = './'\n",
        "\n",
        "        if _check_exist(root, self.tar_file, self.file_list):\n",
        "            self._data, self._true_graph_matrix, self._topology_matrix = \\\n",
        "                _read_data(root, self.tar_file, self.file_list)\n",
        "            return\n",
        "\n",
        "        if download:\n",
        "            _download(root, self.url, self.tar_file, self.md5)\n",
        "\n",
        "        if not _check_exist(root, self.tar_file, self.file_list):\n",
        "            raise RuntimeError('Dataset not found.' +\n",
        "                               ' You can use download=True to download it.')\n",
        "\n",
        "        self._data, self._true_graph_matrix, self._topology_matrix = \\\n",
        "            _read_data(root, self.tar_file, self.file_list)\n",
        "\n",
        "\n",
        "class V18_N55_Wireless(RealDataSet):\n",
        "    \"\"\"\n",
        "    A function for loading the real dataset: V18_N55_Wireless\n",
        "    url: https://raw.githubusercontent.com/gcastle-hub/dataset/master/alarm/18V_55N_Wireless.tar.gz\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.url = ['https://raw.githubusercontent.com/gcastle-hub/dataset/master/alarm/']\n",
        "        self.tar_file = \"18V_55N_Wireless.tar.gz\"\n",
        "        self.md5 = \"36ee135b86c8dbe09668d9284c23575b\"\n",
        "        self.file_list = ['Alarm.csv', 'DAG.npy']\n",
        "\n",
        "\n",
        "class V24_N439_Microwave(RealDataSet):\n",
        "    \"\"\"\n",
        "    A function for loading the real dataset: V24_N439_Microwave\n",
        "    url: https://raw.githubusercontent.com/gcastle-hub/dataset/master/alarm/24V_439N_Microwave.tar.gz\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.url = ['https://raw.githubusercontent.com/gcastle-hub/dataset/master/alarm/']\n",
        "        self.tar_file = \"24V_439N_Microwave.tar.gz\"\n",
        "        self.md5 = \"b4c8b32d34c04a86aa93c7259f7d086c\"\n",
        "        self.file_list = ['Alarm.csv', 'DAG.npy', 'Topology.npy']\n",
        "\n",
        "\n",
        "class V25_N474_Microwave(RealDataSet):\n",
        "    \"\"\"\n",
        "    A function for loading the real dataset: V25_N474_Microwave\n",
        "    url: https://raw.githubusercontent.com/gcastle-hub/dataset/master/alarm/25V_474N_Microwave.tar.gz\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.url = ['https://raw.githubusercontent.com/gcastle-hub/dataset/master/alarm/']\n",
        "        self.tar_file = \"25V_474N_Microwave.tar.gz\"\n",
        "        self.md5 = \"51f43ed622d4b44ef6daf8fabf81e162\"\n",
        "        self.file_list = ['Alarm.csv', 'DAG.npy', 'Topology.npy']\n",
        "\n",
        "\n",
        "class DataSetRegistry(object):\n",
        "    '''\n",
        "    A class for resgistering the datasets, in which each dataset\n",
        "    can be loaded by 'load_dataset' api.\n",
        "    '''\n",
        "\n",
        "    meta = {'V18_N55_Wireless': V18_N55_Wireless,\n",
        "            'V24_N439_Microwave': V24_N439_Microwave,\n",
        "            'V25_N474_Microwave': V25_N474_Microwave}\n",
        "\n",
        "\n",
        "\n",
        "class Real_Data_Standardization(object):\n",
        "    '''\n",
        "    A class for preparing data to simulate random (causal) DAG.\n",
        "\n",
        "    Parameters\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    File_PATH: str\n",
        "               Read file path\n",
        "    File_NAME: str\n",
        "               Read data name\n",
        "\n",
        "    Returns\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    Input data: npz\n",
        "            Raw_data：[d, n, T] sample time series\n",
        "            true_dag: true_causal_matrix\n",
        "    File_PATH_Datasets:\n",
        "            Route of saving test data\n",
        "\n",
        "    Examples\n",
        "    -------------------------------------------------------------------------------------------------\n",
        "    >>> File_PATH = \"../Test/Examples/Test_data/\"\n",
        "    >>> file_name = 'Telephone'\n",
        "    >>> dt = Real_Data_Standardization(File_PATH, file_name)\n",
        "    >>> dt.standardize_data()\n",
        "\n",
        "    >>> File_PATH = \"../Test/Datasets/Synthetic datasets/Krebs_Cycle/\"\n",
        "    >>> file_name = 'Krebs_Cycle'\n",
        "    >>> dt = Real_Data_Standardization(File_PATH, file_name)\n",
        "    >>> dt.standardize_data()\n",
        "\n",
        "    >>> File_PATH = \"../Test/Datasets/Real_data/Microwave/\"\n",
        "    >>> file_name = 'V24_N439_Microwave'\n",
        "    >>> dt = Real_Data_Standardization(File_PATH, file_name)\n",
        "    >>> dt.standardize_data()\n",
        "    '''\n",
        "\n",
        "    def __init__(self, File_PATH='Kreb_Cycles/', filename='Krebs_Cycle'):\n",
        "        self.File_PATH = File_PATH\n",
        "        self.filename = filename\n",
        "\n",
        "    def standardize_data(self):\n",
        "        ################################################  Create Ground Tier Folders #############################################\n",
        "        self.File_PATH_Base = self.File_PATH +'Result_'+ self.filename +'/'\n",
        "\n",
        "        ################################################  Create First Tier Folders #############################################\n",
        "        self.File_PATH_Datasets = self.File_PATH_Base + 'Datasets_'+ self.filename +'/'\n",
        "        if not os.path.exists(self.File_PATH_Datasets):\n",
        "            os.makedirs(self.File_PATH_Datasets)\n",
        "        print('ANM-NCPOP INFO: Created Datasets' + ' File!')\n",
        "\n",
        "        Raw_data = Real_Data_Standardization.Produce_Rawdata(self)[0]\n",
        "        true_dag = Real_Data_Standardization.Produce_Rawdata(self)[1]\n",
        "\n",
        "        # save numpy to npz file\n",
        "        nn = len(true_dag)\n",
        "        ne = np.count_nonzero(true_dag)\n",
        "        data_name = self.filename  +'_'+str(nn)+'Nodes_'+str(ne)+'Edges_TS'\n",
        "        if self.filename in ['IID_Test','THP_Test','V18_N55_Wireless', 'V24_N439_Microwave', 'V25_N474_Microwave']:\n",
        "            topology_matrix_devices = Real_Data_Standardization.Produce_Rawdata(self)[2]\n",
        "            np.savez(self.File_PATH_Datasets + data_name +'.npz', x=Raw_data , y=true_dag , z=topology_matrix_devices)\n",
        "        else:\n",
        "            np.savez(self.File_PATH_Datasets + data_name +'.npz', x=Raw_data , y=true_dag)\n",
        "        print('ANM-NCPOP INFO: Finished '+ data_name+' dataset standardization!')\n",
        "\n",
        "    @staticmethod\n",
        "    def Produce_Rawdata(self):\n",
        "\n",
        "        def readable_File(FilePATH):\n",
        "            read_Dir=os.listdir(FilePATH)\n",
        "            count = 0\n",
        "            readable_F = []\n",
        "            for f in read_Dir:\n",
        "                file = os.path.join(FilePATH, f)\n",
        "                if os.path.isdir(file):\n",
        "                    count = count+1\n",
        "                else:\n",
        "                    readable_F.append(f)\n",
        "            return count,readable_F\n",
        "\n",
        "        self.Read_File = readable_File(self.File_PATH)[1]\n",
        "\n",
        "        # Website data\n",
        "        if self.filename in ['IID_Test','THP_Test','V18_N55_Wireless', 'V24_N439_Microwave', 'V25_N474_Microwave']:\n",
        "            Raw_data, true_dag, topology_matrix_devices  = load_dataset(self.filename, download=True)\n",
        "            return Raw_data, true_dag, topology_matrix_devices\n",
        "\n",
        "        else:\n",
        "            # Check empty files under riute\n",
        "            if len(self.Read_File ) == 0:\n",
        "                raise ValueError('No Data Under the Current Route!')\n",
        "            else:\n",
        "                self.File_PATH_TS = self.File_PATH +self.filename +'_TS/'\n",
        "                File_NAME = []\n",
        "                File_TYPE = []\n",
        "                # Delete files and list readable Files\n",
        "                for i in self.Read_File:\n",
        "                    File_NAME.append(re.split(\"\\.\", i)[0])\n",
        "                    File_TYPE.append(re.split(\"\\.\", i)[1])\n",
        "\n",
        "                ###################################### Deal with Two Dimensions Causality Data ###################################\n",
        "                if self.filename+'.npz' in self.Read_File:\n",
        "                    Test_data = np.load(self.File_PATH + self.filename+'.npz', allow_pickle=True)\n",
        "                    Raw_data = Test_data['x']\n",
        "                    true_dag = Test_data['y']\n",
        "                    return Raw_data, true_dag\n",
        "\n",
        "                elif self.filename+'.tar.gz' in self.Read_File:\n",
        "                    # open file\n",
        "                    file = tarfile.open(self.File_PATH + self.filename + '.tar.gz')\n",
        "                    file_names = file.getnames()\n",
        "                    # extract files\n",
        "                    file.extractall(self.File_PATH)\n",
        "                    file.close()\n",
        "                    Raw_data = np.load(self.File_PATH+file_names[2])\n",
        "                    true_dag = pd.read_csv(self.File_PATH+file_names[3])\n",
        "                    return Raw_data, true_dag\n",
        "\n",
        "                elif self.filename+'.csv' in self.Read_File:\n",
        "                    Raw_data = pd.read_csv(self.File_PATH+ self.filename+'.csv', header=0, index_col=False)\n",
        "                    true_dag = pd.read_csv(self.File_PATH+'true_graph.csv', header=0, index_col=0)\n",
        "                    return Raw_data, true_dag\n",
        "\n",
        "                ################################ Deal with Multi-dimensions Causality Data ###################################\n",
        "                elif os.path.exists(self.File_PATH_TS):\n",
        "                    read_Dir_TS=os.listdir(self.File_PATH_TS)\n",
        "                    true_graph = np.load(self.File_PATH+'true_graph.npz')\n",
        "\n",
        "                    # labels = [\"FUMARATE\", \"GTP\", \"H2O\", \"CIS-ACONITATE\", \"MALATE\",\n",
        "                    # \"OXALOACETATE\", \"FAD\", \"SUCCINYL-COA\", \"NAD\",\n",
        "                    #           \"A-K-GLUTARATE\", \"GDP\", \"NADH\", \"CITRATE\", \"SUCCINATE\",\n",
        "                    # \"ISOCITRATE\", \"ACETY-COA\"]\n",
        "                    #true_dag = pd.DataFrame(true_graph['arr_0'],  index=labels, columns=labels)\n",
        "                    true_dag = pd.DataFrame(true_graph['arr_0'])\n",
        "\n",
        "                    # print(true_dag)\n",
        "                    lds = pd.read_csv(self.File_PATH_TS+ read_Dir_TS[0], delimiter='\\t', index_col=0, header=None)\n",
        "                    feature_name = np.array(lds.index)\n",
        "                    feature_num = len(feature_name)\n",
        "                    sample_num = len(read_Dir_TS)\n",
        "                    T_num = lds.shape[1]\n",
        "                    # if labels == feature_name:\n",
        "                    Raw_data = np.zeros((feature_num, sample_num, T_num))\n",
        "                    for ns in range(sample_num):\n",
        "                        X = pd.read_csv(self.File_PATH_TS+ read_Dir_TS[ns], delimiter='\\t', index_col=0, header=None)\n",
        "                        X_trans = np.transpose(X)\n",
        "                        for fn in range(feature_num):\n",
        "                            Raw_data[fn, ns, :] = list(X_trans[feature_name[fn]])\n",
        "                    return Raw_data, true_dag\n",
        "\n",
        "                else:\n",
        "                    raise ValueError('Unknown input data type.')\n"
      ],
      "metadata": {
        "id": "qBZT-6tB8ewP"
      },
      "id": "qBZT-6tB8ewP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 1.2: Synthetic_Data_Generation__"
      ],
      "metadata": {
        "id": "cfrMd1H28lrL"
      },
      "id": "cfrMd1H28lrL"
    },
    {
      "cell_type": "code",
      "source": [
        "# from Generate_SyntheticData import*\n",
        "from networkx.algorithms import bipartite\n",
        "from scipy.special import expit as sigmoid\n",
        "from itertools import combinations\n",
        "from pickle import TRUE\n",
        "from random import sample\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import logging\n",
        "import tarfile\n",
        "import os\n",
        "import re\n",
        "# import time\n",
        "import random\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "class Generate_Synthetic_Data(object):\n",
        "    '''\n",
        "    Simulate IID datasets for causal structure learning.\n",
        "\n",
        "    Parameters\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    File_PATH\n",
        "            Read data path\n",
        "    n: int\n",
        "            Number of samples for standard trainning dataset.\n",
        "    T: int\n",
        "            Number of timeseries for standard trainning dataset.\n",
        "    method: str, (linear or nonlinear), default='linear'\n",
        "            Distribution for standard trainning dataset.\n",
        "    sem_type: str\n",
        "            gauss, exp, gumbel, uniform, logistic (linear);\n",
        "            mlp, mim, gp, gp-add, quadratic (nonlinear).\n",
        "    nodes: series\n",
        "            Notes of samples for standard trainning dataset.\n",
        "    edges: series\n",
        "            Edges of samples for standard training dataset.\n",
        "    noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "    Returns\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    Raw_data: npz\n",
        "            x：[d, n, T] sample time series\n",
        "            y: true_dag\n",
        "    File_PATH_Datasets:\n",
        "            Route of saving test data\n",
        "\n",
        "    Examples 1\n",
        "    -------------------------------------------------------------------------------------------------\n",
        "    >>> method = 'linear'\n",
        "    >>> sem_type = 'gauss'\n",
        "    >>> nodes = range(6,12,3)\n",
        "    >>> edges = range(10,20,5)\n",
        "    >>> T=200\n",
        "    >>> num_datasets = 120\n",
        "    >>> File_PATH = '../Test/Examples/Test_data/'\n",
        "    >>> noise_scale = 1.0\n",
        "    >>> _ts = Generate_Synthetic_Data(File_PATH, num_datasets, T, method, sem_type, nodes, edges, noise_scale)\n",
        "    >>> _ts.genarate_data()\n",
        "\n",
        "    Examples 2\n",
        "    -------------------------------------------------------------------------------------------------\n",
        "    >>> noise_type = {\n",
        "    >>>     'nonlinear': ['gp-add', 'mlp', 'mim', 'gp', 'quadratic'],\n",
        "    >>>     'linear':  ['gauss', 'exp', 'gumbel', 'uniform', 'logistic']\n",
        "    >>> }\n",
        "    >>> sem_type = ['linear', 'nonlinear']\n",
        "    >>> nodes = range(6,12,3)\n",
        "    >>> edges = range(10,20,5)\n",
        "    >>> T=200\n",
        "    >>> num_datasets = 120\n",
        "    >>> File_PATH = '../Test/Examples/Test_data/'\n",
        "    >>> noise_scale = 1.0\n",
        "\n",
        "    >>> for m in sem_type :\n",
        "    >>>   for s in noise_type[m]:\n",
        "    >>>     _ts = Generate_Synthetic_Data(File_PATH, num_datasets, T, m, s, nodes, edges, noise_scale)\n",
        "    >>>     print(File_PATH, num_datasets, T, m, s, nodes, edges, noise_scale)\n",
        "    >>>     _ts.genarate_data()\n",
        "    '''\n",
        "\n",
        "    def __init__(self, File_PATH, n, T, method, sem_type, nodes, edges, noise_scale):\n",
        "        self.File_PATH = File_PATH\n",
        "        self.n = n\n",
        "        self.T = T\n",
        "        self.method = method\n",
        "        self.sem_type =sem_type\n",
        "        self.filename = self.method.capitalize()+'SEM_' + self.sem_type.capitalize() +'Noise'\n",
        "        self.nodes = nodes\n",
        "        self.edges = edges\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def genarate_data(self):\n",
        "        ################################################  Create Ground Tier Folders #############################################\n",
        "        self.File_PATH_Base = self.File_PATH +'Result_'+ self.filename +'/'\n",
        "\n",
        "        ################################################  Create First Tier Folders #############################################\n",
        "        self.File_PATH_Datasets = self.File_PATH_Base + 'Datasets_'+ self.filename +'/'\n",
        "        if not os.path.exists(self.File_PATH_Datasets):\n",
        "            os.makedirs(self.File_PATH_Datasets)\n",
        "        print('ANM-NCPOP INFO: Created Datasets' + ' File!')\n",
        "\n",
        "        nodes_num = len(self.nodes)\n",
        "        edges_num = len(self.edges)\n",
        "        count = 0\n",
        "        tqdm_csv=os.listdir(self.File_PATH_Datasets)\n",
        "        while len(tqdm_csv) < nodes_num* edges_num:\n",
        "            print('ANM-NCPOP INFO: Generating '+ self.filename + ' Dataset!')\n",
        "            if self.method == 'linear':\n",
        "                for nn in nodes:\n",
        "                    for ne in edges:\n",
        "                        count += 1\n",
        "                        w = DAG.erdos_renyi(n_nodes=nn, n_edges=ne, seed=1)\n",
        "                        self.B = (w != 0).astype(int)\n",
        "                        self.XX = Generate_Synthetic_Data._simulate_linear_sem(self.B, self.n, self.T, self.sem_type, self.noise_scale)\n",
        "                        data_name = self.filename+'_'+str(nn)+'Nodes_'+str(ne)+'Edges_TS'\n",
        "                        np.savez(self.File_PATH_Datasets +data_name+'.npz', x=self.XX , y=self.B)\n",
        "                        logging.info('ANM-NCPOP INFO: Finished synthetic dataset')\n",
        "                        print('ANM-NCPOP INFO: '+ data_name + ' IS DONE!')\n",
        "                print('ANM-NCPOP INFO: '+ str(count) + ' datasets are generated!')\n",
        "                break\n",
        "            elif self.method == 'nonlinear':\n",
        "                for nn in nodes:\n",
        "                    for ne in edges:\n",
        "                        count += 1\n",
        "                        w = DAG.erdos_renyi(n_nodes=nn, n_edges=ne, seed=1)\n",
        "                        self.B = (w != 0).astype(int)\n",
        "                        self.XX = Generate_Synthetic_Data._simulate_nonlinear_sem(self.B, self.n, self.T, self.sem_type, self.noise_scale)\n",
        "                        data_name = self.filename+'_'+str(nn)+'Nodes_'+str(ne)+'Edges_TS'\n",
        "                        np.savez(self.File_PATH_Datasets +data_name+'.npz', x=self.XX , y=self.B)\n",
        "                        logging.info('ANM-NCPOP INFO: Finished synthetic dataset')\n",
        "                        print('ANM-NCPOP INFO: '+ data_name + ' IS DONE!')\n",
        "                print('ANM-NCPOP INFO: '+ str(count) + ' datasets are generated!')\n",
        "                break\n",
        "            else:\n",
        "                raise ValueError('Unknown distribution type. Only linear and nonlinear types are accepted.')\n",
        "\n",
        "            # time.sleep(30)\n",
        "        print('ANM-NCPOP INFO: Finished '+ self.filename +' dataset generation, which can be found under route: '+ self.File_PATH_Datasets)\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_linear_sem(W, n, T, sem_type, noise_scale=1.0):\n",
        "        \"\"\"\n",
        "        Simulate samples from linear SEM with specified type of noise.\n",
        "        For uniform, noise z ~ uniform(-a, a), where a = noise_scale.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        W: np.ndarray\n",
        "            [d, d] weighted adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples, n=inf mimics population risk.\n",
        "        T: int\n",
        "        Number of timeseries for standard trainning dataset.\n",
        "        sem_type: str\n",
        "            gauss, exp, gumbel, uniform, logistic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        XX: np.ndarray\n",
        "            [T, n, d] sample matrix, [d, d] if n and T=inf\n",
        "        \"\"\"\n",
        "        def _simulate_single_equation(X, w, scale):\n",
        "            \"\"\"X: [n, num of parents], w: [num of parents], x: [n]\"\"\"\n",
        "            if sem_type == 'gauss':\n",
        "                z = np.random.normal(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'exp':\n",
        "                z = np.random.exponential(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'gumbel':\n",
        "                z = np.random.gumbel(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'uniform':\n",
        "                z = np.random.uniform(low=-scale, high=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'logistic':\n",
        "                x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a linear model, \\\n",
        "                                 the options are as follows: gauss, exp, \\\n",
        "                                 gumbel, uniform, logistic.')\n",
        "            return x\n",
        "\n",
        "        d = W.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "        G_nx =  nx.from_numpy_array(W, create_using=nx.DiGraph)\n",
        "        if not nx.is_directed_acyclic_graph(G_nx):\n",
        "            raise ValueError('W must be a DAG')\n",
        "        if np.isinf(T):  # population risk for linear gauss SEM\n",
        "            if sem_type == 'gauss':\n",
        "                # make 1/d X'X = true cov\n",
        "                X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)\n",
        "                return X\n",
        "            else:\n",
        "                raise ValueError('population risk not available')\n",
        "        # empirical risk\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        X = np.zeros([T, d])\n",
        "        XX = np.zeros((d, n, T))\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j])\n",
        "        for ns in range(n):\n",
        "            XX[:, ns] = np.transpose(X)\n",
        "        return XX\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_nonlinear_sem(W, n, T, sem_type, noise_scale=1.0):\n",
        "        \"\"\"\n",
        "        Simulate samples from nonlinear SEM.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        B: np.ndarray\n",
        "            [d, d] binary adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples.\n",
        "        T: int\n",
        "            Number of times.\n",
        "        sem_type: str\n",
        "            mlp, mim, gp, gp-add, or quadratic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        XX: np.ndarray\n",
        "            [d, n, T] sample matrix\n",
        "        \"\"\"\n",
        "        if sem_type == 'quadratic':\n",
        "            return Generate_SyntheticData._simulate_quad_sem(W, T, noise_scale)\n",
        "\n",
        "        def _simulate_single_equation(X, scale):\n",
        "            \"\"\"X: [n, num of parents], x: [n]\"\"\"\n",
        "            z = np.random.normal(scale=scale, size=n)\n",
        "            pa_size = X.shape[1]\n",
        "            if pa_size == 0:\n",
        "                return z\n",
        "            if sem_type == 'mlp':\n",
        "                hidden = 100\n",
        "                W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
        "                W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
        "                W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
        "                W2[np.random.rand(hidden) < 0.5] *= -1\n",
        "                x = sigmoid(X @ W1) @ W2 + z\n",
        "            elif sem_type == 'mim':\n",
        "                w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w1[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w2[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w3[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
        "            elif sem_type == 'gp':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = gp.sample_y(X, random_state=None).flatten() + z\n",
        "            elif sem_type == 'gp-add':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
        "                        for i in range(X.shape[1])]) + z\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a nonlinear model, \\\n",
        "                                 the options are as follows: mlp, mim, \\\n",
        "                                 gp, gp-add, or quadratic.')\n",
        "            return x\n",
        "\n",
        "        B = (W != 0).astype(int)\n",
        "        d = B.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "\n",
        "        X = np.zeros([n, d])\n",
        "        G_nx =  nx.from_numpy_array(B, create_using=nx.DiGraph)\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], scale_vec[j])\n",
        "        return X\n",
        "        XX = np.zeros((d, n, T))\n",
        "        for ns in range(n):\n",
        "            XX[:, ns] = np.transpose(X)\n",
        "        return XX\n",
        "\n",
        "\n",
        "\n",
        "class DAG(object):\n",
        "    '''\n",
        "    A class for simulating random (causal) DAG, where any DAG generator\n",
        "    method would return the weighed/binary adjacency matrix of a DAG.\n",
        "    Besides, we recommend using the python package \"NetworkX\"\n",
        "    to create more structures types.\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_permutation(M):\n",
        "        # np.random.permutation permutes first axis only\n",
        "        P = np.random.permutation(np.eye(M.shape[0]))\n",
        "        return P.T @ M @ P\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_acyclic_orientation(B_und):\n",
        "        B = np.tril(DAG._random_permutation(B_und), k=-1)\n",
        "        B_perm = DAG._random_permutation(B)\n",
        "        return B_perm\n",
        "\n",
        "    @staticmethod\n",
        "    def _graph_to_adjmat(G):\n",
        "        return nx.to_numpy_array(G)\n",
        "\n",
        "    @staticmethod\n",
        "    def _BtoW(B, d, w_range):\n",
        "        U = np.random.uniform(low=w_range[0], high=w_range[1], size=[d, d])\n",
        "        U[np.random.rand(d, d) < 0.5] *= -1\n",
        "        W = (B != 0).astype(float) * U\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def _low_rank_dag(d, degree, rank):\n",
        "        \"\"\"\n",
        "        Simulate random low rank DAG with some expected degree.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        d: int\n",
        "            Number of nodes.\n",
        "        degree: int\n",
        "            Expected node degree, in + out.\n",
        "        rank: int\n",
        "            Maximum rank (rank < d-1).\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        B: np.nparray\n",
        "            Initialize DAG.\n",
        "        \"\"\"\n",
        "        prob = float(degree) / (d - 1)\n",
        "        B = np.triu((np.random.rand(d, d) < prob).astype(float), k=1)\n",
        "        total_edge_num = np.sum(B == 1)\n",
        "        sampled_pa = sample(range(d - 1), rank)\n",
        "        sampled_pa.sort(reverse=True)\n",
        "        sampled_ch = []\n",
        "        for i in sampled_pa:\n",
        "            candidate = set(range(i + 1, d))\n",
        "            candidate = candidate - set(sampled_ch)\n",
        "            sampled_ch.append(sample(candidate, 1)[0])\n",
        "            B[i, sampled_ch[-1]] = 1\n",
        "        remaining_pa = list(set(range(d)) - set(sampled_pa))\n",
        "        remaining_ch = list(set(range(d)) - set(sampled_ch))\n",
        "        B[np.ix_(remaining_pa, remaining_ch)] = 0\n",
        "        after_matching_edge_num = np.sum(B == 1)\n",
        "\n",
        "        # delta = total_edge_num - after_matching_edge_num\n",
        "        # mask B\n",
        "        maskedB = B + np.tril(np.ones((d, d)))\n",
        "        maskedB[np.ix_(remaining_pa, remaining_ch)] = 1\n",
        "        B[maskedB == 0] = 1\n",
        "\n",
        "        remaining_ch_set = set([i + d for i in remaining_ch])\n",
        "        sampled_ch_set = set([i + d for i in sampled_ch])\n",
        "        remaining_pa_set = set(remaining_pa)\n",
        "        sampled_pa_set = set(sampled_pa)\n",
        "\n",
        "        edges = np.transpose(np.nonzero(B))\n",
        "        edges[:, 1] += d\n",
        "        bigraph = nx.Graph()\n",
        "        bigraph.add_nodes_from(range(2 * d))\n",
        "        bigraph.add_edges_from(edges)\n",
        "        M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "        while len(M) > 2 * rank:\n",
        "            keys = set(M.keys())\n",
        "            rmv_cand = keys & (remaining_pa_set | remaining_ch_set)\n",
        "            p = sample(rmv_cand, 1)[0]\n",
        "            c = M[p]\n",
        "            # destroy p-c\n",
        "            bigraph.remove_edge(p, c)\n",
        "            M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "\n",
        "        new_edges = np.array(bigraph.edges)\n",
        "        for i in range(len(new_edges)):\n",
        "            new_edges[i,].sort()\n",
        "        new_edges[:, 1] -= d\n",
        "\n",
        "        BB = np.zeros((d, d))\n",
        "        B = np.zeros((d, d))\n",
        "        BB[new_edges[:, 0], new_edges[:, 1]] = 1\n",
        "\n",
        "        if np.sum(BB == 1) > total_edge_num:\n",
        "            delta = total_edge_num - rank\n",
        "            BB[sampled_pa, sampled_ch] = 0\n",
        "            rmv_cand_edges = np.transpose(np.nonzero(BB))\n",
        "            if delta <= 0:\n",
        "                raise RuntimeError(r'Number of edges is below the rank, please \\\n",
        "                                   set a larger edge or degree \\\n",
        "                                   (you can change seed or increase degree).')\n",
        "            selected = np.array(sample(rmv_cand_edges.tolist(), delta))\n",
        "            B[selected[:, 0], selected[:, 1]] = 1\n",
        "            B[sampled_pa, sampled_ch] = 1\n",
        "        else:\n",
        "            B = deepcopy(BB)\n",
        "\n",
        "        B = B.transpose()\n",
        "        return B\n",
        "\n",
        "    @staticmethod\n",
        "    def erdos_renyi(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Erdos-Renyi\n",
        "        creation_prob = (2 * n_edges) / (n_nodes ** 2)\n",
        "        G_und = nx.erdos_renyi_graph(n=n_nodes, p=creation_prob, seed=seed)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_free(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert (n_nodes > 0 and n_edges >= n_nodes and n_edges < n_nodes * n_nodes)\n",
        "        set_random_seed(seed)\n",
        "        # Scale-free, Barabasi-Albert\n",
        "        m = int(round(n_edges / n_nodes))\n",
        "        G_und = nx.barabasi_albert_graph(n=n_nodes, m=m)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def bipartite(n_nodes, n_edges, split_ratio = 0.2, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
        "        n_top = int(split_ratio * n_nodes)\n",
        "        n_bottom = n_nodes -  n_top\n",
        "        creation_prob = n_edges/(n_top*n_bottom)\n",
        "        G_und = bipartite.random_graph(n_top, n_bottom, p=creation_prob, directed=True)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def hierarchical(n_nodes, degree=5, graph_level=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 1\n",
        "        set_random_seed(seed)\n",
        "        prob = float(degree) / (n_nodes - 1)\n",
        "        B = np.tril((np.random.rand(n_nodes, n_nodes) < prob).astype(float), k=-1)\n",
        "        point = sample(range(n_nodes - 1), graph_level - 1)\n",
        "        point.sort()\n",
        "        point = [0] + [x + 1 for x in point] + [n_nodes]\n",
        "        for i in range(graph_level):\n",
        "            B[point[i]:point[i + 1], point[i]:point[i + 1]] = 0\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def low_rank(n_nodes, degree=1, rank=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        B = DAG._low_rank_dag(n_nodes, degree, rank)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n"
      ],
      "metadata": {
        "id": "JOm5Z8LS8e5U"
      },
      "id": "JOm5Z8LS8e5U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 1.3: ANM-NCPOP_Algorithm__"
      ],
      "metadata": {
        "id": "lFCfV-BR8yXt"
      },
      "id": "lFCfV-BR8yXt"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from sklearn.preprocessing import scale\n",
        "from itertools import combinations\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import hsic_test\n",
        "from ncpol2sdpa import*\n",
        "\n",
        "\n",
        "\n",
        "class Anm_ncpop_test(object):\n",
        "    '''\n",
        "    A class for simulating (causal) DAG, where the true DAG is a weighed/binary adjacency matrix based on ground truth.\n",
        "\n",
        "    Parameters\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    File_PATH: str\n",
        "            Read file path\n",
        "    File_NAME: str\n",
        "            Read data name\n",
        "    File_PATH_Summary_Datails: str\n",
        "            Save file path\n",
        "    datasize: series\n",
        "\n",
        "    Timesize: series\n",
        "\n",
        "    Returns\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    Metrics DAG: np.matrix\n",
        "            heatmap between estimate DAG matrix and true DAG\n",
        "    Casaul Metrics: np.matrix\n",
        "            estimate DAG matrix\n",
        "    Summary scores table: pd.dataframe\n",
        "           col_names = ['Datasize','Timesets', 'Duration', 'fdr', 'tpr', 'fpr', 'shd', 'nnz', 'precision', 'recall', 'F1', 'gscore'])\n",
        "    Summary table: pd.dataframe\n",
        "           col_names = ['DataSize', 'Timesets', 'F1_Score', 'Duration']\n",
        "\n",
        "    Examples\n",
        "    -------------------------------------------------------------------------------------------------\n",
        "    >>> # filename = LinearSEM_GaussNoise\n",
        "    >>> # data_name = LinearSEM_GaussNoise_6Nodes_15Edges_TS\n",
        "    >>> # save_name = LinearSEM_GaussNoise_6Nodes_15Edges_TS_15Datasize_5Timesets\n",
        "\n",
        "    >>> datasize = range(5, 40, 5)\n",
        "    >>> Timesize = range(3, 6, 1)\n",
        "    >>> File_PATH_Base = 'Test/Examples/Test_data/'\n",
        "    >>> File_PATH_Summary_Datails = 'Test/Examples/Test_data/Summary'\n",
        "    >>> File_PATH = File_PATH\n",
        "    >>> Data_NAME = 'LinearSEM_GaussNoise_6Nodes_15Edges_TS.npz'\n",
        "    >>> # Data_NAME = 'Krebs_Cycle_16Nodes_43Edges_TS.npz'\n",
        "    >>> rt = Anm_ncpop_test(File_PATH_Base, Data_NAME, File_PATH_Summary_Datails, datasize, Timesize)\n",
        "    >>> rt.Ancpop()\n",
        "\n",
        "    '''\n",
        "    def __init__(self, File_PATH = None, Datasize=range(5, 40, 5), Timeset= range(3, 6, 1)):\n",
        "        self.File_PATH = File_PATH\n",
        "        self.Datasize =  Datasize\n",
        "        # self.Datasize_num = len(self.Datasize)\n",
        "        self.Timeset = Timeset\n",
        "        # self.Timesize_num = len(self.Timesize)\n",
        "        self.filename = filename\n",
        "        # re.split(\"_\", re.split(\"/\", self.File_PATH_Datasets)[-1])[0]\n",
        "\n",
        "\n",
        "    def Ancpop(self):\n",
        "        ################################################  Create Ground Tier Folders #############################################\n",
        "        self.File_PATH_Base = self.File_PATH +'Result_'+ self.filename +'/'\n",
        "        if not os.path.exists(self.File_PATH_Base):\n",
        "            os.makedirs(self.File_PATH_Base)\n",
        "        print('ANM-NCPOP INFO: Created Basement'+ ' File!')\n",
        "\n",
        "        ################################################  Create First Tier Folders #############################################\n",
        "        self.File_PATH_Summary = self.File_PATH_Base + 'Summary_'+ self.filename +'/'\n",
        "        if not os.path.exists(self.File_PATH_Summary):\n",
        "            os.makedirs(self.File_PATH_Summary)\n",
        "        print('ANM-NCPOP INFO: Created Summary'+ ' File!')\n",
        "\n",
        "        '''\n",
        "        self.File_PATH_Datasets = self.File_PATH_Base + 'Datasets_'+ self.filename +'/'\n",
        "        if not os.path.exists(self.File_PATH_Datasets):\n",
        "            os.makedirs(self.File_PATH_Datasets)\n",
        "            dt = Real_Data_Standardization(self.File_PATH, self.filename)\n",
        "            dt.standardize_data()\n",
        "        '''\n",
        "\n",
        "        print('ANM-NCPOP INFO: Created Datasets' + ' File!')\n",
        "\n",
        "        ################################################  Create Second Tier Folders #############################################\n",
        "        self.File_PATH_Summary_Datails = self.File_PATH_Summary + 'Summary_Datails_'+self.filename +'/'\n",
        "        self.File_PATH_MetricsDAG = self.File_PATH_Summary +'MetricsDAG_'+self.filename +'/'\n",
        "        if not os.path.exists(self.File_PATH_Summary_Datails):\n",
        "            os.makedirs(self.File_PATH_Summary_Datails)\n",
        "        print('ANM-NCPOP INFO: Created Summary_Datails'+ ' File!')\n",
        "        if not os.path.exists(self.File_PATH_MetricsDAG):\n",
        "            os.makedirs(self.File_PATH_MetricsDAG)\n",
        "        print('ANM-NCPOP INFO: Created MetricsDAG'+ ' File!')\n",
        "\n",
        "        ################################################  Analyzing Data under Datasets ###############################\n",
        "        tqdm=os.listdir(self.File_PATH_Summary_Datails)\n",
        "        read_Dir=os.listdir(self.File_PATH_Datasets)\n",
        "        while len(tqdm)!= len(read_Dir):\n",
        "            for data_name in read_Dir:\n",
        "                # print(file_f)\n",
        "                filename = utils.saveName_transfer_to_filename(data_name)\n",
        "\n",
        "                df_F1 = self.File_PATH_Summary_Datails + 'F1_'+ data_name +'.csv'\n",
        "                if not os.path.exists(df_F1):\n",
        "                    Rawdata = np.load(self.File_PATH_Datasets+data_name)\n",
        "                    self.Ancpop_estimate(self, Rawdata, data_name)\n",
        "                print('ANM-NCPOP INFO: Finished '+ data_name+'Analyzing!')\n",
        "        print('ANM NCPOP INFO: Finished simulations!')\n",
        "\n",
        "    @staticmethod\n",
        "    def Ancpop_estimate(self, Rawdata, data_name):\n",
        "        Raw_data = Rawdata['x']\n",
        "        true_dag = Rawdata['y']\n",
        "        duration_anm_ncpop = []\n",
        "        f1_anm_ncpop = []\n",
        "        df = pd.DataFrame(columns=['Datasize','Timesets', 'Duration', 'fdr', 'tpr', 'fpr', 'shd', 'nnz', 'precision', 'recall', 'F1', 'gscore'])\n",
        "        for i in self.Datasize:\n",
        "            for j in self.Timeset:\n",
        "                data = Raw_data[:, :i, :j]\n",
        "                t_start = time.time()\n",
        "                # Test ANM-NCPOP\n",
        "                anmNCPO = ANM_NCPO(alpha=0.05)\n",
        "                anmNCPO.learn(data = data)\n",
        "                # Save estimate causal_matrix\n",
        "                save_name = data_name+'_' + str(i) + 'Datasize_'+str(j) +'Timesets'\n",
        "                pd.DataFrame(anmNCPO.causal_matrix).to_csv(self.File_PATH_MetricsDAG + save_name+'.csv',index=False)\n",
        "\n",
        "                # Plot predict_dag and true_dag\n",
        "                GraphDAG(anmNCPO.causal_matrix, true_dag, show=False, save_name = self.File_PATH_MetricsDAG + save_name+'.png')\n",
        "\n",
        "                # Save met.metrics\n",
        "                met = MetricsDAG(anmNCPO.causal_matrix, true_dag)\n",
        "                dict1 = {'Datasize':i, 'Timesets':j, 'Duration':time.time()-t_start}\n",
        "                dict2 = met.metrics\n",
        "                dict = {**dict1, **dict2}\n",
        "                df = pd.concat([df, pd.DataFrame([dict])])\n",
        "                if math.isnan(float(met.metrics['F1'])):\n",
        "                    f1_anm_ncpop.append(0.2)\n",
        "                else:\n",
        "                    f1_anm_ncpop.append(met.metrics['F1'])\n",
        "                print('ANM-NCPOP INFO: ' + save_name +' is done!'+'F1 Score is'+ str(met.metrics['F1'])+'.')\n",
        "                print('ANM-NCPOP INFO: Time Duration is '+ str(time.time()-t_start))\n",
        "                duration_anm_ncpop.append(time.time()-t_start)\n",
        "        df.to_csv(self.File_PATH_MetricsDAG + 'Scores_'+data_name+'.csv', index=False)\n",
        "        df_F1 = pd.DataFrame({\"DataSize\":self.Datasize, \"Timesets\":self.Timesets, 'F1_Score':f1_anm_ncpop, 'Duration':duration_anm_ncpop})\n",
        "        df_F1.to_csv(self.File_PATH_Summary_Datails + 'F1_'+data_name+'.csv',index=False)\n",
        "        return df_F1\n",
        "\n",
        "class NCPOLR(object):\n",
        "    \"\"\"Estimator based on NCPOP Regressor\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Quan Zhou https://github.com/Quan-Zhou/Proper-Learning-of-LDS/blob/master/ncpop/functions.py\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NCPOLR, self).__init__()\n",
        "\n",
        "\n",
        "    def generate_operators(name, n_vars=1, hermitian=None, commutative=False):\n",
        "        \"\"\"Generates a number of commutative or noncommutative operators\n",
        "\n",
        "        :param name: The prefix in the symbolic representation of the noncommuting\n",
        "                    variables. This will be suffixed by a number from 0 to\n",
        "                    n_vars-1 if n_vars > 1.\n",
        "        :type name: str.\n",
        "        :param n_vars: The number of variables.\n",
        "        :type n_vars: int.\n",
        "        :param hermitian: Optional parameter to request Hermitian variables .\n",
        "        :type hermitian: bool.\n",
        "        :param commutative: Optional parameter to request commutative variables.\n",
        "                            Commutative variables are Hermitian by default.\n",
        "        :type commutative: bool.\n",
        "\n",
        "        :returns: list of :class:`sympy.physics.quantum.operator.Operator` or\n",
        "                  :class:`sympy.physics.quantum.operator.HermitianOperator`\n",
        "                  variables\n",
        "\n",
        "        :Example:\n",
        "\n",
        "        >>> generate_variables('y', 2, commutative=True)\n",
        "        ￼[y0, y1]\n",
        "        \"\"\"\n",
        "\n",
        "        variables = []\n",
        "        for i in range(n_vars):\n",
        "            if n_vars > 1:\n",
        "                var_name = '%s%s' % (name, i)\n",
        "            else:\n",
        "                var_name = '%s' % name\n",
        "            if hermitian is not None and hermitian:\n",
        "                variables.append(HermitianOperator(var_name))\n",
        "            else:\n",
        "                variables.append(Operator(var_name))\n",
        "            variables[-1].is_commutative = commutative\n",
        "        return variables\n",
        "\n",
        "    def estimate(self, X, Y):\n",
        "        \"\"\"Fit Estimator based on NCPOP Regressor model and predict y or produce residuals.\n",
        "        The module converts a noncommutative optimization problem provided in SymPy\n",
        "        format to an SDPA semidefinite programming problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array\n",
        "            Variable seen as cause\n",
        "        Y: array\n",
        "            Variable seen as effect\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predict: array\n",
        "            regression predict values of y or residuals\n",
        "        \"\"\"\n",
        "\n",
        "        T = len(Y)\n",
        "        level = 1\n",
        "\n",
        "        # Decision Variables\n",
        "        G = generate_operators(\"G\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        f = generate_operators(\"f\", n_vars=T, hermitian=True, commutative=False)\n",
        "        n = generate_operators(\"m\", n_vars=T, hermitian=True, commutative=False)\n",
        "        p = generate_operators(\"p\", n_vars=T, hermitian=True, commutative=False)\n",
        "\n",
        "        # Objective\n",
        "        obj = sum((Y[i]-f[i])**2 for i in range(T)) + 0.00005*sum(p[i] for i in range(T))\n",
        "\n",
        "        # Constraints\n",
        "        ine1 = [f[i] - G*X[i] - n[i] for i in range(T)]\n",
        "        ine2 = [-f[i] + G*X[i] + n[i] for i in range(T)]\n",
        "        ine3 = [p[i]-n[i] for i in range(T)]\n",
        "        ine4 = [p[i]+n[i] for i in range(T)]\n",
        "        ines = ine1+ine2+ine3+ine4\n",
        "\n",
        "        # Solve the NCPO\n",
        "        sdp = SdpRelaxation(variables = flatten([G,f,n,p]),verbose = 1)\n",
        "        sdp.get_relaxation(level, objective=obj, inequalities=ines)\n",
        "        sdp.solve(solver='mosek')\n",
        "        #sdp.solve(solver='sdpa', solverparameters={\"executable\":\"sdpa_gmp\",\"executable\": \"C:/Users/zhouq/Documents/sdpa7-windows/sdpa.exe\"})\n",
        "        print(sdp.primal, sdp.dual, sdp.status)\n",
        "\n",
        "        if(sdp.status != 'infeasible'):\n",
        "            print('ok.')\n",
        "            est_noise = []\n",
        "            for i in range(T):\n",
        "                est_noise.append(sdp[n[i]])\n",
        "            print(est_noise)\n",
        "            return est_noise\n",
        "        else:\n",
        "            print('Cannot find feasible solution.')\n",
        "            return\n",
        "\n",
        "    def estimate2(self, X, Y):\n",
        "        \"\"\"Fit Estimator based on NCPOP Regressor model and predict y or produce residuals.\n",
        "        The module converts a noncommutative optimization problem provided in SymPy\n",
        "        format to an SDPA semidefinite programming problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        Y: array\n",
        "            Variable seen as effect\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predict: array\n",
        "            regression predict values of y or residuals\n",
        "        \"\"\"\n",
        "        Y = np.transpose(Y)\n",
        "        T = len(Y)-1\n",
        "        level = 1\n",
        "\n",
        "\n",
        "        # Decision Variables\n",
        "        G = generate_operators(\"G\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        Fdash = generate_operators(\"Fdash\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        # m = generate_operators(\"m\", n_vars=T+1, hermitian=True, commutative=False)\n",
        "        q = generate_operators(\"q\", n_vars=T, hermitian=True, commutative=False)\n",
        "        p = generate_operators(\"p\", n_vars=T, hermitian=True, commutative=False)\n",
        "        f = generate_operators(\"f\", n_vars=T, hermitian=True, commutative=False)\n",
        "\n",
        "        # Objective\n",
        "        obj = sum((Y[i]-f[i])**2 for i in range(T)) + 0.001*sum(p[i]**2 for i in range(T)) + 0.0005*sum(q[i]**2 for i in range(T))\n",
        "\n",
        "        #c1*sum(p[i]**2 for i in range(T)) + c2*sum(q[i]**2 for i in range(T))\n",
        "\n",
        "        # Constraints\n",
        "        ine1 = [f[i] - Fdash*X[i+1] - p[i] for i in range(T)]\n",
        "        ine2 = [-f[i] + Fdash*X[i+1] + p[i] for i in range(T)]\n",
        "        ine3 = [X[i+1] - G*X[i] - q[i] for i in range(T)]\n",
        "        ine4 = [-X[i+1] + G*X[i] + q[i] for i in range(T)]\n",
        "        #ine5 = [(Y[i]-f[i])**2 for i in range(T)]\n",
        "        ines = ine1+ine2+ine3+ine4 #+ine5\n",
        "\n",
        "        # Solve the NCPO\n",
        "        sdp = SdpRelaxation(variables = flatten([G,Fdash,f,p,q]),verbose = 1)\n",
        "        sdp.get_relaxation(level, objective=obj, inequalities=ines)\n",
        "        sdp.solve(solver='mosek')\n",
        "\n",
        "        #sdp.solve(solver='sdpa', solverparameters={\"executable\":\"sdpa_gmp\",\"executable\": \"C:/Users/zhouq/Documents/sdpa7-windows/sdpa.exe\"})\n",
        "        print(sdp.primal, sdp.dual, sdp.status)\n",
        "\n",
        "        if(sdp.status != 'infeasible'):\n",
        "            print('ok.')\n",
        "            est_noise = []\n",
        "            for i in range(T):\n",
        "                est_noise.append(sdp[p[i]])\n",
        "            print(est_noise)\n",
        "            return est_noise, X[1:]\n",
        "        else:\n",
        "            print('Cannot find feasible solution.')\n",
        "            return\n",
        "\n",
        "\n",
        "class ANM_NCPO(BaseLearner):\n",
        "    \"\"\"\n",
        "    Nonlinear causal discovery with additive noise models\n",
        "\n",
        "    Use Estimator based on NCPOP Regressor and independent Gaussian noise,\n",
        "    For the independence test, we implemented the HSIC with a Gaussian kernel,\n",
        "    where we used the gamma distribution as an approximation for the\n",
        "    distribution of the HSIC under the null hypothesis of independence\n",
        "    in order to calculate the p-value of the test result.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Hoyer, Patrik O and Janzing, Dominik and Mooij, Joris M and Peters,\n",
        "    Jonas and Schölkopf, Bernhard,\n",
        "    \"Nonlinear causal discovery with additive noise models\", NIPS 2009\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float, default 0.05\n",
        "        significance level be used to compute threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : array like shape of (n_features, n_features)\n",
        "        Learned causal structure matrix.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.05):\n",
        "        super(ANM_NCPO, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def learn(self, data, columns=None, regressor=NCPOLR(),test_method=hsic_test, **kwargs):\n",
        "        \"\"\"Set up and run the ANM_NCPOP algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: numpy.ndarray or Tensor\n",
        "            Training data.\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is NCPOLR.\n",
        "            If user defined, must implement `estimate` self.method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test self.method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "        \"\"\"\n",
        "\n",
        "        self.regressor = regressor\n",
        "\n",
        "        # create learning model and ground truth model\n",
        "        data = Tensor(data, columns=columns)\n",
        "\n",
        "        node_num = data.shape[0]\n",
        "        self.causal_matrix = Tensor(np.zeros((node_num, node_num)))\n",
        "\n",
        "        for i, j in combinations(range(node_num), 2):\n",
        "            x = data[i, :, :]\n",
        "            y = data[j, :, :]\n",
        "            xx = x.reshape(-1,1)\n",
        "            yy = y.reshape(-1,1)\n",
        "\n",
        "            flag = test_method(xx, yy, alpha=self.alpha)\n",
        "            if flag == 1:\n",
        "                continue\n",
        "            # test x-->y\n",
        "            flag = self.ANMNCPO_fitness(x, y, regressor = regressor, test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[i, j] = 1\n",
        "            # test y-->x\n",
        "            flag = self.ANMNCPO_fitness(y, x, regressor = regressor, test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[j, i] = 1\n",
        "\n",
        "    def ANMNCPO_fitness(self, x, y, regressor=NCPOLR(), test_method=hsic_test):\n",
        "        \"\"\"Compute the fitness score of the ANM_NCPOP Regression model in the x->y direction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: array\n",
        "            Variable seen as cause\n",
        "        y: array\n",
        "            Variable seen as effect\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is NCPOP.\n",
        "            If user defined, must implement `estimate` self.method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test self.method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "        Returns\n",
        "        -------\n",
        "        out: int, 0 or 1\n",
        "            If 1, residuals n is independent of x, then accept x --> y\n",
        "            If 0, residuals n is not independent of x, then reject x --> y\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        x = scale(x).reshape(-1)\n",
        "        y = scale(y).reshape(-1)\n",
        "\n",
        "        ncpop_res = regressor.estimate(x, y)\n",
        "        print(x)\n",
        "        print(y)\n",
        "\n",
        "        flag = test_method(np.asarray(ncpop_res[0]).reshape((-1, 1)), np.asarray(ncpop_res[1]).reshape((-1, 1)), alpha=self.alpha)\n",
        "\n",
        "        print(flag)\n",
        "\n",
        "        return flag\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hP6jvLLn44CT"
      },
      "id": "hP6jvLLn44CT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Step 2: Standardize Real data Or Generate Synthetic data__"
      ],
      "metadata": {
        "id": "ey3T5UNqHAzf"
      },
      "id": "ey3T5UNqHAzf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 2.1: Compress Krebs_Cycle time series__"
      ],
      "metadata": {
        "id": "qiiFTTxf7KAr"
      },
      "id": "qiiFTTxf7KAr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Krebs_Cycle\n",
        "File_PATH = \"../Path/to/the/Real/Data/\"\n",
        "file_name = 'Krebs_Cycle'\n",
        "dt = Real_Data_Standardization(File_PATH, file_name)\n",
        "dt.standardize_data()"
      ],
      "metadata": {
        "id": "3_izC5Dy6hwU"
      },
      "id": "3_izC5Dy6hwU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 2.2: Generate Synthetic data__"
      ],
      "metadata": {
        "id": "gKah7IVJ5TQ9"
      },
      "id": "gKah7IVJ5TQ9"
    },
    {
      "cell_type": "code",
      "source": [
        "method = 'linear'\n",
        "sem_type = 'gauss'\n",
        "nodes = range(6,12,3)\n",
        "edges = range(10,20,5)\n",
        "T=200\n",
        "num_datasets = 120\n",
        "Save_PATH = '../Path/to/Save/the/Synthetic/Data/'\n",
        "noise_scale = 1.0\n",
        "_ts = Generate_Synthetic_Data(Save_PATH, num_datasets, T, method, sem_type, nodes, edges, noise_scale)\n",
        "_ts.genarate_data()"
      ],
      "metadata": {
        "id": "podBqIqD5YZO"
      },
      "id": "podBqIqD5YZO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Step 3: Test ANM-NCPOP__\n"
      ],
      "metadata": {
        "id": "-r4xcm2W5uCQ"
      },
      "id": "-r4xcm2W5uCQ"
    },
    {
      "cell_type": "code",
      "source": [
        "datasize = range(5, 40, 5)\n",
        "Timesize = range(3, 6, 1)\n",
        "# Krebs_Cycle\n",
        "File_PATH = \"../Path/to/Read/the/Testdata/And/Save/the/Results/\"\n",
        "rt = Anm_ncpop_test(File_PATH, datasize, Timesize)\n",
        "rt.Ancpop()"
      ],
      "metadata": {
        "id": "gwdJDQruzdkB"
      },
      "id": "gwdJDQruzdkB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
