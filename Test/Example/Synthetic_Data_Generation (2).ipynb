{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ANMNCPOP_Synthetic_Data_Generation\n",
        "* In our example, weighted random DAG is produced according to num_nodes and num_edges. Test raw datasets is the time series generating from weighted random DAG and SEM type. For further analysis, all useful infomation is extracted and saved as **npz file** storing causality Data as NumPy array x and y.\n",
        "\n",
        "* In our paper, we generated LinearGauss_6_15, LinearGauss_6_15_TS and Krebs_Cycle data in respect to testing orignal two dimensions data and multi-features time series.\n",
        "\n",
        "# LinearGauss_6_15\n",
        "\n",
        "LinearGauss_6_15.npz\n",
        "\n",
        "* __x__: is an array with two dimensions(Features_Samples) generated from weighted random DAG and SEM type.\n",
        "* __y__: represents the weighted random DAG generating the artificial data.\n",
        "\n",
        "# Krebs_Cycle\n",
        "\n",
        "Krebs_Cycle_16_43_TS.npz\n",
        "\n",
        "Multiple features time series are saved under file Krebs_Cycle_TS. The causal_matrix is saved as true_graph.csv.\n",
        "\n",
        "* __x__(Features_Samples_Timesets): is an array in shape(F, S, T), where the number of row F is features_num, the number of column S is smples_num and the number of deep T is timesets.\n",
        "* __y__(Features_Features): is a nonsymmetric square matrix.\n",
        "\n",
        "# LinearGauss_6_15_TS\n",
        "LinearGauss_6_15_TS.npz\n",
        "\n",
        "* __x__(Features_Samples_Timesets): is an unitary three dimensions time seires array generated from weighted random DAG and SEM type with high dimension hidden state.\n",
        "* __y__(Features_Features): represents the weighted random DAG generating the artificial data.\n"
      ],
      "metadata": {
        "id": "5gQWZ1GduguB"
      },
      "id": "5gQWZ1GduguB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Get start__\n",
        "\n",
        "* mount drive\n",
        "* set envirment"
      ],
      "metadata": {
        "id": "-Dt3I4ccxuw1"
      },
      "id": "-Dt3I4ccxuw1"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/NCPOP/Causal_Models_Learning/Test/\")\n",
        "# os.chdir(\"/content/drive/MyDrive/Colab Notebooks/NCPOP/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTqOomJUueln",
        "outputId": "a3da12d6-e4c4-4bfc-ff45-bb59235c99ef"
      },
      "id": "jTqOomJUueln",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from Generate_SyntheticData import*\n",
        "from networkx.algorithms import bipartite\n",
        "from scipy.special import expit as sigmoid\n",
        "from itertools import combinations\n",
        "from pickle import TRUE\n",
        "from random import sample\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import logging\n",
        "import tarfile\n",
        "import os\n",
        "import re\n",
        "# import time\n",
        "import random\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "class Generate_Synthetic_Data(object):\n",
        "    '''\n",
        "    Simulate IID datasets for causal structure learning.\n",
        "\n",
        "    Parameters\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    File_PATH\n",
        "            Read data path\n",
        "    n: int\n",
        "            Number of samples for standard trainning dataset.\n",
        "    T: int\n",
        "            Number of timeseries for standard trainning dataset.\n",
        "    method: str, (linear or nonlinear), default='linear'\n",
        "            Distribution for standard trainning dataset.\n",
        "    sem_type: str\n",
        "            gauss, exp, gumbel, uniform, logistic (linear);\n",
        "            mlp, mim, gp, gp-add, quadratic (nonlinear).\n",
        "    nodes: series\n",
        "            Notes of samples for standard trainning dataset.\n",
        "    edges: series\n",
        "            Edges of samples for standard training dataset.\n",
        "    noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "    Returns\n",
        "    ------------------------------------------------------------------------------------------------\n",
        "    Raw_data: npz\n",
        "            xï¼š[d, n, T] sample time series\n",
        "            y: true_dag\n",
        "    File_PATH_Datasets:\n",
        "            Route of saving test data\n",
        "\n",
        "    Examples 1\n",
        "    -------------------------------------------------------------------------------------------------\n",
        "    >>> method = 'linear'\n",
        "    >>> sem_type = 'gauss'\n",
        "    >>> nodes = range(6,12,3)\n",
        "    >>> edges = range(10,20,5)\n",
        "    >>> T=200\n",
        "    >>> num_datasets = 120\n",
        "    >>> File_PATH = '../Test/Examples/Test_data/'\n",
        "    >>> noise_scale = 1.0\n",
        "    >>> _ts = Generate_Synthetic_Data(File_PATH, num_datasets, T, method, sem_type, nodes, edges, noise_scale)\n",
        "    >>> _ts.genarate_data()\n",
        "\n",
        "    Examples 2\n",
        "    -------------------------------------------------------------------------------------------------\n",
        "    >>> noise_type = {\n",
        "    >>>     'nonlinear': ['gp-add', 'mlp', 'mim', 'gp', 'quadratic'],\n",
        "    >>>     'linear':  ['gauss', 'exp', 'gumbel', 'uniform', 'logistic']\n",
        "    >>> }\n",
        "    >>> sem_type = ['linear', 'nonlinear']\n",
        "    >>> nodes = range(6,12,3)\n",
        "    >>> edges = range(10,20,5)\n",
        "    >>> T=200\n",
        "    >>> num_datasets = 120\n",
        "    >>> File_PATH = '../Test/Examples/Test_data/'\n",
        "    >>> noise_scale = 1.0\n",
        "\n",
        "    >>> for m in sem_type :\n",
        "    >>>   for s in noise_type[m]:\n",
        "    >>>     _ts = Generate_Synthetic_Data(File_PATH, num_datasets, T, m, s, nodes, edges, noise_scale)\n",
        "    >>>     print(File_PATH, num_datasets, T, m, s, nodes, edges, noise_scale)\n",
        "    >>>     _ts.genarate_data()\n",
        "    '''\n",
        "\n",
        "    def __init__(self, File_PATH, n, T, method, sem_type, nodes, edges, noise_scale):\n",
        "        self.File_PATH = File_PATH\n",
        "        self.n = n\n",
        "        self.T = T\n",
        "        self.method = method\n",
        "        self.sem_type =sem_type\n",
        "        self.filename = self.method.capitalize()+'SEM_' + self.sem_type.capitalize() +'Noise'\n",
        "        self.nodes = nodes\n",
        "        self.edges = edges\n",
        "        self.noise_scale = noise_scale\n",
        "\n",
        "    def genarate_data(self):\n",
        "        ################################################  Create Ground Tier Folders #############################################\n",
        "        self.File_PATH_Base = self.File_PATH +'Result_'+ self.filename +'/'\n",
        "\n",
        "        ################################################  Create First Tier Folders #############################################\n",
        "        self.File_PATH_Datasets = self.File_PATH_Base + 'Datasets_'+ self.filename +'/'\n",
        "        if not os.path.exists(self.File_PATH_Datasets):\n",
        "            os.makedirs(self.File_PATH_Datasets)\n",
        "        print('ANM-NCPOP INFO: Created Datasets' + ' File!')\n",
        "\n",
        "        nodes_num = len(self.nodes)\n",
        "        edges_num = len(self.edges)\n",
        "        count = 0\n",
        "        tqdm_csv=os.listdir(self.File_PATH_Datasets)\n",
        "        while len(tqdm_csv) < nodes_num* edges_num:\n",
        "            print('ANM-NCPOP INFO: Generating '+ self.filename + ' Dataset!')\n",
        "            if self.method == 'linear':\n",
        "                for nn in nodes:\n",
        "                    for ne in edges:\n",
        "                        count += 1\n",
        "                        w = DAG.erdos_renyi(n_nodes=nn, n_edges=ne, seed=1)\n",
        "                        self.B = (w != 0).astype(int)\n",
        "                        self.XX = Generate_Synthetic_Data._simulate_linear_sem(self.B, self.n, self.T, self.sem_type, self.noise_scale)\n",
        "                        data_name = self.filename+'_'+str(nn)+'Nodes_'+str(ne)+'Edges_TS'\n",
        "                        np.savez(self.File_PATH_Datasets +data_name+'.npz', x=self.XX , y=self.B)\n",
        "                        logging.info('ANM-NCPOP INFO: Finished synthetic dataset')\n",
        "                        print('ANM-NCPOP INFO: '+ data_name + ' IS DONE!')\n",
        "                print('ANM-NCPOP INFO: '+ str(count) + ' datasets are generated!')\n",
        "                break\n",
        "            elif self.method == 'nonlinear':\n",
        "                for nn in nodes:\n",
        "                    for ne in edges:\n",
        "                        count += 1\n",
        "                        w = DAG.erdos_renyi(n_nodes=nn, n_edges=ne, seed=1)\n",
        "                        self.B = (w != 0).astype(int)\n",
        "                        self.XX = Generate_Synthetic_Data._simulate_nonlinear_sem(self.B, self.n, self.T, self.sem_type, self.noise_scale)\n",
        "                        data_name = self.filename+'_'+str(nn)+'Nodes_'+str(ne)+'Edges_TS'\n",
        "                        np.savez(self.File_PATH_Datasets +data_name+'.npz', x=self.XX , y=self.B)\n",
        "                        logging.info('ANM-NCPOP INFO: Finished synthetic dataset')\n",
        "                        print('ANM-NCPOP INFO: '+ data_name + ' IS DONE!')\n",
        "                print('ANM-NCPOP INFO: '+ str(count) + ' datasets are generated!')\n",
        "                break\n",
        "            else:\n",
        "                raise ValueError('Unknown distribution type. Only linear and nonlinear types are accepted.')\n",
        "\n",
        "            # time.sleep(30)\n",
        "        print('ANM-NCPOP INFO: Finished '+ self.filename +' dataset generation, which can be found under route: '+ self.File_PATH_Datasets)\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_linear_sem(W, n, T, sem_type, noise_scale=1.0):\n",
        "        \"\"\"\n",
        "        Simulate samples from linear SEM with specified type of noise.\n",
        "        For uniform, noise z ~ uniform(-a, a), where a = noise_scale.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        W: np.ndarray\n",
        "            [d, d] weighted adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples, n=inf mimics population risk.\n",
        "        T: int\n",
        "        Number of timeseries for standard trainning dataset.\n",
        "        sem_type: str\n",
        "            gauss, exp, gumbel, uniform, logistic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        XX: np.ndarray\n",
        "            [T, n, d] sample matrix, [d, d] if n and T=inf\n",
        "        \"\"\"\n",
        "        def _simulate_single_equation(X, w, scale):\n",
        "            \"\"\"X: [n, num of parents], w: [num of parents], x: [n]\"\"\"\n",
        "            if sem_type == 'gauss':\n",
        "                z = np.random.normal(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'exp':\n",
        "                z = np.random.exponential(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'gumbel':\n",
        "                z = np.random.gumbel(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'uniform':\n",
        "                z = np.random.uniform(low=-scale, high=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'logistic':\n",
        "                x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a linear model, \\\n",
        "                                 the options are as follows: gauss, exp, \\\n",
        "                                 gumbel, uniform, logistic.')\n",
        "            return x\n",
        "\n",
        "        d = W.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "        G_nx =  nx.from_numpy_array(W, create_using=nx.DiGraph)\n",
        "        if not nx.is_directed_acyclic_graph(G_nx):\n",
        "            raise ValueError('W must be a DAG')\n",
        "        if np.isinf(T):  # population risk for linear gauss SEM\n",
        "            if sem_type == 'gauss':\n",
        "                # make 1/d X'X = true cov\n",
        "                X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)\n",
        "                return X\n",
        "            else:\n",
        "                raise ValueError('population risk not available')\n",
        "        # empirical risk\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        X = np.zeros([T, d])\n",
        "        XX = np.zeros((d, n, T))\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j])\n",
        "        for ns in range(n):\n",
        "            XX[:, ns] = np.transpose(X)\n",
        "        return XX\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_nonlinear_sem(W, n, T, sem_type, noise_scale=1.0):\n",
        "        \"\"\"\n",
        "        Simulate samples from nonlinear SEM.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        B: np.ndarray\n",
        "            [d, d] binary adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples.\n",
        "        T: int\n",
        "            Number of times.\n",
        "        sem_type: str\n",
        "            mlp, mim, gp, gp-add, or quadratic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        XX: np.ndarray\n",
        "            [d, n, T] sample matrix\n",
        "        \"\"\"\n",
        "        if sem_type == 'quadratic':\n",
        "            return Generate_SyntheticData._simulate_quad_sem(W, T, noise_scale)\n",
        "\n",
        "        def _simulate_single_equation(X, scale):\n",
        "            \"\"\"X: [n, num of parents], x: [n]\"\"\"\n",
        "            z = np.random.normal(scale=scale, size=n)\n",
        "            pa_size = X.shape[1]\n",
        "            if pa_size == 0:\n",
        "                return z\n",
        "            if sem_type == 'mlp':\n",
        "                hidden = 100\n",
        "                W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
        "                W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
        "                W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
        "                W2[np.random.rand(hidden) < 0.5] *= -1\n",
        "                x = sigmoid(X @ W1) @ W2 + z\n",
        "            elif sem_type == 'mim':\n",
        "                w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w1[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w2[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w3[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
        "            elif sem_type == 'gp':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = gp.sample_y(X, random_state=None).flatten() + z\n",
        "            elif sem_type == 'gp-add':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
        "                        for i in range(X.shape[1])]) + z\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a nonlinear model, \\\n",
        "                                 the options are as follows: mlp, mim, \\\n",
        "                                 gp, gp-add, or quadratic.')\n",
        "            return x\n",
        "\n",
        "        B = (W != 0).astype(int)\n",
        "        d = B.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "\n",
        "        X = np.zeros([n, d])\n",
        "        G_nx =  nx.from_numpy_array(B, create_using=nx.DiGraph)\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], scale_vec[j])\n",
        "        return X\n",
        "        XX = np.zeros((d, n, T))\n",
        "        for ns in range(n):\n",
        "            XX[:, ns] = np.transpose(X)\n",
        "        return XX\n",
        "\n",
        "\n",
        "\n",
        "class DAG(object):\n",
        "    '''\n",
        "    A class for simulating random (causal) DAG, where any DAG generator\n",
        "    method would return the weighed/binary adjacency matrix of a DAG.\n",
        "    Besides, we recommend using the python package \"NetworkX\"\n",
        "    to create more structures types.\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_permutation(M):\n",
        "        # np.random.permutation permutes first axis only\n",
        "        P = np.random.permutation(np.eye(M.shape[0]))\n",
        "        return P.T @ M @ P\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_acyclic_orientation(B_und):\n",
        "        B = np.tril(DAG._random_permutation(B_und), k=-1)\n",
        "        B_perm = DAG._random_permutation(B)\n",
        "        return B_perm\n",
        "\n",
        "    @staticmethod\n",
        "    def _graph_to_adjmat(G):\n",
        "        return nx.to_numpy_array(G)\n",
        "\n",
        "    @staticmethod\n",
        "    def _BtoW(B, d, w_range):\n",
        "        U = np.random.uniform(low=w_range[0], high=w_range[1], size=[d, d])\n",
        "        U[np.random.rand(d, d) < 0.5] *= -1\n",
        "        W = (B != 0).astype(float) * U\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def _low_rank_dag(d, degree, rank):\n",
        "        \"\"\"\n",
        "        Simulate random low rank DAG with some expected degree.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        d: int\n",
        "            Number of nodes.\n",
        "        degree: int\n",
        "            Expected node degree, in + out.\n",
        "        rank: int\n",
        "            Maximum rank (rank < d-1).\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        B: np.nparray\n",
        "            Initialize DAG.\n",
        "        \"\"\"\n",
        "        prob = float(degree) / (d - 1)\n",
        "        B = np.triu((np.random.rand(d, d) < prob).astype(float), k=1)\n",
        "        total_edge_num = np.sum(B == 1)\n",
        "        sampled_pa = sample(range(d - 1), rank)\n",
        "        sampled_pa.sort(reverse=True)\n",
        "        sampled_ch = []\n",
        "        for i in sampled_pa:\n",
        "            candidate = set(range(i + 1, d))\n",
        "            candidate = candidate - set(sampled_ch)\n",
        "            sampled_ch.append(sample(candidate, 1)[0])\n",
        "            B[i, sampled_ch[-1]] = 1\n",
        "        remaining_pa = list(set(range(d)) - set(sampled_pa))\n",
        "        remaining_ch = list(set(range(d)) - set(sampled_ch))\n",
        "        B[np.ix_(remaining_pa, remaining_ch)] = 0\n",
        "        after_matching_edge_num = np.sum(B == 1)\n",
        "\n",
        "        # delta = total_edge_num - after_matching_edge_num\n",
        "        # mask B\n",
        "        maskedB = B + np.tril(np.ones((d, d)))\n",
        "        maskedB[np.ix_(remaining_pa, remaining_ch)] = 1\n",
        "        B[maskedB == 0] = 1\n",
        "\n",
        "        remaining_ch_set = set([i + d for i in remaining_ch])\n",
        "        sampled_ch_set = set([i + d for i in sampled_ch])\n",
        "        remaining_pa_set = set(remaining_pa)\n",
        "        sampled_pa_set = set(sampled_pa)\n",
        "\n",
        "        edges = np.transpose(np.nonzero(B))\n",
        "        edges[:, 1] += d\n",
        "        bigraph = nx.Graph()\n",
        "        bigraph.add_nodes_from(range(2 * d))\n",
        "        bigraph.add_edges_from(edges)\n",
        "        M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "        while len(M) > 2 * rank:\n",
        "            keys = set(M.keys())\n",
        "            rmv_cand = keys & (remaining_pa_set | remaining_ch_set)\n",
        "            p = sample(rmv_cand, 1)[0]\n",
        "            c = M[p]\n",
        "            # destroy p-c\n",
        "            bigraph.remove_edge(p, c)\n",
        "            M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "\n",
        "        new_edges = np.array(bigraph.edges)\n",
        "        for i in range(len(new_edges)):\n",
        "            new_edges[i,].sort()\n",
        "        new_edges[:, 1] -= d\n",
        "\n",
        "        BB = np.zeros((d, d))\n",
        "        B = np.zeros((d, d))\n",
        "        BB[new_edges[:, 0], new_edges[:, 1]] = 1\n",
        "\n",
        "        if np.sum(BB == 1) > total_edge_num:\n",
        "            delta = total_edge_num - rank\n",
        "            BB[sampled_pa, sampled_ch] = 0\n",
        "            rmv_cand_edges = np.transpose(np.nonzero(BB))\n",
        "            if delta <= 0:\n",
        "                raise RuntimeError(r'Number of edges is below the rank, please \\\n",
        "                                   set a larger edge or degree \\\n",
        "                                   (you can change seed or increase degree).')\n",
        "            selected = np.array(sample(rmv_cand_edges.tolist(), delta))\n",
        "            B[selected[:, 0], selected[:, 1]] = 1\n",
        "            B[sampled_pa, sampled_ch] = 1\n",
        "        else:\n",
        "            B = deepcopy(BB)\n",
        "\n",
        "        B = B.transpose()\n",
        "        return B\n",
        "\n",
        "    @staticmethod\n",
        "    def erdos_renyi(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Erdos-Renyi\n",
        "        creation_prob = (2 * n_edges) / (n_nodes ** 2)\n",
        "        G_und = nx.erdos_renyi_graph(n=n_nodes, p=creation_prob, seed=seed)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_free(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert (n_nodes > 0 and n_edges >= n_nodes and n_edges < n_nodes * n_nodes)\n",
        "        set_random_seed(seed)\n",
        "        # Scale-free, Barabasi-Albert\n",
        "        m = int(round(n_edges / n_nodes))\n",
        "        G_und = nx.barabasi_albert_graph(n=n_nodes, m=m)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def bipartite(n_nodes, n_edges, split_ratio = 0.2, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
        "        n_top = int(split_ratio * n_nodes)\n",
        "        n_bottom = n_nodes -  n_top\n",
        "        creation_prob = n_edges/(n_top*n_bottom)\n",
        "        G_und = bipartite.random_graph(n_top, n_bottom, p=creation_prob, directed=True)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def hierarchical(n_nodes, degree=5, graph_level=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 1\n",
        "        set_random_seed(seed)\n",
        "        prob = float(degree) / (n_nodes - 1)\n",
        "        B = np.tril((np.random.rand(n_nodes, n_nodes) < prob).astype(float), k=-1)\n",
        "        point = sample(range(n_nodes - 1), graph_level - 1)\n",
        "        point.sort()\n",
        "        point = [0] + [x + 1 for x in point] + [n_nodes]\n",
        "        for i in range(graph_level):\n",
        "            B[point[i]:point[i + 1], point[i]:point[i + 1]] = 0\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def low_rank(n_nodes, degree=1, rank=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        B = DAG._low_rank_dag(n_nodes, degree, rank)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n"
      ],
      "metadata": {
        "id": "3wLfKnV9xeuO"
      },
      "id": "3wLfKnV9xeuO",
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Test 1__"
      ],
      "metadata": {
        "id": "NHZzCK6k3AjO"
      },
      "id": "NHZzCK6k3AjO"
    },
    {
      "cell_type": "code",
      "source": [
        "method = 'linear'\n",
        "sem_type = 'gauss'\n",
        "nodes = range(6,12,3)\n",
        "edges = range(10,20,5)\n",
        "T=200\n",
        "num_datasets = 120\n",
        "File_PATH = '../Test/Examples/Test_data/'\n",
        "noise_scale = 1.0\n",
        "_ts = Generate_Synthetic_Data(File_PATH, num_datasets, T, method, sem_type, nodes, edges, noise_scale)"
      ],
      "metadata": {
        "id": "5SOGom2KyzXn"
      },
      "id": "5SOGom2KyzXn",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ts.genarate_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrSmdm9Y2Tqt",
        "outputId": "bb4de430-0f84-4188-bcb9-62a5a15edd26"
      },
      "id": "GrSmdm9Y2Tqt",
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Generating LinearSEM_GaussNoise Dataset!\n",
            "ANM-NCPOP INFO: LinearSEM_GaussNoise_6Nodes_10Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: LinearSEM_GaussNoise_6Nodes_15Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: LinearSEM_GaussNoise_9Nodes_10Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: LinearSEM_GaussNoise_9Nodes_15Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: 4 datasets are generated!\n",
            "ANM-NCPOP INFO: Finished LinearSEM_GaussNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_LinearSEM_GaussNoise/Datasets_LinearSEM_GaussNoise/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Test 2__"
      ],
      "metadata": {
        "id": "2-GAiRKvcJjd"
      },
      "id": "2-GAiRKvcJjd"
    },
    {
      "cell_type": "code",
      "source": [
        "noise_type = {\n",
        "    'nonlinear': ['gp-add', 'mlp', 'mim', 'gp', 'quadratic'],\n",
        "    'linear':  ['gauss', 'exp', 'gumbel', 'uniform', 'logistic']\n",
        "}\n",
        "sem_type = ['linear', 'nonlinear']\n",
        "nodes = range(6,12,3)\n",
        "edges = range(10,20,5)\n",
        "T=200\n",
        "num_datasets = 120\n",
        "File_PATH = '../Test/Examples/Test_data/'\n",
        "noise_scale = 1.0\n",
        "\n",
        "for m in sem_type :\n",
        "  for s in noise_type[m]:\n",
        "    _ts = Generate_Synthetic_Data(File_PATH, num_datasets, T, m, s, nodes, edges, noise_scale)\n",
        "    print(File_PATH, num_datasets, T, m, s, nodes, edges, noise_scale)\n",
        "    _ts.genarate_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lmD_cfrDGRt",
        "outputId": "6b2cb105-cc13-48e4-80fc-d06fce06a4b2"
      },
      "id": "0lmD_cfrDGRt",
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../Test/Examples/Test_data/ 120 200 linear gauss range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished LinearSEM_GaussNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_LinearSEM_GaussNoise/Datasets_LinearSEM_GaussNoise/\n",
            "../Test/Examples/Test_data/ 120 200 linear exp range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished LinearSEM_ExpNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_LinearSEM_ExpNoise/Datasets_LinearSEM_ExpNoise/\n",
            "../Test/Examples/Test_data/ 120 200 linear gumbel range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished LinearSEM_GumbelNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_LinearSEM_GumbelNoise/Datasets_LinearSEM_GumbelNoise/\n",
            "../Test/Examples/Test_data/ 120 200 linear uniform range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished LinearSEM_UniformNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_LinearSEM_UniformNoise/Datasets_LinearSEM_UniformNoise/\n",
            "../Test/Examples/Test_data/ 120 200 linear logistic range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished LinearSEM_LogisticNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_LinearSEM_LogisticNoise/Datasets_LinearSEM_LogisticNoise/\n",
            "../Test/Examples/Test_data/ 120 200 nonlinear gp-add range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished NonlinearSEM_Gp-addNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_NonlinearSEM_Gp-addNoise/Datasets_NonlinearSEM_Gp-addNoise/\n",
            "../Test/Examples/Test_data/ 120 200 nonlinear mlp range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished NonlinearSEM_MlpNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_NonlinearSEM_MlpNoise/Datasets_NonlinearSEM_MlpNoise/\n",
            "../Test/Examples/Test_data/ 120 200 nonlinear mim range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished NonlinearSEM_MimNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_NonlinearSEM_MimNoise/Datasets_NonlinearSEM_MimNoise/\n",
            "../Test/Examples/Test_data/ 120 200 nonlinear gp range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Finished NonlinearSEM_GpNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_NonlinearSEM_GpNoise/Datasets_NonlinearSEM_GpNoise/\n",
            "../Test/Examples/Test_data/ 120 200 nonlinear quadratic range(6, 12, 3) range(10, 20, 5) 1.0\n",
            "ANM-NCPOP INFO: Created Datasets File!\n",
            "ANM-NCPOP INFO: Generating NonlinearSEM_QuadraticNoise Dataset!\n",
            "ANM-NCPOP INFO: NonlinearSEM_QuadraticNoise_6Nodes_10Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: NonlinearSEM_QuadraticNoise_6Nodes_15Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: NonlinearSEM_QuadraticNoise_9Nodes_10Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: NonlinearSEM_QuadraticNoise_9Nodes_15Edges_TS IS DONE!\n",
            "ANM-NCPOP INFO: 4 datasets are generated!\n",
            "ANM-NCPOP INFO: Finished NonlinearSEM_QuadraticNoise dataset generation, which can be found under route: ../Test/Examples/Test_data/Result_NonlinearSEM_QuadraticNoise/Datasets_NonlinearSEM_QuadraticNoise/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step-by-Step__"
      ],
      "metadata": {
        "id": "vodgfAwBcN9c"
      },
      "id": "vodgfAwBcN9c"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4QWDBxTnod3",
        "outputId": "3c36339e-142e-4650-f7fc-2f2aaf487cc9"
      },
      "id": "x4QWDBxTnod3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 1: Generate IID Timesets Class__"
      ],
      "metadata": {
        "id": "sA_RPz9EKbbV"
      },
      "id": "sA_RPz9EKbbV"
    },
    {
      "cell_type": "code",
      "source": [
        "# from Generate_SyntheticData import*\n",
        "from itertools import combinations\n",
        "# from BuiltinDataSet import DAG\n",
        "from pickle import TRUE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import logging\n",
        "import tarfile\n",
        "import os\n",
        "import re\n",
        "# from castle.datasets.simulator import DAG\n",
        "\n",
        "\n",
        "class Generate_SyntheticData(object):\n",
        "    '''\n",
        "    Simulate IID datasets for causal structure learning.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W: np.ndarray\n",
        "        Weighted adjacency matrix for the target causal graph.\n",
        "    n: int\n",
        "        Number of samples for standard trainning dataset.\n",
        "    T: int\n",
        "        Number of timeseries for standard trainning dataset.\n",
        "    method: str, (linear or nonlinear), default='linear'\n",
        "        Distribution for standard trainning dataset.\n",
        "    sem_type: str\n",
        "        gauss, exp, gumbel, uniform, logistic (linear);\n",
        "        mlp, mim, gp, gp-add, quadratic (nonlinear).\n",
        "    noise_scale: float\n",
        "        Scale parameter of noise distribution in linear SEM.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, W, n=1000, T=500, method='linear',\n",
        "                 sem_type='gauss', noise_scale=1.0):\n",
        "\n",
        "        self.B = (W != 0).astype(int)\n",
        "        if method == 'linear':\n",
        "            self.XX = Generate_SyntheticData._simulate_linear_sem(\n",
        "                    W, n, T, sem_type, noise_scale)\n",
        "        elif method == 'nonlinear':\n",
        "            self.XX = Generate_SyntheticData._simulate_nonlinear_sem(\n",
        "                    W, n, T, sem_type, noise_scale)\n",
        "        logging.info('Finished synthetic dataset')\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_linear_sem(W, n, T, sem_type, noise_scale):\n",
        "        \"\"\"\n",
        "        Simulate samples from linear SEM with specified type of noise.\n",
        "        For uniform, noise z ~ uniform(-a, a), where a = noise_scale.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        W: np.ndarray\n",
        "            [d, d] weighted adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples, n=inf mimics population risk.\n",
        "        T: int\n",
        "        Number of timeseries for standard trainning dataset.\n",
        "        sem_type: str\n",
        "            gauss, exp, gumbel, uniform, logistic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        XX: np.ndarray\n",
        "            [T, n, d] sample matrix, [d, d] if n and T=inf\n",
        "        \"\"\"\n",
        "        def _simulate_single_equation(X, w, scale):\n",
        "            \"\"\"X: [n, num of parents], w: [num of parents], x: [n]\"\"\"\n",
        "            if sem_type == 'gauss':\n",
        "                z = np.random.normal(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'exp':\n",
        "                z = np.random.exponential(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'gumbel':\n",
        "                z = np.random.gumbel(scale=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'uniform':\n",
        "                z = np.random.uniform(low=-scale, high=scale, size=T)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'logistic':\n",
        "                x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a linear model, \\\n",
        "                                 the options are as follows: gauss, exp, \\\n",
        "                                 gumbel, uniform, logistic.')\n",
        "            return x\n",
        "\n",
        "        d = W.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "        G_nx =  nx.from_numpy_array(W, create_using=nx.DiGraph)\n",
        "        if not nx.is_directed_acyclic_graph(G_nx):\n",
        "            raise ValueError('W must be a DAG')\n",
        "        if np.isinf(T):  # population risk for linear gauss SEM\n",
        "            if sem_type == 'gauss':\n",
        "                # make 1/d X'X = true cov\n",
        "                X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)\n",
        "                return X\n",
        "            else:\n",
        "                raise ValueError('population risk not available')\n",
        "        # empirical risk\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        X = np.zeros([T, d])\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j])\n",
        "        XX = np.zeros((d, n, T))\n",
        "        for nn in range(n):\n",
        "            X_trans = np.transpose(X)\n",
        "            for dd in range(d):\n",
        "                XX[dd, nn, :] = list(X_trans[dd])\n",
        "        return XX\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_nonlinear_sem(W, n, T, sem_type, noise_scale):\n",
        "        \"\"\"\n",
        "        Simulate samples from nonlinear SEM.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        B: np.ndarray\n",
        "            [d, d] binary adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples.\n",
        "        T: int\n",
        "            Number of time.\n",
        "        sem_type: str\n",
        "            mlp, mim, gp, gp-add, or quadratic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        XX: np.ndarray\n",
        "            [n, d] sample matrix\n",
        "        \"\"\"\n",
        "        if sem_type == 'quadratic':\n",
        "            return _simulate_quad_sem(W, n, noise_scale)\n",
        "\n",
        "        def _simulate_single_equation(X, scale):\n",
        "            \"\"\"X: [n, num of parents], x: [n]\"\"\"\n",
        "            z = np.random.normal(scale=scale, size=n)\n",
        "            pa_size = X.shape[1]\n",
        "            if pa_size == 0:\n",
        "                return z\n",
        "            if sem_type == 'mlp':\n",
        "                hidden = 100\n",
        "                W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
        "                W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
        "                W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
        "                W2[np.random.rand(hidden) < 0.5] *= -1\n",
        "                x = sigmoid(X @ W1) @ W2 + z\n",
        "            elif sem_type == 'mim':\n",
        "                w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w1[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w2[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w3[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
        "            elif sem_type == 'gp':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = gp.sample_y(X, random_state=None).flatten() + z\n",
        "            elif sem_type == 'gp-add':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
        "                        for i in range(X.shape[1])]) + z\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a nonlinear model, \\\n",
        "                                 the options are as follows: mlp, mim, \\\n",
        "                                 gp, gp-add, or quadratic.')\n",
        "            return x\n",
        "\n",
        "        B = (W != 0).astype(int)\n",
        "        d = B.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "\n",
        "        X = np.zeros([n, d])\n",
        "        G_nx = nx.DiGraph(B)\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], scale_vec[j])\n",
        "        return X\n",
        "\n",
        "        XX = np.zeros((d, n, T))\n",
        "        for nn in range(n):\n",
        "            X_trans = np.transpose(X)\n",
        "            for dd in range(d):\n",
        "                XX[dd, :, T] = list(X_trans[dd])\n",
        "        return XX\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_quad_sem(W, n, noise_scale):\n",
        "        \"\"\"\n",
        "        Simulate samples from SEM with specified type of noise.\n",
        "        Coefficient is randomly drawn but specifically designed\n",
        "        to avoid overflow issues.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        W: np.ndarray\n",
        "            weigthed DAG.\n",
        "        n: int\n",
        "            Number of samples.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        X: np.ndarray\n",
        "            [n,d] sample matrix\n",
        "        \"\"\"\n",
        "        def generate_quadratic_coef(random_zero=True):\n",
        "            if random_zero and np.random.randint(low=0, high=2):\n",
        "                return 0\n",
        "            else:\n",
        "                coef = np.random.uniform(low=0.5, high=1)\n",
        "                if np.random.randint(low=0, high=2):\n",
        "                    coef *= -1\n",
        "                return coef\n",
        "\n",
        "        G = nx.DiGraph(W)\n",
        "        d = W.shape[0]\n",
        "        X = np.zeros([n, d])\n",
        "        ordered_vertices = list(nx.topological_sort(G))\n",
        "        assert len(ordered_vertices) == d\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G.predecessors(j))\n",
        "\n",
        "            if len(parents) == 0:\n",
        "                eta = np.zeros([n])\n",
        "            elif len(parents) == 1:\n",
        "                # We don't generate random zero coefficient if there is only one parent\n",
        "                eta = np.zeros([n])\n",
        "                used_parents = set()\n",
        "                p = parents[0]\n",
        "                num_terms = 0\n",
        "\n",
        "                # Linear term\n",
        "                coef = generate_quadratic_coef(random_zero=False)\n",
        "                if coef != 0:\n",
        "                    eta += coef * X[:, p]\n",
        "                    used_parents.add(p)\n",
        "                    num_terms += 1\n",
        "\n",
        "                # Squared term\n",
        "                coef = generate_quadratic_coef(random_zero=False)\n",
        "                if coef != 0:\n",
        "                    eta += coef * np.square(X[:, p])\n",
        "                    used_parents.add(p)\n",
        "                    num_terms += 1\n",
        "\n",
        "                if num_terms > 0:\n",
        "                    eta /= num_terms    # Compute average\n",
        "\n",
        "                # Remove parent if both coef is zero\n",
        "                if p not in used_parents:\n",
        "                    W[p, j] = 0\n",
        "            else:    # More than 1 parent\n",
        "                eta = np.zeros([n])\n",
        "                used_parents = set()\n",
        "                num_terms = 0\n",
        "\n",
        "                for p in parents:\n",
        "                    # Linear terms\n",
        "                    coef = generate_quadratic_coef(random_zero=True)\n",
        "                    if coef > 0:\n",
        "                        eta += coef * X[:, p]\n",
        "                        used_parents.add(p)\n",
        "                        num_terms += 1\n",
        "\n",
        "                    # Squared terms\n",
        "                    coef = generate_quadratic_coef(random_zero=True)\n",
        "                    if coef > 0:\n",
        "                        eta += coef * np.square(X[:, p])\n",
        "                        used_parents.add(p)\n",
        "                        num_terms += 1\n",
        "\n",
        "                # Cross terms\n",
        "                for p1, p2 in combinations(parents, 2):\n",
        "                    coef = generate_quadratic_coef(random_zero=True)\n",
        "                    if coef > 0:\n",
        "                        eta += coef * X[:, p1] * X[:, p2]\n",
        "                        used_parents.add(p1)\n",
        "                        used_parents.add(p2)\n",
        "                        num_terms += 1\n",
        "\n",
        "                if num_terms > 0:\n",
        "                    eta /= num_terms    # Compute average\n",
        "\n",
        "                # Remove parent if both coef is zero\n",
        "                unused_parents = set(parents) - used_parents\n",
        "                if p in unused_parents:\n",
        "                    W[p, j] = 0\n",
        "\n",
        "            X[:, j] = eta + np.random.normal(scale=noise_scale, size=n)\n",
        "\n",
        "        return X\n",
        "\n"
      ],
      "metadata": {
        "id": "G60ottpWH29N"
      },
      "id": "G60ottpWH29N",
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 2: Generate DAG Class__"
      ],
      "metadata": {
        "id": "96ax3fmLkwGG"
      },
      "id": "96ax3fmLkwGG"
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "\n",
        "# Huawei Technologies Co., Ltd.\n",
        "#\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Copyright (c) Xun Zheng (https://github.com/xunzheng/notears)\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import random\n",
        "from random import sample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from itertools import combinations\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "class DAG(object):\n",
        "    '''\n",
        "    A class for simulating random (causal) DAG, where any DAG generator\n",
        "    method would return the weighed/binary adjacency matrix of a DAG.\n",
        "    Besides, we recommend using the python package \"NetworkX\"\n",
        "    to create more structures types.\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_permutation(M):\n",
        "        # np.random.permutation permutes first axis only\n",
        "        P = np.random.permutation(np.eye(M.shape[0]))\n",
        "        return P.T @ M @ P\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_acyclic_orientation(B_und):\n",
        "        B = np.tril(DAG._random_permutation(B_und), k=-1)\n",
        "        B_perm = DAG._random_permutation(B)\n",
        "        return B_perm\n",
        "\n",
        "    @staticmethod\n",
        "    def _graph_to_adjmat(G):\n",
        "        return nx.to_numpy_array(G)\n",
        "\n",
        "    @staticmethod\n",
        "    def _BtoW(B, d, w_range):\n",
        "        U = np.random.uniform(low=w_range[0], high=w_range[1], size=[d, d])\n",
        "        U[np.random.rand(d, d) < 0.5] *= -1\n",
        "        W = (B != 0).astype(float) * U\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def _low_rank_dag(d, degree, rank):\n",
        "        \"\"\"\n",
        "        Simulate random low rank DAG with some expected degree.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        d: int\n",
        "            Number of nodes.\n",
        "        degree: int\n",
        "            Expected node degree, in + out.\n",
        "        rank: int\n",
        "            Maximum rank (rank < d-1).\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        B: np.nparray\n",
        "            Initialize DAG.\n",
        "        \"\"\"\n",
        "        prob = float(degree) / (d - 1)\n",
        "        B = np.triu((np.random.rand(d, d) < prob).astype(float), k=1)\n",
        "        total_edge_num = np.sum(B == 1)\n",
        "        sampled_pa = sample(range(d - 1), rank)\n",
        "        sampled_pa.sort(reverse=True)\n",
        "        sampled_ch = []\n",
        "        for i in sampled_pa:\n",
        "            candidate = set(range(i + 1, d))\n",
        "            candidate = candidate - set(sampled_ch)\n",
        "            sampled_ch.append(sample(candidate, 1)[0])\n",
        "            B[i, sampled_ch[-1]] = 1\n",
        "        remaining_pa = list(set(range(d)) - set(sampled_pa))\n",
        "        remaining_ch = list(set(range(d)) - set(sampled_ch))\n",
        "        B[np.ix_(remaining_pa, remaining_ch)] = 0\n",
        "        after_matching_edge_num = np.sum(B == 1)\n",
        "\n",
        "        # delta = total_edge_num - after_matching_edge_num\n",
        "        # mask B\n",
        "        maskedB = B + np.tril(np.ones((d, d)))\n",
        "        maskedB[np.ix_(remaining_pa, remaining_ch)] = 1\n",
        "        B[maskedB == 0] = 1\n",
        "\n",
        "        remaining_ch_set = set([i + d for i in remaining_ch])\n",
        "        sampled_ch_set = set([i + d for i in sampled_ch])\n",
        "        remaining_pa_set = set(remaining_pa)\n",
        "        sampled_pa_set = set(sampled_pa)\n",
        "\n",
        "        edges = np.transpose(np.nonzero(B))\n",
        "        edges[:, 1] += d\n",
        "        bigraph = nx.Graph()\n",
        "        bigraph.add_nodes_from(range(2 * d))\n",
        "        bigraph.add_edges_from(edges)\n",
        "        M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "        while len(M) > 2 * rank:\n",
        "            keys = set(M.keys())\n",
        "            rmv_cand = keys & (remaining_pa_set | remaining_ch_set)\n",
        "            p = sample(rmv_cand, 1)[0]\n",
        "            c = M[p]\n",
        "            # destroy p-c\n",
        "            bigraph.remove_edge(p, c)\n",
        "            M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "\n",
        "        new_edges = np.array(bigraph.edges)\n",
        "        for i in range(len(new_edges)):\n",
        "            new_edges[i,].sort()\n",
        "        new_edges[:, 1] -= d\n",
        "\n",
        "        BB = np.zeros((d, d))\n",
        "        B = np.zeros((d, d))\n",
        "        BB[new_edges[:, 0], new_edges[:, 1]] = 1\n",
        "\n",
        "        if np.sum(BB == 1) > total_edge_num:\n",
        "            delta = total_edge_num - rank\n",
        "            BB[sampled_pa, sampled_ch] = 0\n",
        "            rmv_cand_edges = np.transpose(np.nonzero(BB))\n",
        "            if delta <= 0:\n",
        "                raise RuntimeError(r'Number of edges is below the rank, please \\\n",
        "                                   set a larger edge or degree \\\n",
        "                                   (you can change seed or increase degree).')\n",
        "            selected = np.array(sample(rmv_cand_edges.tolist(), delta))\n",
        "            B[selected[:, 0], selected[:, 1]] = 1\n",
        "            B[sampled_pa, sampled_ch] = 1\n",
        "        else:\n",
        "            B = deepcopy(BB)\n",
        "\n",
        "        B = B.transpose()\n",
        "        return B\n",
        "\n",
        "    @staticmethod\n",
        "    def erdos_renyi(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Erdos-Renyi\n",
        "        creation_prob = (2 * n_edges) / (n_nodes ** 2)\n",
        "        G_und = nx.erdos_renyi_graph(n=n_nodes, p=creation_prob, seed=seed)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_free(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert (n_nodes > 0 and n_edges >= n_nodes and n_edges < n_nodes * n_nodes)\n",
        "        set_random_seed(seed)\n",
        "        # Scale-free, Barabasi-Albert\n",
        "        m = int(round(n_edges / n_nodes))\n",
        "        G_und = nx.barabasi_albert_graph(n=n_nodes, m=m)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def bipartite(n_nodes, n_edges, split_ratio = 0.2, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
        "        n_top = int(split_ratio * n_nodes)\n",
        "        n_bottom = n_nodes -  n_top\n",
        "        creation_prob = n_edges/(n_top*n_bottom)\n",
        "        G_und = bipartite.random_graph(n_top, n_bottom, p=creation_prob, directed=True)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def hierarchical(n_nodes, degree=5, graph_level=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 1\n",
        "        set_random_seed(seed)\n",
        "        prob = float(degree) / (n_nodes - 1)\n",
        "        B = np.tril((np.random.rand(n_nodes, n_nodes) < prob).astype(float), k=-1)\n",
        "        point = sample(range(n_nodes - 1), graph_level - 1)\n",
        "        point.sort()\n",
        "        point = [0] + [x + 1 for x in point] + [n_nodes]\n",
        "        for i in range(graph_level):\n",
        "            B[point[i]:point[i + 1], point[i]:point[i + 1]] = 0\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def low_rank(n_nodes, degree=1, rank=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        B = DAG._low_rank_dag(n_nodes, degree, rank)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Topology(object):\n",
        "    \"\"\"\n",
        "    A class for generating some classical (undirected) network structures,\n",
        "    in which any graph generator method would return the adjacency matrix of\n",
        "    a network structure.\n",
        "    In fact, we recommend to directly use the python package \"NetworkX\"\n",
        "    to create various structures you need.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def erdos_renyi(n_nodes, n_edges, seed=None):\n",
        "        \"\"\"\n",
        "        Generate topology matrix\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes : int, greater than 0\n",
        "            The number of nodes.\n",
        "        n_edges : int, greater than 0\n",
        "            Use to calculate probability for edge creation.\n",
        "        seed : integer, random_state, or None (default)\n",
        "            Indicator of random number generation state.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        B: np.matrix\n",
        "        \"\"\"\n",
        "        assert n_nodes > 0, 'The number of nodes must be greater than 0.'\n",
        "        creation_prob = (2*n_edges)/(n_nodes**2)\n",
        "        G = nx.erdos_renyi_graph(n=n_nodes, p=creation_prob, seed=seed)\n",
        "        B = nx.to_numpy_array(G)\n",
        "        return B\n",
        "\n",
        "\n",
        "class THPSimulation(object):\n",
        "    \"\"\"\n",
        "    A class for simulating event sequences with\n",
        "    THP (Topological Hawkes Process) setting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    causal_matrix: np.matrix\n",
        "        The casual matrix.\n",
        "    topology_matrix: np.matrix\n",
        "        Interpreted as an adjacency matrix to generate graph.\n",
        "        Has two dimension, should be square.\n",
        "    mu_range: tuple, default=(0.00005, 0.0001)\n",
        "    alpha_range: tuple, default=(0.005, 0.007)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, causal_matrix, topology_matrix,\n",
        "                 mu_range=(0.00005, 0.0001), alpha_range=(0.005, 0.007)):\n",
        "\n",
        "        assert (isinstance(causal_matrix, np.ndarray) and\n",
        "                causal_matrix.ndim == 2 and\n",
        "                causal_matrix.shape[0] == causal_matrix.shape[1]),\\\n",
        "            'casual_matrix should be np.matrix object, two dimension, square.'\n",
        "        assert (isinstance(topology_matrix, np.ndarray) and\n",
        "                topology_matrix.ndim == 2 and\n",
        "                topology_matrix.shape[0] == topology_matrix.shape[1]),\\\n",
        "            'topology_matrix should be np.matrix object, two dimension, square.'\n",
        "\n",
        "        self._causal_matrix = (causal_matrix != 0).astype(int)\n",
        "\n",
        "        self._topo = nx.from_numpy_array(topology_matrix,\n",
        "                                          create_using=nx.Graph)\n",
        "\n",
        "        self._mu_range = mu_range\n",
        "        self._alpha_range = alpha_range\n",
        "\n",
        "    def simulate(self, T, max_hop=1, beta=10):\n",
        "        \"\"\"\n",
        "        Generate simulation data.\n",
        "        \"\"\"\n",
        "        N = self._causal_matrix.shape[0]\n",
        "\n",
        "        mu = np.random.uniform(*self._mu_range, N)\n",
        "\n",
        "        alpha = np.random.uniform(*self._alpha_range, [N, N])\n",
        "        alpha = alpha * self._causal_matrix\n",
        "        alpha = np.ones([max_hop+1, N, N]) * alpha\n",
        "\n",
        "        immigrant_events = dict()\n",
        "        for node in self._topo.nodes:\n",
        "            immigrant_events[node] = self._trigger_events(mu, 0, T, beta)\n",
        "\n",
        "        base_events = immigrant_events.copy()\n",
        "        events = immigrant_events.copy()\n",
        "        while sum(map(len, base_events.values())) != 0:\n",
        "            offspring_events = dict()\n",
        "            for node in tqdm(self._topo.nodes):\n",
        "                offspring_events[node] = []\n",
        "                for k in range(max_hop+1):\n",
        "                    k_base_events = []\n",
        "                    for neighbor in self._get_k_hop_neighbors(\n",
        "                            self._topo, node, k):\n",
        "                        k_base_events += base_events[neighbor]\n",
        "                    k_new_events = [self._trigger_events(\n",
        "                        alpha[k, i], start_time, duration, beta)\n",
        "                        for (i, start_time, duration) in k_base_events]\n",
        "                    for event_group in k_new_events:\n",
        "                        offspring_events[node] += event_group\n",
        "                events[node] += offspring_events[node]\n",
        "            base_events = offspring_events\n",
        "\n",
        "        Xn_list = []\n",
        "        for node, event_group in events.items():\n",
        "            Xn = pd.DataFrame(event_group,\n",
        "                              columns=['event', 'timestamp', 'duration'])\n",
        "            Xn.insert(0, 'node', node)\n",
        "            Xn_list.append(Xn.reindex(columns=['event', 'timestamp', 'node']))\n",
        "        X = pd.concat(Xn_list, sort=False, ignore_index=True)\n",
        "        return X\n",
        "\n",
        "    @staticmethod\n",
        "    def _trigger_events(intensity_vec, start_time, duration, beta):\n",
        "\n",
        "        events = []\n",
        "        for i, intensity in enumerate(intensity_vec):\n",
        "            if intensity:\n",
        "                trigger_time = start_time\n",
        "                while True:\n",
        "                    trigger_time = round(trigger_time + np.random.exponential(\n",
        "                        1 / intensity))\n",
        "                    if trigger_time > start_time + duration:\n",
        "                        break\n",
        "                    sub_duration = (np.max((0, np.random.exponential(beta)))).round()\n",
        "                    events.append((i, trigger_time, sub_duration))\n",
        "        return events\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_k_hop_neighbors(G, node, k):\n",
        "        if k == 0:\n",
        "            return {node}\n",
        "        else:\n",
        "            return (set(nx.single_source_dijkstra_path_length(G, node, k).keys())\n",
        "                    - set(nx.single_source_dijkstra_path_length(\n",
        "                        G, node, k - 1).keys()))\n"
      ],
      "metadata": {
        "id": "JS4_BSdSvrhy"
      },
      "id": "JS4_BSdSvrhy",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Step 3: Test Generated Time Series__"
      ],
      "metadata": {
        "id": "5iM4IO2NB3fW"
      },
      "id": "5iM4IO2NB3fW"
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ############################################################################################################\n",
        "    #########################################  Test ###########################\n",
        "    ############################################################################################################\n",
        "\n",
        "    method = 'linear'\n",
        "    sem_type = 'gauss'\n",
        "    num_nodes = 9\n",
        "    num_edges = 20\n",
        "    num_datasets = 120\n",
        "    T=500\n",
        "    # Weighted adjacency matrix for the target causal graph\n",
        "    '''\n",
        "    true_graph_matrix = DAG.erdos_renyi(n_nodes=10, n_edges=10)\n",
        "    topology_matrix = Topology.erdos_renyi(n_nodes=20, n_edges=20)\n",
        "    simulator = THPSimulation(true_graph_matrix, topology_matrix,\n",
        "                                  mu_range=(0.00005, 0.0001),\n",
        "                                  alpha_range=(0.005, 0.007))\n",
        "    _data = simulator.simulate(T=25000, max_hop=2)'''\n",
        "\n",
        "    # replaced 'to_numpy_matrix' with 'to_numpy_array'\n",
        "    weighted_random_dag = DAG.erdos_renyi(n_nodes=num_nodes, n_edges=num_edges, seed=1)\n",
        "    # _simulate_linear_sem(W =weighted_random_dag, n = num_datasets, sem_type = 'gauss', noise_scale=1.0)\n",
        "    dataset = Generate_SyntheticData(W=weighted_random_dag, n=num_datasets, T=T, method=method, sem_type=sem_type)\n",
        "    true_dag, data = dataset.B, dataset.XX\n",
        "    file_name = method.capitalize()+sem_type.capitalize()+'_'+str(num_nodes)+'_'+str(num_edges)+'_TS'\n",
        "    np.savez(file_name+'.npz', x=dataset.XX, y=dataset.B)\n",
        "    print('INFO: Check for '+file_name + '!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yUOG5uTBfj-",
        "outputId": "15c87fbb-b7da-4e2b-d1db-77352327be80"
      },
      "id": "7yUOG5uTBfj-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Check for LinearGauss_9_20_TS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    ############################################################################################################\n",
        "    ############### Methodï¼Œ SEM Type, Numbers of nodes, Numbers of edges, Numbers of datasets and T ###########\n",
        "    ############################################################################################################\n",
        "\n",
        "    sem_type = {\n",
        "        'nonlinear': ['gp-add','mlp', 'mim', 'gp', 'quadratic'],\n",
        "        'linear':  ['gauss', 'exp', 'gumbel', 'uniform', 'logistic']\n",
        "    }\n",
        "\n",
        "    # Creat DataFrame\n",
        "    sem_type = pd.DataFrame(sem_type)\n",
        "\n",
        "    T=500\n",
        "    count = 0\n",
        "    nodenum = range(6,10,3)\n",
        "    edgenum = range(10,16,5)\n",
        "    for m in ['nonlinear', 'linear']:\n",
        "      for s in sem_type[m]:\n",
        "        for n in nodenum:\n",
        "          for e in edgenum:\n",
        "            count = count +1\n",
        "            weighted_random_dag = DAG.erdos_renyi(n_nodes=n, n_edges=e, weight_range=(0.5, 2.0), seed=1)\n",
        "            dataset = Generate_SyntheticData(W=weighted_random_dag, n=120, T=T, method=m, sem_type=s)\n",
        "            true_dag, XX = dataset.B, dataset.XX\n",
        "            file_name = m.capitalize()+s.capitalize()+'_'+str(n)+'Nodes_'+str(e)+'Edges_TS'\n",
        "            Save_path = \"/content/drive/MyDrive/Colab Notebooks/NCPOP/Causal_Models_Learning/Test/Datasets/Synthetic datasets/Generate_SyntheticData/\"\n",
        "            np.savez(Save_path+file_name+'.npz', x=XX , y=true_dag)\n",
        "            print('INFO: Check for '+file_name + '!')\n",
        "    print('In total, '+str(count) + ' datasets are generated!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhgQ6ucqnHZ1",
        "outputId": "2df9000e-b71a-4819-8610-be603a8ff830"
      },
      "id": "MhgQ6ucqnHZ1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Check for NonlinearGp-add_6Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearGp-add_6Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearGp-add_9Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearGp-add_9Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearMlp_6Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearMlp_6Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearMlp_9Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearMlp_9Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearMim_6Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearMim_6Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearMim_9Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearMim_9Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearGp_6Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearGp_6Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearGp_9Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearGp_9Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearQuadratic_6Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearQuadratic_6Nodes_15Edges_TS!\n",
            "INFO: Check for NonlinearQuadratic_9Nodes_10Edges_TS!\n",
            "INFO: Check for NonlinearQuadratic_9Nodes_15Edges_TS!\n",
            "INFO: Check for LinearGauss_6Nodes_10Edges_TS!\n",
            "INFO: Check for LinearGauss_6Nodes_15Edges_TS!\n",
            "INFO: Check for LinearGauss_9Nodes_10Edges_TS!\n",
            "INFO: Check for LinearGauss_9Nodes_15Edges_TS!\n",
            "INFO: Check for LinearExp_6Nodes_10Edges_TS!\n",
            "INFO: Check for LinearExp_6Nodes_15Edges_TS!\n",
            "INFO: Check for LinearExp_9Nodes_10Edges_TS!\n",
            "INFO: Check for LinearExp_9Nodes_15Edges_TS!\n",
            "INFO: Check for LinearGumbel_6Nodes_10Edges_TS!\n",
            "INFO: Check for LinearGumbel_6Nodes_15Edges_TS!\n",
            "INFO: Check for LinearGumbel_9Nodes_10Edges_TS!\n",
            "INFO: Check for LinearGumbel_9Nodes_15Edges_TS!\n",
            "INFO: Check for LinearUniform_6Nodes_10Edges_TS!\n",
            "INFO: Check for LinearUniform_6Nodes_15Edges_TS!\n",
            "INFO: Check for LinearUniform_9Nodes_10Edges_TS!\n",
            "INFO: Check for LinearUniform_9Nodes_15Edges_TS!\n",
            "INFO: Check for LinearLogistic_6Nodes_10Edges_TS!\n",
            "INFO: Check for LinearLogistic_6Nodes_15Edges_TS!\n",
            "INFO: Check for LinearLogistic_9Nodes_10Edges_TS!\n",
            "INFO: Check for LinearLogistic_9Nodes_15Edges_TS!\n",
            "In total 40 datasets are generated!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backup"
      ],
      "metadata": {
        "id": "K7JkC-59r-zc"
      },
      "id": "K7JkC-59r-zc"
    },
    {
      "cell_type": "code",
      "source": [
        "File_PATH = \"Test/Datasets/Real_data/Telephone/\"\n",
        "file_name = 'Telephone'\n",
        "\n",
        "File_PATH = \"Test/Datasets/Real_data/Microwave/\"\n",
        "file_name = '25V_474N_Microwave'\n",
        "\n",
        "Krebs_Cycle\n",
        "File_PATH = \"Test/Datasets/Synthetic_data/Kreb_Cycles/\"\n",
        "file_name = 'Krebs_Cycle'\n",
        "\n",
        "dt = Real_Data_Standardization(File_PATH, file_name)\n",
        "dt.Produce_Rawdata()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "w6IHLIvpyXfe",
        "outputId": "72c015f4-20e4-4059-cc88-d4075e1d20b1"
      },
      "id": "w6IHLIvpyXfe",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Krebs_Cycle' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-e754d4f1d98b>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'25V_474N_Microwave'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mKrebs_Cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mFile_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Test/Datasets/Synthetic_data/Kreb_Cycles/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Krebs_Cycle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Krebs_Cycle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=6, n_edges=15, weight_range=(0.5, 2.0), seed=1)\n",
        "print(weighted_random_dag)\n",
        "pd.DataFrame(weighted_random_dag).to_csv('weighted_random_dag_6_15.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAct2oRAZA5c",
        "outputId": "e9696bb3-867c-4c94-d046-6ad9a63ad5f7"
      },
      "id": "CAct2oRAZA5c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.         -0.         -1.50461906 -0.         -1.76946638 -0.        ]\n",
            " [ 1.28682224 -0.          0.84436582 -1.30162086  1.87094304  1.18580721]\n",
            " [-0.          0.          0.          0.          1.70413626 -0.        ]\n",
            " [-0.          0.          1.74372036 -0.          0.90957496  0.        ]\n",
            " [-0.         -0.          0.         -0.          0.          0.        ]\n",
            " [-0.7131802   0.         -1.11880826  0.          1.43604498 -0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Generate IID Datasets__"
      ],
      "metadata": {
        "id": "lKADz6ei4QIu"
      },
      "id": "lKADz6ei4QIu"
    },
    {
      "cell_type": "code",
      "source": [
        "from castle.datasets import DAG, IIDSimulation\n",
        "\n",
        "sem_type = {\n",
        "    'nonlinear': ['gp-add','mlp', 'mim', 'gp', 'quadratic'],\n",
        "    'linear':  ['gauss', 'exp', 'gumbel', 'uniform', 'logistic']\n",
        "}\n",
        "\n",
        "# creat DataFrame\n",
        "sem_type = pd.DataFrame(sem_type)\n",
        "\n",
        "DataSize = range(5,45,5)\n",
        "nodenum = range(3,18,3)\n",
        "edgenum = range(5,20,5)\n",
        "for m in ['nonlinear','linear']:\n",
        "  for s in sem_type[m]:\n",
        "    for n in nodenum:\n",
        "      for e in edgenum:\n",
        "        weighted_random_dag = DAG.erdos_renyi(n_nodes=n, n_edges=e, weight_range=(0.5, 2.0), seed=1)\n",
        "        dataset = IIDSimulation(W=weighted_random_dag, n=100, method=m, sem_type=s)\n",
        "        true_dag, X = dataset.B, dataset.X\n",
        "        # save numpy to npz file\n",
        "        Save_path = \"/content/drive/MyDrive/Colab Notebooks/NCPOP/Causal_Models_Learning/Test/Datasets/Synthetic datasets/Generate_SyntheticData/\"\n",
        "        sname = Save_path+ m+s + '_'+str(n)+'_'+str(e)\n",
        "        np.savez(sname+'.npz', x=X , y=true_dag)\n",
        "        print('INFO: Check for '+sname + '!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxehPfTPwm9G",
        "outputId": "02c2c7b5-cf43-4101-da2b-22604558eba3"
      },
      "id": "LxehPfTPwm9G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 120, 500) (16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Test_GenerateIID__"
      ],
      "metadata": {
        "id": "G4zutmN9UG7d"
      },
      "id": "G4zutmN9UG7d"
    },
    {
      "cell_type": "code",
      "source": [
        "from BuiltinDataSet import*\n",
        "from pickle import TRUE\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import logging\n",
        "import tarfile\n",
        "import os\n",
        "import re\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=6, n_edges=15, seed=1)\n",
        "dataset = IIDSimulation(W=weighted_random_dag, n=500, method='linear', sem_type='gauss')\n",
        "true_dag, data = dataset.B, dataset.X\n",
        "np.savez('./Test_Causality_Datasets/Synthetic datasets/linearGauss_6_15.npz', x=true_dag, y=data)\n"
      ],
      "metadata": {
        "id": "W7yG5LFUB2IA"
      },
      "id": "W7yG5LFUB2IA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Generate LDS Timesets*__"
      ],
      "metadata": {
        "id": "unFbc5fsTaZR"
      },
      "id": "unFbc5fsTaZR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaa85b05",
      "metadata": {
        "id": "aaa85b05"
      },
      "outputs": [],
      "source": [
        "from inputlds import*\n",
        "import numpy as np\n",
        "\n",
        "class DataGenerate(object):\n",
        "    \"\"\"Generator based on NCPOP Regressor\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Kozdoba, Mark and Marecek, Jakub and Tchrakian, Tigran and Mannor, Shie,\n",
        "    \"On-line learning of linear dynamical systems: Exponential forgetting in kalman filters\",\n",
        "    In Proceedings of the AAAI Conference on Artificial Intelligence, 2019\n",
        "\n",
        "    Zhou, Quan and Marecek, Jakub,\n",
        "    \"Proper Learning of Linear Dynamical Systems as a Non-Commutative Polynomial Optimisation Problem\",\n",
        "    arXiv, 2020\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(DataGenerate, self).__init__()\n",
        "\n",
        "    def data_generation(self, g, f_dash, proc_noise_std, obs_noise_std, T):\n",
        "        '''\n",
        "        Generate the T*len(f_dash) time series data from Linear dynamical system with proc_noise and obs_noise\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        g: Hidden state parameter\n",
        "        f_dash: Observation state parameter\n",
        "        proc_noise_std: Hidden state noise\n",
        "        obs_noise_std: Observation state noise\n",
        "        T: Time\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list: T*len(f_dash) list\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> from inputlds import*\n",
        "        >>> import numpy as np\n",
        "        >>> T=10\n",
        "        >>> g = np.matrix([[0.8,0,0],[0,0.9,0],[0,0,0.1]])\n",
        "        >>> f_dash = np.matrix([[1.0,0.5,0.3],[0.1,0.1,0.1]])\n",
        "        >>> proc_noise_std=0.01\n",
        "        >>> obs_noise_std=0.01\n",
        "        >>> ANM_NCPOP_DataGenerate().data_generation(g,f_dash,proc_noise_std,obs_noise_std,T)\n",
        "\n",
        "        '''\n",
        "\n",
        "        n=len(g)\n",
        "        m=len(f_dash)\n",
        "        ds1 = dynamical_system(g,np.zeros((n,m)),f_dash,np.zeros((m,m)),\n",
        "                process_noise='gaussian',\n",
        "                observation_noise='gaussian',\n",
        "                process_noise_std=proc_noise_std,\n",
        "                observation_noise_std=obs_noise_std)\n",
        "        inputs = np.zeros((m,T))\n",
        "        h0=np.ones(ds1.d) # initial state\n",
        "        ds1.solve(h0=h0, inputs=inputs, T=T)\n",
        "        return np.asarray(ds1.outputs).reshape(T,m).tolist()\n",
        "\n",
        "\n",
        "    def data_generation_dim(self, m, n, proc_noise_std,obs_noise_std,T):\n",
        "        '''\n",
        "        Generate the T*m time series data from Linear dynamical system with proc_noise and obs_noise\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n: Hidden state dimension\n",
        "        m: Observation state dimension\n",
        "        proc_noise_std: Hidden state noise\n",
        "        obs_noise_std: Observation state noise\n",
        "        T: Time\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        list:  T*m list\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> from inputlds import*\n",
        "        >>> import numpy as np\n",
        "        >>> n=3\n",
        "        >>> m=2\n",
        "        >>> T=20\n",
        "        >>> proc_noise_std=0.01\n",
        "        >>> obs_noise_std=0.01\n",
        "        >>> ANM_NCPOP_DataGenerate().data_generation(m, n, proc_noise_std, obs_noise_std, T)\n",
        "        '''\n",
        "\n",
        "        g = np.random.randint(0, 2, (n,n))\n",
        "        f_dash = np.random.randint(0, 2, (m,n))\n",
        "        ds1 = dynamical_system(g,np.zeros((n,m)),f_dash,np.zeros((m,m)),\n",
        "                process_noise='gaussian',\n",
        "                observation_noise='gaussian',\n",
        "                process_noise_std=proc_noise_std,\n",
        "                observation_noise_std=obs_noise_std)\n",
        "        inputs = np.zeros((m,T))\n",
        "        h0=np.ones(ds1.d) # initial state\n",
        "        ds1.solve(h0=h0, inputs=inputs, T=T)\n",
        "        return np.asarray(ds1.outputs).reshape(T,m).tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Data Generation class"
      ],
      "metadata": {
        "id": "u3m9eeel_sEw"
      },
      "id": "u3m9eeel_sEw"
    },
    {
      "cell_type": "code",
      "source": [
        "from inputlds import*\n",
        "import numpy as np\n",
        "T=10\n",
        "g = np.matrix([[0.8,0,0],[0,0.9,0],[0,0,0.1]])\n",
        "f_dash = np.matrix([[1.0,0.5,0.3],[0.1,0.1,0.1]])\n",
        "proc_noise_std=0.01\n",
        "obs_noise_std=0.01\n",
        "my_array = DataGenerate().data_generation(g,f_dash,proc_noise_std,obs_noise_std,T)\n",
        "my_array\n",
        "df = pd.DataFrame(my_array)\n",
        "df.to_csv('lds_data.csv', index=False, header=['col1','col2'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWTC2X612H0a",
        "outputId": "87adbf58-0b20-4078-d1e7-04a8ca8a9865"
      },
      "id": "SWTC2X612H0a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.2747847483078838, 0.19594359308281661],\n",
              " [1.0454801175607462, 0.14571383137678165],\n",
              " [0.859624443447321, 0.11136022132381274],\n",
              " [0.7522937590854241, 0.10262556381907625],\n",
              " [0.6384014976018458, 0.10143255711088069],\n",
              " [0.4983139644085479, 0.0856117047216572],\n",
              " [0.4550613076727763, 0.0647366171990469],\n",
              " [0.3835754692208594, 0.04253721891070902],\n",
              " [0.33822278060577826, 0.044561053378408044],\n",
              " [0.2729466435786535, 0.048692423439118775]]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from inputlds import*\n",
        "import numpy as np\n",
        "n=3\n",
        "m=2\n",
        "T=10\n",
        "proc_noise_std=0.01\n",
        "obs_noise_std=0.01\n",
        "DataGenerate().data_generation_dim(m,n,proc_noise_std,obs_noise_std,T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUf1dCa-4LIT",
        "outputId": "8eb3f7f5-9538-4edd-f0ab-f52c2691a9dc"
      },
      "id": "JUf1dCa-4LIT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0014143954478208506, 3.00304475804575],\n",
              " [-0.00860099022621434, 3.966124614146327],\n",
              " [-0.007635469446289098, 4.934599419454813],\n",
              " [0.01215965486252258, 5.9442183250672675],\n",
              " [0.010714011287707595, 6.904107192953694],\n",
              " [-0.007857632146403469, 7.87991343574115],\n",
              " [0.018804761747808166, 8.92670669775614],\n",
              " [-0.0023952497348262093, 9.929117447988316],\n",
              " [0.009015414863869095, 10.938080588526418],\n",
              " [0.009820536471639578, 11.921019820155285]]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}