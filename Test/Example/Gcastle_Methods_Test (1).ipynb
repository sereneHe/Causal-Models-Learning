{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#_gCastle Tool Colab Notebook Template_\n",
        "\n",
        "## Introduction\n",
        "[gCastle](https://github.com/huawei-noah/trustworthyAI/tree/f96c9b52d5e38c74c14969dbf6c48b380f3c3dab/gcastle) is a causal structure learning(CSL) tool chain developed by Huawei Noah's Ark Lab. The package contains various functionalities related to causal learning and evaluation, including:\n",
        "![castle](https://drive.google.com/uc?id=1MwURFTgB6hJvsM3-YhsGOMyz5xs6vMJ3)\n",
        "\n",
        "## Evaluation:\n",
        "* Various commonly used metrics for causal structure learning, including F1, SHD, FDR, TPR, FDR, NNZ, etc.\n",
        "\n",
        "* Heatmap(est_graph, truth_graph)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dZqdD13QyBoY"
      },
      "id": "dZqdD13QyBoY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 1: Get start__\n",
        "\n",
        "\n",
        "* Mounting drive\n",
        "* Setting environment\n",
        "* Installing packages"
      ],
      "metadata": {
        "id": "U55kxvMLEr-C"
      },
      "id": "U55kxvMLEr-C"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/NCPOP/Causal_Models_Learning/Test/Results/TwoDim/Gcastle_Results/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbeoURBnEl0D",
        "outputId": "bf5cb008-27ab-4d31-ee4b-0d159d5ed50f"
      },
      "id": "ZbeoURBnEl0D",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### for colab ###\n",
        "# To execute the notebook directly in colab make sure your MOSEK license file is in one the locations\n",
        "#\n",
        "# /content/mosek.lic   or   /root/mosek/mosek.lic\n",
        "#\n",
        "# inside this notebook's internal filesystem.\n",
        "# Install MOSEK and ncpol2sdpa if not already installed\n",
        "\n",
        "import os\n",
        "os.environ['MOSEKLM_LICENSE_FILE']=\"/content/drive/MyDrive/Colab Notebooks/NCPOP/\""
      ],
      "metadata": {
        "id": "EchioM6GvBHG"
      },
      "id": "EchioM6GvBHG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gcastle==1.0.3rc2\n",
        "!pip install networkx==2.5.1\n",
        "!pip install mosek torch\n",
        "!pip install ncpol2sdpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azCYJy_LFQDj",
        "outputId": "e8230d28-68ea-4865-d6c7-7eddfbe4b9ff"
      },
      "id": "azCYJy_LFQDj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gcastle==1.0.3rc2\n",
            "  Downloading gcastle-1.0.3rc2-py3-none-any.whl (187 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/187.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/187.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (3.3)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (4.66.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->gcastle==1.0.3rc2) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->gcastle==1.0.3rc2) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.1->gcastle==1.0.3rc2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.1->gcastle==1.0.3rc2) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.16.0)\n",
            "Installing collected packages: gcastle\n",
            "Successfully installed gcastle-1.0.3rc2\n",
            "Collecting networkx==2.5.1\n",
            "  Downloading networkx-2.5.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from networkx==2.5.1) (4.4.2)\n",
            "Installing collected packages: networkx\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.2.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nccl-cu12==2.19.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.2.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed networkx-2.5.1\n",
            "Collecting mosek\n",
            "  Downloading Mosek-10.1.31-cp37-abi3-manylinux2014_x86_64.whl (14.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mosek) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (2.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.10/dist-packages (from networkx->torch) (4.4.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mosek, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed mosek-10.1.31 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting ncpol2sdpa\n",
            "  Downloading ncpol2sdpa-1.12.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sympy>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from ncpol2sdpa) (1.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ncpol2sdpa) (1.25.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=0.7.2->ncpol2sdpa) (1.3.0)\n",
            "Building wheels for collected packages: ncpol2sdpa\n",
            "  Building wheel for ncpol2sdpa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ncpol2sdpa: filename=ncpol2sdpa-1.12.2-py3-none-any.whl size=70795 sha256=2e1b1b5ea3c3c2b931b1cbc904fd2edb789af4991a1044a8a0c0fe2fc54873a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/d7/fe/ab61f3bf30a350feab3bb4dccd63932d56cbbd32b9ec0d94fa\n",
            "Successfully built ncpol2sdpa\n",
            "Installing collected packages: ncpol2sdpa\n",
            "Successfully installed ncpol2sdpa-1.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 2: Input training data__\n",
        "* Real data\n",
        "\n",
        "* Synthetic data"
      ],
      "metadata": {
        "id": "TIt_YZEMY7d_"
      },
      "id": "TIt_YZEMY7d_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read LinearGauss_6_15 data"
      ],
      "metadata": {
        "id": "PXLFWzA_Hrx_"
      },
      "id": "PXLFWzA_Hrx_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7621b15d",
      "metadata": {
        "id": "7621b15d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb2ad23-5e9c-42f1-caa0-f54cf6c7c519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8.665e-01  1.924e+00 -6.816e+00 -2.305e+00 -7.145e+00  2.090e+00]\n",
            " [ 6.982e-01  9.911e-01 -3.660e+00 -9.425e-01 -6.728e+00  8.597e-01]\n",
            " [-5.116e-01  3.806e-01  9.820e-01  4.091e-01  3.640e+00  7.874e-01]\n",
            " [ 1.044e-01 -7.478e-01  4.407e+00  1.593e+00  6.593e+00 -1.035e+00]\n",
            " [-1.457e+00 -1.318e+00  5.959e+00  3.081e+00  1.121e+01 -2.001e-01]\n",
            " [ 1.280e+00  1.333e+00 -9.019e+00 -2.400e+00 -1.121e+01  3.284e+00]\n",
            " [-9.025e-01 -6.845e-01  3.859e-01 -4.006e-01 -7.040e-01 -8.001e-02]\n",
            " [ 2.201e+00  1.256e+00 -2.151e+00  2.831e-01 -3.003e+00  4.081e-01]\n",
            " [-3.571e+00  4.967e-01  2.041e+00  2.796e-01  1.547e+01  3.617e+00]\n",
            " [ 7.809e-01  3.293e-01 -3.973e+00 -1.747e+00 -9.098e+00 -2.628e-01]\n",
            " [-7.380e-01 -1.190e-03  1.918e+00 -3.664e-01  4.376e+00 -1.207e+00]\n",
            " [-5.062e-01 -3.073e-01  5.626e+00  1.642e+00  1.080e+01 -1.089e+00]\n",
            " [-4.404e-01 -1.835e-01 -5.955e-01 -4.853e-01 -1.015e-01  2.859e-01]\n",
            " [-2.338e+00 -5.530e-01  6.507e+00  1.918e+00  1.652e+01 -6.782e-01]\n",
            " [-1.221e+00 -1.557e-01 -1.292e+00 -1.105e+00 -7.790e-01 -4.086e-01]\n",
            " [-2.133e-01  5.724e-01 -5.787e+00 -1.609e+00 -7.160e+00  1.838e+00]\n",
            " [-1.478e-01  7.600e-01 -4.737e+00 -1.520e+00 -4.665e+00  3.029e+00]\n",
            " [ 1.130e+00  7.357e-01  5.466e-01  2.792e-01 -5.294e-01 -1.861e-01]\n",
            " [-8.424e-01  1.014e+00 -1.054e+00 -2.949e-01  3.311e+00  1.469e+00]\n",
            " [ 8.497e-01 -7.384e-01  1.105e+00  3.290e-01 -5.409e+00 -2.479e+00]\n",
            " [-2.242e-01  2.577e-01 -1.813e-01 -1.619e-01  1.520e+00  3.317e-01]\n",
            " [-2.139e+00 -1.137e+00  5.893e+00  1.377e+00  1.233e+01 -2.831e-01]\n",
            " [-1.079e-01  1.578e+00 -3.685e+00 -7.683e-01  1.656e+00  3.303e+00]\n",
            " [ 8.475e-01 -1.751e-01 -2.467e+00 -5.805e-01 -5.620e+00  1.162e-01]\n",
            " [-1.795e+00 -1.729e+00  4.254e+00  1.474e+00  8.109e+00 -6.696e-01]\n",
            " [-8.688e-01 -2.441e-01 -1.401e-01  5.910e-01  2.949e+00  8.365e-01]\n",
            " [ 1.149e+00  1.931e-01 -3.416e+00 -2.750e-01 -4.968e+00  1.535e+00]\n",
            " [ 1.351e+00  1.248e+00 -2.062e+00  3.906e-01 -1.385e+00  7.600e-01]\n",
            " [ 7.750e-01 -4.932e-02 -6.368e-01  9.047e-02 -3.355e+00 -1.056e+00]\n",
            " [ 1.494e-02 -7.703e-01  6.624e+00  2.951e+00  1.082e+01 -1.107e+00]\n",
            " [-3.459e+00 -7.741e-01  8.975e+00  2.824e+00  2.257e+01  1.087e+00]\n",
            " [ 9.297e-01  6.100e-01 -5.378e+00 -8.489e-01 -7.856e+00  9.485e-01]\n",
            " [-4.027e-01 -1.547e+00  1.053e+00  1.489e-01 -2.818e+00 -1.842e+00]\n",
            " [-1.835e+00 -2.048e-01 -1.466e+00 -1.572e+00  1.024e+00  9.988e-01]\n",
            " [-2.664e+00 -1.468e+00  6.072e+00  1.785e+00  1.269e+01 -1.356e-01]] [[0 0 1 0 1 0]\n",
            " [1 0 1 1 1 1]\n",
            " [0 0 0 0 1 0]\n",
            " [0 0 1 0 1 0]\n",
            " [0 0 0 0 0 0]\n",
            " [1 0 1 0 1 0]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "File_PATH = \"/content/drive/MyDrive/Colab Notebooks/NCPOP/Causal_Models_Learning/Test/Examples/Test_data/\"\n",
        "# file_name = 'Krebs_Cycle_16_43_TS'\n",
        "\n",
        "file_name = 'linearGauss_6_15'\n",
        "# file_name = 'linearGauss_10_20'\n",
        "# file_name = 'NonlinearGauss_6_15'\n",
        "# file_name = 'NonlinearGauss_10_20'\n",
        "data = np.load(File_PATH+file_name+'.npz')\n",
        "X = data['x']\n",
        "true_dag = data['y']\n",
        "print(X, true_dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __Step 3: Methods__"
      ],
      "metadata": {
        "id": "TNjNht4eLNeI"
      },
      "id": "TNjNht4eLNeI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IID/Constraint-based"
      ],
      "metadata": {
        "id": "SpWGD1W9HW3q"
      },
      "id": "SpWGD1W9HW3q"
    },
    {
      "cell_type": "markdown",
      "id": "892f21e3",
      "metadata": {
        "id": "892f21e3"
      },
      "source": [
        "##__M 1.1: PC_variant_stable__\n",
        "* A classic causal discovery algorithm based on conditional independence tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a14c05",
      "metadata": {
        "scrolled": true,
        "id": "44a14c05",
        "outputId": "2269beaf-4f4b-4a22-aef2-6061a850097f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApMAAAEjCAYAAABuNIoVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyfUlEQVR4nO3de3QU9f3/8VcSyQYhWcIt4RKJIOqXaxQkAl5rNLUURVEpXgjR4lcMFszxRisk1EpQlKYFFPUrWo9S8EotChYjl68liIaDRSwgChLRhCCQhSgJJvP7w1/22zUJ2Vmyu7OfPB/nzDnmw1w+M8m+fM/MZ2eiLMuyBAAAAAQgOtwdAAAAQOSimAQAAEDAKCYBAAAQMIpJAAAABIxiEgAAAAGjmAQAAEDAKCYBAAAQMIpJAAAABIxiEgAAAAGjmETEmThxotq3bx/ubgCAUaKiojRlypRwdwMRiGLScBs2bFB+fr4OHz4c7q4AQFCRd0B4UEwabsOGDZo1axbhCsB45B0QHhSTCIoffvhBNTU14e4GADRQV1enY8eOhbsbAamqqgp3F4AGKCYdbN++fbr11luVlJQkl8ul/v37a/HixT7zzJ8/X/3799epp56qxMREDR06VEuWLJEk5efn695775UknX766YqKilJUVJT27Nnjdx9eeeUV9evXT3FxcRowYIDeeOMNTZw4Uampqd559uzZo6ioKD322GMqLCxUnz595HK59Omnn6qmpkYzZ87UkCFD5Ha71a5dO1144YVas2aNz3b+cx1//OMf1atXL7Vt21YXX3yxPvnkkyaPz5gxY9S+fXt16dJF99xzj2pra/3eNwDmOFHe1Y8FfOmll9S/f3+5XC6tWrVKa9euVVRUlNauXeuzrvplnn/+eZ/27du367rrrlPHjh0VFxenoUOH6s0337Td1++//16/+c1v1LlzZ8XHx+uqq67Svn37FBUVpfz8fJ99ioqK0qeffqobb7xRiYmJuuCCCyRJ//rXvzRx4kT17t1bcXFxSk5O1q233qpvv/22wXGJiorS9u3bdcMNNyghIUGdOnXS1KlTmyyoly9frgEDBnj/v7Nq1Srb+4jW5ZRwdwCNKy8v1/nnn+8NwS5dumjlypW67bbb5PF4NG3aND3zzDP6zW9+o+uuu84bDP/617/0wQcf6MYbb9S1116rnTt36q9//av++Mc/qnPnzpKkLl26+NWHt956S+PGjdPAgQNVUFCgQ4cO6bbbblOPHj0anf+5557TsWPHdPvtt8vlcqljx47yeDz6n//5H40fP16TJk3SkSNH9OyzzyozM1ObNm1SWlqazzpeeOEFHTlyRDk5OTp27Jj+9Kc/6Wc/+5m2bt2qpKQk73y1tbXKzMxUenq6HnvsMb377rt6/PHH1adPH02ePDmwgw4gYjWXd++9955efvllTZkyRZ07d1Zqaqqt2+Hbtm3TyJEj1aNHDz3wwANq166dXn75ZY0ZM0avvfaarrnmGr/XNXHiRL388su65ZZbdP7552vdunUaNWpUk/Nff/316tu3r2bPni3LsiRJq1ev1hdffKHs7GwlJydr27Ztevrpp7Vt2zZt3LhRUVFRPuu44YYblJqaqoKCAm3cuFF//vOfdejQIb3wwgs+873//vt6/fXXdeeddyo+Pl5//vOfNXbsWO3du1edOnXyex/RylhwpNtuu83q1q2bdeDAAZ/2X/3qV5bb7ba+++476+qrr7b69+9/wvXMnTvXkmTt3r3bdh8GDhxo9ezZ0zpy5Ii3be3atZYkq1evXt623bt3W5KshIQEa//+/T7r+OGHH6zq6mqftkOHDllJSUnWrbfe2mAdbdu2tb766itv+wcffGBJsu6++25vW1ZWliXJ+v3vf++z3nPOOccaMmSI7f0EYIam8k6SFR0dbW3bts2nfc2aNZYka82aNT7t9Xn03HPPedsuu+wya+DAgdaxY8e8bXV1ddaIESOsvn37+t3HkpISS5I1bdo0n/aJEydakqy8vDxvW15eniXJGj9+fIP1fPfddw3a/vrXv1qSrPXr1zdYx1VXXeUz75133mlJsj7++GNvmyQrNjbW2rVrl7ft448/tiRZ8+fP93sf0fpwm9uBLMvSa6+9ptGjR8uyLB04cMA7ZWZmqrKyUps3b1aHDh301Vdf6cMPP2zxPnz99dfaunWrJkyY4PMYnosvvlgDBw5sdJmxY8c2uOoZExOj2NhYST+OUzp48KB++OEHDR06VJs3b26wjjFjxvhc+Rw2bJjS09P19ttvN5j3jjvu8Pn5wgsv1BdffOH/TgJoNS6++GL169cvoGUPHjyo9957TzfccIOOHDnizeNvv/1WmZmZ+uyzz7Rv3z6/1lV/y/jOO+/0ab/rrruaXOanWSdJbdu29f73sWPHdODAAZ1//vmS1Gi25uTkNLq9n2ZrRkaG+vTp4/150KBBSkhIIFtxQhSTDlRRUaHDhw/r6aefVpcuXXym7OxsSdL+/ft1//33q3379ho2bJj69u2rnJwc/fOf/2yRPnz55ZeSpDPOOKPBvzXWJv04Tqkxf/nLXzRo0CDFxcWpU6dO6tKli9566y1VVlY2mLdv374N2s4888wG4zzj4uIaFK6JiYk6dOhQo30A0Lo1lU/+2LVrlyzL0owZMxpkcl5enqQfM9kfX375paKjoxv0p6lcbarvBw8e1NSpU5WUlKS2bduqS5cu3vn8ydY+ffooOjq6QbaedtppDZYlW9Ecxkw6UF1dnSTp5ptvVlZWVqPzDBo0SF27dtWOHTu0YsUKrVq1Sq+99pqeeOIJzZw5U7NmzQpllyX5ninXe/HFFzVx4kSNGTNG9957r7p27aqYmBgVFBTo888/D3hbMTExJ9NVAK1MY/n003GF9X76Rb76TL7nnnuUmZnZ6DInKgZPVmN9v+GGG7Rhwwbde++9SktLU/v27VVXV6ef//zn3v6eSFP73lS2Wv9/rCbQGIpJB+rSpYvi4+NVW1urjIyME87brl07jRs3TuPGjVNNTY2uvfZaPfzww5o+fbri4uKaDIzm9OrVS9KPZ+Q/1VhbU1599VX17t1br7/+uk9f6s/mf+qzzz5r0LZz506fb48DQGPs5l1iYqIkNfgiTv2dmXq9e/eWJLVp06bZTG5Or169VFdXp927d/tcLbSTq4cOHVJRUZFmzZqlmTNnetsby8///Lf/vMK5a9cu1dXVka1oEdzmdqCYmBiNHTtWr732WqOPxamoqJCkBo+AiI2NVb9+/WRZlo4fPy7px2JTahiWzenevbsGDBigF154QUePHvW2r1u3Tlu3brW1L5LvWe0HH3yg4uLiRudfvny5z9ijTZs26YMPPtCVV15pq/8AWh+7ederVy/FxMRo/fr1Pu1PPPGEz89du3bVJZdcoqeeekrffPNNg/XUZ7I/6q9s/nQb8+fP93sdjeWqJBUWFja5zMKFCxvdHtmKlsCVSYeaM2eO1qxZo/T0dE2aNEn9+vXTwYMHtXnzZr377rs6ePCgrrjiCiUnJ2vkyJFKSkrSv//9by1YsECjRo1SfHy8JGnIkCGSpN/97nf61a9+pTZt2mj06NHe0D2R2bNn6+qrr9bIkSOVnZ2tQ4cOacGCBRowYIBPgXkiv/zlL/X666/rmmuu0ahRo7R7924tWrRI/fr1a3QdZ5xxhi644AJNnjxZ1dXVKiwsVKdOnXTffffZOHoAWqOm8q4pbrdb119/vebPn6+oqCj16dNHK1asaHT848KFC3XBBRdo4MCBmjRpknr37q3y8nIVFxfrq6++0scff+x3H8eOHavCwkJ9++233kcD7dy5U5J/V1cTEhJ00UUX6dFHH9Xx48fVo0cP/eMf/9Du3bubXGb37t266qqr9POf/1zFxcV68cUXdeONN2rw4MF+9Rs4oTB+kxzNKC8vt3JycqyUlBSrTZs2VnJysnXZZZdZTz/9tGVZlvXUU09ZF110kdWpUyfL5XJZffr0se69916rsrLSZz0PPfSQ1aNHDys6Otr2Y4KWLl1qnX322ZbL5bIGDBhgvfnmm9bYsWOts88+2ztP/WM05s6d22D5uro6a/bs2VavXr0sl8tlnXPOOdaKFSusrKysRh8vNHfuXOvxxx+3UlJSLJfLZV144YU+j66wrB8fDdSuXbsG26p/BAaA1quxvJNk5eTkNDp/RUWFNXbsWOvUU0+1EhMTrf/+7/+2PvnkkwaPBrIsy/r888+tCRMmWMnJyVabNm2sHj16WL/85S+tV1991VYfq6qqrJycHKtjx45W+/btrTFjxlg7duywJFlz5szxzlefaRUVFQ3W8dVXX1nXXHON1aFDB8vtdlvXX3+99fXXXzf5eKFPP/3Uuu6666z4+HgrMTHRmjJlivX999/7rLOp49SrVy8rKyvL1j6idYmyLEbVwp60tDR16dJFq1evbrF17tmzR6effrrmzp2re+65p8XWCwCRYMuWLTrnnHP04osv6qabbmqx9ebn52vWrFmqqKjwPsgdaGmMmUSTjh8/rh9++MGnbe3atfr44491ySWXhKdTMNb69es1evRode/eXVFRUVq+fHmzy6xdu1bnnnuuXC6XzjjjjAavvwOc6Pvvv2/QVlhYqOjoaF100UVh6BFMEa4cZcxkK1RZWdlomP2n5ORk7du3TxkZGbr55pvVvXt3bd++XYsWLVJycnKjD9EFTkZVVZUGDx6sW2+9Vddee22z8+/evVujRo3SHXfcoZdeeklFRUX69a9/rW7dujX5+BYgmMrKyk74723btpXb7dajjz6qkpISXXrppTrllFO0cuVKrVy5UrfffrtSUlJC1FuYKGw5Gu777Ai9+tcRnmiyLMs6fPiwdcMNN1g9evSwYmNjrcTEROu6667zedVWSznRuEu0PpKsN95444Tz3HfffQ1eJzpu3DgrMzMziD0DmtZcrtaPO/zHP/5hjRw50kpMTLTatGlj9enTx8rPz7eOHz/e4n060bhLmC2UOcqVyVbovvvu080339zsfG63W8uWLQtBj6TU1FQeihsBjh07ppqaGr/ntyyrwbdTXS6XXC7XSfeluLi4wTP/MjMzNW3atJNeNxCI5saRd+/eXZJ0+eWX6/LLLw9Fl5Sfn6/8/PyQbAv+MTFHKSZboX79+gX8jlq0XseOHWv0TRwn0r59+waPgMrLy2uR/7mVlZUpKSnJpy0pKUkej0fff/+97b4CJ+tkH2gO85maoxSTAPxi50y63tGjR1VaWqqEhARvW0ucTQNAJDI1R0NeTNbV1enrr79WfHx8wK/6AxA4y7J05MgRde/eXdHR9h/oEBUV5ddn17IsWZalhIQEnxBsKcnJySovL/dpKy8vV0JCgvFXJclRILzIUV8hLya//vprvq0GOEBpaal69uxpezl/Q1Bq+Lq3ljR8+HC9/fbbPm2rV6/W8OHDg7ZNpyBHAWcgR38U8mKy/jV/P71k6wRutzvcXYgYlZWV4e5Co5z4O3TasfJ4PEpJSfF+Fu2Kjo72+4y6rq7O7/UePXpUu3bt8v68e/dubdmyRR07dtRpp52m6dOna9++fXrhhRckSXfccYcWLFig++67T7feeqvee+89vfzyy3rrrbfs71SEcXKOOpETc0FyXjbUc+LxctqxIkcbdjSkKisrLUkNXvnnBGrmsQ5Mvo8OcqJwH5dIOFaBfgbrl4uNjbVcLlezU2xsrK3trFmzptHjV/84laysLOviiy9usExaWpoVGxtr9e7du8Hr70zl5Bx1onBnQKRkQ71wH5dIOFbkqK+Qv07R4/HI7XarsrLScWfUjD3yX4j/bPzmxN+h045VoJ/B+uVcLpffZ9TV1dWO/KxHOifnqBM5MRck52VDPSceL6cdK3LUF9/mBmCLnbE+AICGTMtRikkAtpgWggAQaqblKMUkAFvsDBwHADRkWo5STAKwxbQzagAINdNylGISgC2mhSAAhJppOUoxCcAW00IQAELNtBylmARgi2khCAChZlqOUkwCsCU6Otqvd9HaeWsDALQmpuUoxSQAW/w9ozbprBsAWpJpOUoxCcAW00IQAELNtBylmARgi2khCAChZlqONn/DvhELFy5Uamqq4uLilJ6erk2bNrV0vwA4VH0I+jOhaeQo0HqZlqO2i8lly5YpNzdXeXl52rx5swYPHqzMzEzt378/GP0D4DCmhWA4kKNA62ZajtouJufNm6dJkyYpOztb/fr106JFi3Tqqadq8eLFwegfAIep/xaiPxMaR44CrZtpOWqrlzU1NSopKVFGRsb/rSA6WhkZGSouLm50merqank8Hp8JQOQy7Yw61MhRAKblqK1i8sCBA6qtrVVSUpJPe1JSksrKyhpdpqCgQG632zulpKQE3lsAYWdaCIYaOQrAtBwN+vXT6dOnq7Ky0juVlpYGe5MAgsi0EIwE5ChgFtNy1NajgTp37qyYmBiVl5f7tJeXlys5ObnRZVwul1wuV+A9BOAokTSOx4nIUQCm5aitPYmNjdWQIUNUVFTkbaurq1NRUZGGDx/e4p0D4DymDRwPNXIUgGk5avuh5bm5ucrKytLQoUM1bNgwFRYWqqqqStnZ2cHoHwCHMe1hu+FAjgKtm2k5aruYHDdunCoqKjRz5kyVlZUpLS1Nq1atajCYHICZTAvBcCBHgdbNtBwN6HWKU6ZM0ZQpU1q6LwAigGkhGC7kKNB6mZajvJsbgG2REnAA4FQm5SjFJABb/B0UbllWCHoDAJHHtBylmARgi2m3ZwAg1EzLUYpJALaYFoIAEGqm5SjFJABbYmJiFBMTE+5uAEDEMi1HKSYB2GLaWB8ACDXTcpRiEoAtpt2eAYBQMy1HKSYB2GJaCAJAqJmWoxSTAGwx7fYMAISaaTlKMQnAFtPOqAEg1EzLUYpJALaYdkYNAKFmWo5STP4HJ/7SnHpW4tR+OfF3aBrTzqgjmdvtDncXGuAz6D8+I/4z7ViZlqMUkwBsiYqK8uuMuq6uLgS9AYDIY1qONr8nAPAf6m/P+DPZtXDhQqWmpiouLk7p6enatGnTCecvLCzUWWedpbZt2yolJUV33323jh07FuiuAUBImJajXJkEYIu/AWc3BJctW6bc3FwtWrRI6enpKiwsVGZmpnbs2KGuXbs2mH/JkiV64IEHtHjxYo0YMUI7d+7UxIkTFRUVpXnz5tnaNgCEkmk5ypVJALbUj/XxZ7Jj3rx5mjRpkrKzs9WvXz8tWrRIp556qhYvXtzo/Bs2bNDIkSN14403KjU1VVdccYXGjx/f7Fk4AISbaTlKMQnAFru3Zzwej89UXV3dYJ01NTUqKSlRRkaGz3YyMjJUXFzcaD9GjBihkpISb+h98cUXevvtt/WLX/wiCHsNAC3HtBzlNjcAW+x+CzElJcWnPS8vT/n5+T5tBw4cUG1trZKSknzak5KStH379kbXf+ONN+rAgQO64IILZFmWfvjhB91xxx367W9/a2NvACD0TMtRikkAttgNwdLSUiUkJHjbXS5Xi/Rj7dq1mj17tp544gmlp6dr165dmjp1qh566CHNmDGjRbYBAMFgWo5STAKwxe7A8YSEBJ8QbEznzp0VExOj8vJyn/by8nIlJyc3usyMGTN0yy236Ne//rUkaeDAgaqqqtLtt9+u3/3udwF9CxIAQsG0HCVtAdgSjIHjsbGxGjJkiIqKirxtdXV1Kioq0vDhwxtd5rvvvmsQdDExMZJ4cDYAZzMtR7kyCcCWYD3SIjc3V1lZWRo6dKiGDRumwsJCVVVVKTs7W5I0YcIE9ejRQwUFBZKk0aNHa968eTrnnHO8t2dmzJih0aNHe8MQAJzItBylmARgS7BCcNy4caqoqNDMmTNVVlamtLQ0rVq1yjuYfO/evT7rfPDBBxUVFaUHH3xQ+/btU5cuXTR69Gg9/PDD9nYIAELMtByNskJ8P8jj8cjtdquysrLZ+/+InPdyOgW3N5sX6GewfrnLL79cbdq0aXb+48ePa/Xq1XzWg6D+d+FETvwMkqMIFnL0R1yZBGCL3W8hAgB8mZajFJMAbAnW7RkAaC1My1HbvVy/fr1Gjx6t7t27KyoqSsuXLw9CtwA4VbBeA9aakKNA62ZajtouJquqqjR48GAtXLgwGP0B4HB2XwOGhshRoHUzLUdt3+a+8sordeWVVwajLwAigGljfcKBHAVaN9NyNOhjJqurq31eSO7xeIK9SQBBZFoIRgJyFDCLaTka9OunBQUFcrvd3umnLysHEFlMG+sTCchRwCym5WjQi8np06ersrLSO5WWlgZ7kwCCyLQQjATkKGAW03I06Le5XS6XXC5XsDcDIERMe6RFJCBHAbOYlqM8ZxKALaaN9QGAUDMtR20Xk0ePHtWuXbu8P+/evVtbtmxRx44dddppp7Vo5wA4j2khGA7kKNC6mZajtovJjz76SJdeeqn359zcXElSVlaWnn/++RbrGABnMu32TDiQo0DrZlqO2i4mL7nkElmWFYy+AIgApp1RhwM5CrRupuUoYyYB2BYpAQcATmVSjlJMArDFtDNqAAg103KUYhKALaaFIACEmmk5SjEJwBbTQhAAQs20HKWYBGCLad9CBIBQMy1HKSYB2GLaGTUAhJppOUoxCcAW00IQAELNtBylmARgi2khCAChZlqOUkwCsMW0EASAUDMtRykmAdhiWggCQKiZlqMUkwBsMS0EASDUTMtRikmH4/299jjxg2fa79C0EIxklZWVSkhICHc3HM+0z2CwOfGz67TfocfjkdvtDnh503KUYhKALaaFIACEmmk5SjEJwBbTHrYLAKFmWo5STAKwxbQzagAINdNylGISgC2mhSAAhJppOUoxCcC2SAk4AHAqk3KUYhKALaadUQNAqJmWoxSTAGwxLQQBINRMy1GKSQC2mBaCABBqpuUoxSQAW0wLQQAINdNylGISgC2mhSAAhJppORoZT8ME4BgxMTF+T3YtXLhQqampiouLU3p6ujZt2nTC+Q8fPqycnBx169ZNLpdLZ555pt5+++1Adw0AQsK0HOXKJABbgnVGvWzZMuXm5mrRokVKT09XYWGhMjMztWPHDnXt2rXB/DU1Nbr88svVtWtXvfrqq+rRo4e+/PJLdejQwdZ2ASDUTMtRikkAtgQrBOfNm6dJkyYpOztbkrRo0SK99dZbWrx4sR544IEG8y9evFgHDx7Uhg0b1KZNG0lSamqqrW0CQDiYlqPc5gZgS30I+jNJksfj8Zmqq6sbrLOmpkYlJSXKyMjwtkVHRysjI0PFxcWN9uPNN9/U8OHDlZOTo6SkJA0YMECzZ89WbW1tcHYcAFqIaTlKMQnAFrshmJKSIrfb7Z0KCgoarPPAgQOqra1VUlKST3tSUpLKysoa7ccXX3yhV199VbW1tXr77bc1Y8YMPf744/rDH/7Q8jsNAC3ItBy1dZu7oKBAr7/+urZv3662bdtqxIgReuSRR3TWWWfZ2iiAyGX39kxpaakSEhK87S6Xq0X6UVdXp65du+rpp59WTEyMhgwZon379mnu3LnKy8trkW0EAzkKwLQctXVlct26dcrJydHGjRu1evVqHT9+XFdccYWqqqps7wCAyGT3jDohIcFnaiwEO3furJiYGJWXl/u0l5eXKzk5udF+dOvWTWeeeabPtx3/67/+S2VlZaqpqWnBPW5Z5CgA03LUVjG5atUqTZw4Uf3799fgwYP1/PPPa+/evSopKbGzGgARzG4I+iM2NlZDhgxRUVGRt62urk5FRUUaPnx4o8uMHDlSu3btUl1dnbdt586d6tatm2JjYwPfwSAjRwGYlqMnNWaysrJSktSxY8cm56murm4wcBRA5ApGCEpSbm6unnnmGf3lL3/Rv//9b02ePFlVVVXebyVOmDBB06dP984/efJkHTx4UFOnTtXOnTv11ltvafbs2crJyWnR/Q02chRofUzL0YAfDVRXV6dp06Zp5MiRGjBgQJPzFRQUaNasWYFuBoDDBOuRFuPGjVNFRYVmzpypsrIypaWladWqVd7B5Hv37lV09P+d/6akpOidd97R3XffrUGDBqlHjx6aOnWq7r//fns7FEbkKNA6mZajUZZlWbaW+P8mT56slStX6v3331fPnj2bnK+6utrnK+wej0cpKSmqrKz0GUwKtAS7H7xQCPAjFjQej0dut9v2Z7B+ud/+9reKi4trdv5jx45p9uzZfNZPgByFE5GjzSNHfQV0ZXLKlClasWKF1q9ff8IAlH78xlFLfesIQPgF64y6tSFHgdbLtBy1VUxalqW77rpLb7zxhtauXavTTz89WP0C4FCmhWCokaMATMtRW8VkTk6OlixZor/97W+Kj4/3PgTT7Xarbdu2QekgAGcxLQRDjRwFYFqO2vo295NPPqnKykpdcskl6tatm3datmxZsPoHwGGC9S3E1oIcBWBajtq+zQ2gdTPtjDrUyFEApuVowI8GAtA6mRaCABBqpuUoxSQAW0wLQQAINdNylGISgC2mhSAAhJppOUoxCcCWmJgYxcTE+DUfAKAh03KUYhKALaadUQNAqJmWoxSTAGwxLQQBINRMy1GKSQC2mBaCABBqpuUoxSQAW0wLQQAINdNylGISgG2REnAA4FQm5SjFJABbTDujBoBQMy1HKSYB2GJaCAJAqJmWoxSTDufUPyTeL9x6mRaCMJ9T/xbJ0dbLtBylmARgi2kP2wWAUDMtRykmAdhi2hk1AISaaTlKMQnAFtNCEABCzbQcpZgEYEt0dLSio6P9mg8A0JBpOUoxCcAW086oASDUTMtRikkAtpgWggAQaqblKMUkAFtMC0EACDXTcpRiEoAtpoUgAISaaTlKMQnAFtMGjgNAqJmWoxSTAGyJioryK+Ai5YwaAELNtBylmARgi2m3ZwAg1EzLUYpJALaYdnsGAELNtBylmARgi2ln1AAQaqblKMUkAFtMC0EACDXTctTW9dMnn3xSgwYNUkJCghISEjR8+HCtXLkyWH0D4ED1IejPhIbIUQCm5aitYrJnz56aM2eOSkpK9NFHH+lnP/uZrr76am3bti1Y/QPgMKaFYKiRowBMy1Fbt7lHjx7t8/PDDz+sJ598Uhs3blT//v1btGMAnMm0geOhRo4CMC1HAx4zWVtbq1deeUVVVVUaPnx4k/NVV1erurra+7PH4wl0kwAcwLSxPuFEjgKtk2k5aruY3Lp1q4YPH65jx46pffv2euONN9SvX78m5y8oKNCsWbNOqpMAnMO0M+pwIEeB1s20HLXdy7POOktbtmzRBx98oMmTJysrK0uffvppk/NPnz5dlZWV3qm0tPSkOgwgvOpD0J/JroULFyo1NVVxcXFKT0/Xpk2b/Fpu6dKlioqK0pgxY2xvMxzIUaB1My1HbfcyNjZWZ5xxhoYMGaKCggINHjxYf/rTn5qc3+Vyeb+1WD8BiFzBGji+bNky5ebmKi8vT5s3b9bgwYOVmZmp/fv3n3C5PXv26J577tGFF154MrsVUuQo0LqZlqMnff20rq7OZywPALMFKwTnzZunSZMmKTs7W/369dOiRYt06qmnavHixU0uU1tbq5tuukmzZs1S7969T3bXwoYcBVoX03LU1pjJ6dOn68orr9Rpp52mI0eOaMmSJVq7dq3eeeedgDYOIPLYHTj+0y+LuFwuuVwun7aamhqVlJRo+vTp3rbo6GhlZGSouLi4yW38/ve/V9euXXXbbbfpf//3f+3sRtiQowBMy1FbxeT+/fs1YcIEffPNN3K73Ro0aJDeeecdXX755QFtHEDksRuCKSkpPu15eXnKz8/3aTtw4IBqa2uVlJTk056UlKTt27c3uv73339fzz77rLZs2eJ/5x2AHAVgWo7aKiafffbZk9oYgMgXFRXl16Dw+hAsLS31GeP307PpQBw5ckS33HKLnnnmGXXu3Pmk1xdK5CgA03KUd3MDsMXuGbU/Xxjp3LmzYmJiVF5e7tNeXl6u5OTkBvN//vnn2rNnj88DwOvq6iRJp5xyinbs2KE+ffo020cACAfTcjQyHmAEwDGCMXA8NjZWQ4YMUVFRkbetrq5ORUVFjT7M++yzz9bWrVu1ZcsW73TVVVfp0ksv1ZYtWxrcEgIAJzEtR7kyCcCWYL25ITc3V1lZWRo6dKiGDRumwsJCVVVVKTs7W5I0YcIE9ejRQwUFBYqLi9OAAQN8lu/QoYMkNWgHAKcxLUcpJgHYEhMTo5iYGL/ms2PcuHGqqKjQzJkzVVZWprS0NK1atco7mHzv3r0R8zYIADgR03I0yrIsq8XXegIej0dut1uVlZU8eNcPTn0vZ4j/bPzmxOPltGMV6Gewfrm///3vateuXbPzV1VVafTo0XzWg4ActceJuSA5LxvqOfF4Oe1YkaO+uDIJwJZg3Z4BgNbCtBylmARgi2khCAChZlqOUkwCsMW0EASAUDMtRykmAdhiWggCQKiZlqMUkwBsMS0EASDUTMtRikkAtpgWggAQaqblKMUkAFtMC0EACDXTcpRi0uGc9mwtp+N4BZ9pIQjzkQv2cLyCz7QcpZgEYItpIQgAoWZajlJMArDFtBAEgFAzLUcpJgHYFikBBwBOZVKOUkwCsMW0M2oACDXTcpRiEoAtpoUgAISaaTlKMQnAFtNCEABCzbQcjQ53BwAAABC5uDIJwBbTzqgBINRMy1GKSQC2REdHKzq6+Zsa/swDAK2RaTlKMQnAFtPOqAEg1EzLUYpJALaYFoIAEGqm5SjFJABbTAtBAAg103KUYhKALaaFIACEmmk5SjEJwBbTQhAAQs20HD2prwnNmTNHUVFRmjZtWgt1BwBaF3IUQKQL+Mrkhx9+qKeeekqDBg1qyf4AcDjTzqjDiRwFWifTcjSgK5NHjx7VTTfdpGeeeUaJiYkt3ScADlYfgv5MaBo5CrRepuVoQMVkTk6ORo0apYyMjGbnra6ulsfj8ZkARK76h+36M6Fp5CjQepmWo7Zvcy9dulSbN2/Whx9+6Nf8BQUFmjVrlu2OAXAm027PhAM5CrRupuWorZK3tLRUU6dO1UsvvaS4uDi/lpk+fboqKyu9U2lpaUAdBeAMpt2eCTVyFIBpOWrrymRJSYn279+vc88919tWW1ur9evXa8GCBaqurlZMTIzPMi6XSy6Xq2V6CwARjhwFYBpbxeRll12mrVu3+rRlZ2fr7LPP1v33398gAAGYKVLOlp2IHAUgmZWjtorJ+Ph4DRgwwKetXbt26tSpU4N2AGYybaxPqJGjAEzL0cj4mhAAAAAc6aRfp7h27doW6AaASGHaGbUTkKNA62JajvJubgC2mBaCABBqpuUoxSQAW0wLQQAINdNylDGTAGwJ5vPRFi5cqNTUVMXFxSk9PV2bNm1qct5nnnlGF154oRITE5WYmKiMjIwTzg8ATmFajlJMArAlWCG4bNky5ebmKi8vT5s3b9bgwYOVmZmp/fv3Nzr/2rVrNX78eK1Zs0bFxcVKSUnRFVdcoX379rXEbgJA0JiWo1GWZVm2ljhJHo9HbrdblZWVSkhICOWmASjwz2D9cp988oni4+Obnf/IkSMaMGCA39tJT0/XeeedpwULFkiS6urqlJKSorvuuksPPPBAs8vX1tYqMTFRCxYs0IQJE5rfoQhGjgLhRY764sokAFvsnlF7PB6fqbq6usE6a2pqVFJSooyMDG9bdHS0MjIyVFxc7Fe/vvvuOx0/flwdO3ZsmR0FgCAxLUcpJgHYYjcEU1JS5Ha7vVNBQUGDdR44cEC1tbVKSkryaU9KSlJZWZlf/br//vvVvXt3nyAFACcyLUf5NjcAW+x+C7G0tNTn9kww3jE9Z84cLV26VGvXrlVcXFyLrx8AWpJpOUoxCSCoEhISmh3r07lzZ8XExKi8vNynvby8XMnJySdc9rHHHtOcOXP07rvvatCgQSfdXwBwGqfnKLe5AdgSjG8hxsbGasiQISoqKvK21dXVqaioSMOHD29yuUcffVQPPfSQVq1apaFDh57UfgFAqJiWo1yZBGBLsB62m5ubq6ysLA0dOlTDhg1TYWGhqqqqlJ2dLUmaMGGCevTo4R0r9Mgjj2jmzJlasmSJUlNTvWOC2rdvr/bt29vcKwAIHdNyNGzFpNvtDtemmxTipyQhCJz4tgDT/q6CFYLjxo1TRUWFZs6cqbKyMqWlpWnVqlXeweR79+5VdPT/3Ux58sknVVNTo+uuu85nPXl5ecrPz7e17UhFjvrHibkgOfNYSc48Xk49VoEyLUfD9pxJJzLtj7U1IgSbd7LPR/vss8/8fj5a3759eRZiEJCj9jgxFyRnHivJmcfLaceKHPXFbW4AtgTrjBoAWgvTcpQv4AAAACBgXJkEYItpZ9QAEGqm5SjFJABbTAtBAAg103KUYhKALaaFIACEmmk5yphJAAAABIwrkwBsi5SzZQBwKpNylGISgC2m3Z4BgFAzLUcpJgHYYloIAkComZajFJMAbDEtBAEg1EzLUb6AAwAAgIBxZRKALaadUQNAqJmWo1yZBAAAQMC4MgnAFtPOqAEg1EzLUVtXJvPz870HoH46++yzg9U3ADAOOQrANLavTPbv31/vvvvu/63gFC5uAq2JaWfU4UCOAq2baTlqO8FOOeUUJScn+z1/dXW1qqurvT97PB67mwTgIKaFYDiQo0DrZlqO2v4Czmeffabu3burd+/euummm7R3794Tzl9QUCC32+2dUlJSAu4sAJiAHAVgkijLsix/Z165cqWOHj2qs846S998841mzZqlffv26ZNPPlF8fHyjyzR2Ru3UILRxKOBQTjyLc9rflcfjkdvtVmVlpRISEmwv98033/i1nMfjUbdu3Wxvx3TkaOg5MRckZx4ryZnHy2nHihz1Zes295VXXun970GDBik9PV29evXSyy+/rNtuu63RZVwul1wu18n1EgAMQY4CMM1Jjfru0KGDzjzzTO3ataul+gPA4Uwb6xNu5CjQ+piWoyf10PKjR4/q888/V7du3VqqPwDQqpCjACKdrWLynnvu0bp167Rnzx5t2LBB11xzjWJiYjR+/Phg9Q+Aw/z0GYknmtAQOQrAtBy1dZv7q6++0vjx4/Xtt9+qS5cuuuCCC7Rx40Z16dIlWP0DAKOQowBMY6uYXLp0abD6AQCtAjkKwDS8dgGALaYNHAeAUDMtRykmAdhiWggCQKiZlqMn9W1uAAAAtG5cmQRgi2ln1AAQaqblKFcmAQAAEDCuTAKwxbQzagAINdNylCuTAAAACBhXJgHYYtoZNQCEmmk5ypVJAAAABIwrkwBsMe2MGgBCzbQcDXkxaVlWqDfpN4/HE+4uwEBO+7uq70+gn8VghuDChQs1d+5clZWVafDgwZo/f76GDRvW5PyvvPKKZsyYoT179qhv37565JFH9Itf/ML2diMNOWoGjpX/nHasyNGfsEKstLTUksTExBTmqbS01NZnt7Ky0pJkHT582Kqrq2t2Onz4sCXJqqys9Gv9S5cutWJjY63Fixdb27ZtsyZNmmR16NDBKi8vb3T+f/7zn1ZMTIz16KOPWp9++qn14IMPWm3atLG2bt1qa78iETnKxOSMiRz9UZRlhfYUt66uTl9//bXi4+NP6vKtx+NRSkqKSktLlZCQ0II9NBPHy3+mHyvLsnTkyBF1795d0dH+D5v2eDxyu92qrKz067jYnT89PV3nnXeeFixYIOnHrEhJSdFdd92lBx54oMH848aNU1VVlVasWOFtO//885WWlqZFixb5vV+RqKVyVDL/770lcaz8Z/qxIkd9hfw2d3R0tHr27Nli60tISDDyDzVYOF7+M/lYud3ugJf193ZT/Xw/nd/lcsnlcvm01dTUqKSkRNOnT/e2RUdHKyMjQ8XFxY2uv7i4WLm5uT5tmZmZWr58uV/9i2QtnaOS2X/vLY1j5T+TjxU5+n/4Ag4Av8TGxio5OVkpKSl+L9O+ffsG8+fl5Sk/P9+n7cCBA6qtrVVSUpJPe1JSkrZv397ousvKyhqdv6yszO/+AUAomZqjFJMA/BIXF6fdu3erpqbG72Usy2pwG/anZ9MA0FqYmqMRW0y6XC7l5eU57oA6FcfLfxyrpsXFxSkuLq7F19u5c2fFxMSovLzcp728vFzJycmNLpOcnGxrfjSOv3f/caz8x7Fqmok5GvIv4ABAY9LT0zVs2DDNnz9f0o8Dx0877TRNmTKlyYHj3333nf7+979720aMGKFBgwYZ/wUcAGhMuHI0Yq9MAjBLbm6usrKyNHToUA0bNkyFhYWqqqpSdna2JGnChAnq0aOHCgoKJElTp07VxRdfrMcff1yjRo3S0qVL9dFHH+npp58O524AQNiEK0cpJgE4wrhx41RRUaGZM2eqrKxMaWlpWrVqlXdw+N69e30ewTFixAgtWbJEDz74oH7729+qb9++Wr58uQYMGBCuXQCAsApXjnKbGwAAAAHz/0mbAAAAwE9QTAIAACBgEVtMLly4UKmpqYqLi1N6ero2bdoU7i45TkFBgc477zzFx8era9euGjNmjHbs2BHubkWEOXPmKCoqStOmTQt3V4CgIUebR44GjhxtPSKymFy2bJlyc3OVl5enzZs3a/DgwcrMzNT+/fvD3TVHWbdunXJycrRx40atXr1ax48f1xVXXKGqqqpwd83RPvzwQz311FMaNGhQuLsCBA056h9yNDDkaOsSkV/Asfsic/yooqJCXbt21bp163TRRReFuzuOdPToUZ177rl64okn9Ic//EFpaWkqLCwMd7eAFkeOBoYcbR452vpE3JXJ+heZZ2RkeNuae5E5flRZWSlJ6tixY5h74lw5OTkaNWqUz98XYBpyNHDkaPPI0dYn4p4zGciLzPHjVYdp06Zp5MiRPIevCUuXLtXmzZv14YcfhrsrQFCRo4EhR5tHjrZOEVdMIjA5OTn65JNP9P7774e7K45UWlqqqVOnavXq1UF5ZyqAyEeOnhg52npFXDEZyIvMW7spU6ZoxYoVWr9+vXr27Bnu7jhSSUmJ9u/fr3PPPdfbVltbq/Xr12vBggWqrq5WTExMGHsItBxy1D5ytHnkaOsVcWMmY2NjNWTIEBUVFXnb6urqVFRUpOHDh4exZ85jWZamTJmiN954Q++9955OP/30cHfJsS677DJt3bpVW7Zs8U5Dhw7VTTfdpC1bthCAMAo56j9y1H/kaOsVcVcmpeZfZI4f5eTkaMmSJfrb3/6m+Ph4lZWVSZLcbrfatm0b5t45S3x8fIMxUO3atVOnTp0YGwUjkaP+IUf9R462XhFZTDb3InP86Mknn5QkXXLJJT7tzz33nCZOnBj6DgFwDHLUP+Qo0LyIfM4kAAAAnCHixkwCAADAOSgmAQAAEDCKSQAAAASMYhIAAAABo5gEAABAwCgmAQAAEDCKSQAAAASMYhIAAAABo5gEAABAwCgmAQAAEDCKSQAAAATs/wGmssc+PfEzYAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.1, 'tpr': 0.6923, 'fpr': 0.5, 'shd': 4, 'nnz': 10, 'precision': 0.9, 'recall': 0.6923, 'F1': 0.7826, 'gscore': 0.6154}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import PC\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import load_dataset\n",
        "\n",
        "# X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "pc = PC(variant='stable')\n",
        "pc.learn(X)\n",
        "GraphDAG(pc.causal_matrix, true_dag, save_name='Result_pc_variant_stable')\n",
        "met = MetricsDAG(pc.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_pc_variant_stable.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__M 1.2: PC_variant_parallel__"
      ],
      "metadata": {
        "id": "AnFJr7sOH6Tj"
      },
      "id": "AnFJr7sOH6Tj"
    },
    {
      "cell_type": "code",
      "source": [
        "pc = PC(variant='parallel')\n",
        "pc.learn(X, p_cores=2)\n",
        "GraphDAG(pc.causal_matrix, true_dag, save_name='Result_pc_variant_parallel')\n",
        "met = MetricsDAG(pc.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_pc_variant_parallel.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "cNtkmnSvH9ul",
        "outputId": "9f8726cc-5f1e-457a-f083-e3115d9a94a5"
      },
      "id": "cNtkmnSvH9ul",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.2, 'tpr': 0.6154, 'fpr': 1.0, 'shd': 5, 'nnz': 10, 'precision': 0.8, 'recall': 0.6154, 'F1': 0.6957, 'gscore': 0.4615}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dfa8c74",
      "metadata": {
        "id": "5dfa8c74",
        "outputId": "0b5b0fff-f9a7-4f8a-950d-5eb204beca06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The type of param `ci_test` expect callable,but got <class 'str'>.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ef9ab51147f7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# true_dag, X = load_dataset(name='iid_test')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X,true_dag, _ = load_dataset('IID_Test')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mskeleton\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_skeleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fisherz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskeleton\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/castle/algorithms/pc/pc.py\u001b[0m in \u001b[0;36mfind_skeleton\u001b[0;34m(data, alpha, ci_test, variant, base_skeleton, p_cores, s, batch)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mci_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mci_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         raise ValueError(f'The type of param `ci_test` expect callable,'\n\u001b[0m\u001b[1;32m    257\u001b[0m                          f'but got {type(ci_test)}.')\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The type of param `ci_test` expect callable,but got <class 'str'>."
          ]
        }
      ],
      "source": [
        "from castle.algorithms.pc.pc import find_skeleton\n",
        "from castle.datasets import load_dataset\n",
        "\n",
        "# true_dag, X = load_dataset(name='iid_test')\n",
        "# X,true_dag, _ = load_dataset('IID_Test')\n",
        "skeleton, sep_set = find_skeleton(X, 0.05, 'fisherz')\n",
        "print(skeleton)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IID/Function-based"
      ],
      "metadata": {
        "id": "GSx24stZHPV0"
      },
      "id": "GSx24stZHPV0"
    },
    {
      "cell_type": "markdown",
      "id": "389636db",
      "metadata": {
        "id": "389636db"
      },
      "source": [
        "##__M2: ANM-Nonlinear__\n",
        "* Nonlinear causal discovery with additive noise models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### M 2.1: ANM-GPR\n",
        "* Estimator based on Gaussian Process Regressor"
      ],
      "metadata": {
        "id": "b0IdSt73IR5H"
      },
      "id": "b0IdSt73IR5H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce14d3aa",
      "metadata": {
        "scrolled": true,
        "id": "ce14d3aa",
        "outputId": "8b010875-bdb7-4e26-aa54-711aeacb9934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.06 ]\n",
            " [0.422]\n",
            " [0.376]\n",
            " [0.68 ]\n",
            " [0.47 ]\n",
            " [0.322]\n",
            " [0.919]\n",
            " [0.15 ]\n",
            " [0.059]\n",
            " [0.697]] [0.406 0.769 0.588 0.472 0.902 0.377 0.192 0.507 0.396 0.419]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "x = np.random.rand(10).reshape((-1, 1))\n",
        "y = np.random.rand(10)#.reshape((-1, 1))\n",
        "gpr = GPR(alpha=1e-10)\n",
        "y_pred = gpr.estimate(x, y)\n",
        "print(x, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ANMNonlinear(BaseLearner):\n",
        "    def __init__(self, alpha=0.05):\n",
        "        super(ANMNonlinear, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def learn(self, data, columns=None, regressor=GPR(), test_method=hsic_test, **kwargs):\n",
        "\n",
        "        self.regressor = regressor\n",
        "\n",
        "        # create learning model and ground truth model\n",
        "        data = Tensor(data, columns=columns)\n",
        "\n",
        "        node_num = data.shape[1]\n",
        "        self.causal_matrix = Tensor(np.zeros((node_num, node_num)),\n",
        "                                    index=data.columns,\n",
        "                                    columns=data.columns)\n",
        "\n",
        "        for i, j in combinations(range(node_num), 2):\n",
        "            x = data[:, i].reshape((-1, 1))\n",
        "            y = data[:, j].reshape((-1, 1))\n",
        "\n",
        "            flag = test_method(x, y, alpha=self.alpha)\n",
        "            if flag == 1:\n",
        "                continue\n",
        "            # test x-->y\n",
        "            flag = self.anm_estimate(x, y, regressor=regressor,\n",
        "                                     test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[i, j] = 1\n",
        "            # test y-->x\n",
        "            flag = self.anm_estimate(y, x, regressor=regressor,\n",
        "                                     test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[j, i] = 1\n",
        "\n",
        "    def anm_estimate(self, x, y, regressor=GPR(), test_method=hsic_test):\n",
        "\n",
        "        x = scale(x).reshape((-1, 1))\n",
        "        y = scale(y).reshape((-1, 1))\n",
        "\n",
        "        y_predict = regressor.estimate(x, y)\n",
        "        flag = test_method(y - y_predict, x, alpha=self.alpha)\n",
        "        print(flag)\n",
        "\n",
        "        return flag"
      ],
      "metadata": {
        "id": "-8YSVugOKZm_"
      },
      "id": "-8YSVugOKZm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    >>> from castle.common import GraphDAG\n",
        "    >>> from castle.metrics import MetricsDAG\n",
        "    >>> from castle.datasets import DAG, IIDSimulation\n",
        "    >>> from castle.algorithms.anm import ANMNonlinear\n",
        "\n",
        "    >>> weighted_random_dag = DAG.erdos_renyi(n_nodes=6, n_edges=10,\n",
        "    >>>                                      weight_range=(0.5, 2.0), seed=1)\n",
        "    >>> dataset = IIDSimulation(W=weighted_random_dag, n=1000,\n",
        "    >>>                         method='nonlinear', sem_type='gp-add')\n",
        "    >>> true_dag, X = dataset.B, dataset.X\n",
        "\n",
        "    >>> anm = ANMNonlinear(alpha=0.05)\n",
        "    >>> anm.learn(data=X)\n",
        "\n",
        "    >>> # plot predict_dag and true_dag\n",
        "    >>> GraphDAG(anm.causal_matrix, true_dag, show=False, save_name='result')\n",
        "\n",
        "    you can also provide more parameters to use it. like the flowing:\n",
        "    >>> from sklearn.gaussian_process.kernels import Matern, RBF\n",
        "    >>> kernel = Matern(nu=1.5)\n",
        "    >>> # kernel = 1.0 * RBF(1.0)\n",
        "    >>> anm = ANMNonlinear(alpha=0.05)\n",
        "    >>> anm.learn(data=X, regressor=GPR(kernel=kernel))\n",
        "    >>> # plot predict_dag and true_dag\n",
        "    >>> GraphDAG(anm.causal_matrix, true_dag, show=False, save_name='result')"
      ],
      "metadata": {
        "id": "wYwWCD7JlwmN"
      },
      "id": "wYwWCD7JlwmN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "145fb6d8",
      "metadata": {
        "id": "145fb6d8",
        "outputId": "481dec0a-e9ae-4bd5-fd10-e0cf6691572e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7c0cf3b0bae0>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# print(X, true_dag)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0manm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mANMNonlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0manm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# plot predict_dag and true_dag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/castle/algorithms/anm/_anm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, data, columns, regressor, test_method, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/castle/common/independence_tests.py\u001b[0m in \u001b[0;36mhsic_test\u001b[0;34m(x, y, alpha, normalize)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rbf_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rbf_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m     \u001b[0mKc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0mLc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from sklearn.gaussian_process.kernels import Matern, RBF\n",
        "from sklearn.preprocessing import scale\n",
        "from itertools import combinations\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import hsic_test\n",
        "from math import sqrt\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "\n",
        "from castle.datasets import DAG, IIDSimulation\n",
        "from castle.datasets import load_dataset\n",
        "from castle.algorithms import ANMNonlinear\n",
        "'''\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=6, n_edges=10,\n",
        "                                    weight_range=(0.5, 2.0), seed=1)\n",
        "dataset = IIDSimulation(W=weighted_random_dag, n=1000,method='nonlinear', sem_type='gp-add')\n",
        "\n",
        "\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=6, n_edges=15,\n",
        "                                     weight_range=(0.5, 2.0), seed=1)\n",
        "dataset = IIDSimulation(W=weighted_random_dag, n=1000,\n",
        "                        method='nonlinear', sem_type='gp-add')\n",
        "true_dag, X = dataset.B, dataset.X\n",
        "\n",
        "# X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "'''\n",
        "# print(X, true_dag)\n",
        "anm = ANMNonlinear(alpha=0.05)\n",
        "anm.learn(data=X)\n",
        "\n",
        "# plot predict_dag and true_dag\n",
        "met = MetricsDAG(anm.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "GraphDAG(anm.causal_matrix, true_dag, save_name='Result_anm')\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_anm.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  M 2.2: ANM-GPR-Kernel"
      ],
      "metadata": {
        "id": "yCi_380vSrIt"
      },
      "id": "yCi_380vSrIt"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from itertools import combinations\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import hsic_test\n",
        "\n",
        "\n",
        "class GPR(object):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(GPR, self).__init__()\n",
        "        self.regressor = GaussianProcessRegressor(**kwargs)\n",
        "\n",
        "    def estimate(self, x, y):\n",
        "        self.regressor.fit(x, y)\n",
        "        y_predict = self.regressor.predict(x)\n",
        "\n",
        "        return y_predict\n"
      ],
      "metadata": {
        "id": "O78x296zJzL5"
      },
      "id": "O78x296zJzL5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.gaussian_process.kernels import Matern, RBF\n",
        "# from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
        "\n",
        "kernel = Matern(nu=1.5)\n",
        "# kernel = 1.0 * RBF(1.0)\n",
        "anm = ANMNonlinear(alpha=0.05)\n",
        "anm.learn(data=X[:30], regressor=GPR(kernel=kernel))\n",
        "# plot predict_dag and true_dag\n",
        "met = MetricsDAG(anm.causal_matrix, true_dag)\n",
        "GraphDAG(anm.causal_matrix, true_dag, save_name='Result_anm_kernel')\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_anm_kernel.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "9pr_5Mq9SarD",
        "outputId": "1f97ee9e-c92e-429b-cfa6-a06b30d35cca"
      },
      "id": "9pr_5Mq9SarD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/gaussian_process/kernels.py:420: ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/castle/metrics/evaluation.py:224: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  precision = TP/TP_FP\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.0, 'tpr': 0.0, 'fpr': 0.0, 'shd': 13, 'nnz': 0, 'precision': nan, 'recall': 0.0, 'F1': nan, 'gscore': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3761a94f",
      "metadata": {
        "id": "3761a94f",
        "outputId": "eb9b7a91-5e0e-41b2-c73e-7070453f6bbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from castle.algorithms.anm import ANMNonlinear\n",
        "np.random.seed(1)\n",
        "x = np.random.rand(500, 2)\n",
        "anm = ANMNonlinear(alpha=0.05)\n",
        "print(anm.anm_estimate(x[:, [0]], x[:, [1]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  M 2.3: ANM-NCPOP"
      ],
      "metadata": {
        "id": "H3JcGzNjtUEY"
      },
      "id": "H3JcGzNjtUEY"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "from itertools import combinations\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import hsic_test\n",
        "# from inputlds import*\n",
        "# from functions import*\n",
        "from ncpol2sdpa import*\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class NCPOLR(object):\n",
        "    \"\"\"Estimator based on NCPOP Regressor\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Quan Zhou https://github.com/Quan-Zhou/Proper-Learning-of-LDS/blob/master/ncpop/functions.py\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NCPOLR, self).__init__()\n",
        "\n",
        "    def estimate_1hidden(self, X, Y):\n",
        "        \"\"\"Fit Estimator based on NCPOP Regressor model and predict y or produce residuals.\n",
        "        The module converts a noncommutative optimization problem provided in SymPy\n",
        "        format to an SDPA semidefinite programming problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array\n",
        "            Variable seen as cause\n",
        "        Y: array\n",
        "            Variable seen as effect\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predict: array\n",
        "            regression predict values of y or residuals\n",
        "        \"\"\"\n",
        "\n",
        "        T = len(Y)\n",
        "        level = 1\n",
        "\n",
        "        # Decision Variables\n",
        "        G = generate_operators(\"G\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        f = generate_operators(\"f\", n_vars=T, hermitian=True, commutative=False)\n",
        "        n = generate_operators(\"m\", n_vars=T, hermitian=True, commutative=False)\n",
        "        p = generate_operators(\"p\", n_vars=T, hermitian=True, commutative=False)\n",
        "\n",
        "        # Objective\n",
        "        obj = sum((Y[i]-f[i])**2 for i in range(T)) + 0.5*sum(p[i] for i in range(T))\n",
        "\n",
        "        # Constraints\n",
        "        ine1 = [f[i] - G*X[i] - n[i] for i in range(T)]\n",
        "        ine2 = [-f[i] + G*X[i] + n[i] for i in range(T)]\n",
        "        ine3 = [p[i]-n[i] for i in range(T)]\n",
        "        ine4 = [p[i]+n[i] for i in range(T)]\n",
        "        ines = ine1+ine2+ine3+ine4\n",
        "\n",
        "        # Solve the NCPO\n",
        "        sdp = SdpRelaxation(variables = flatten([G,f,n,p]),verbose = 1)\n",
        "        sdp.get_relaxation(level, objective=obj, inequalities=ines)\n",
        "        sdp.solve(solver='mosek')\n",
        "        #sdp.solve(solver='sdpa', solverparameters={\"executable\":\"sdpa_gmp\",\"executable\": \"C:/Users/zhouq/Documents/sdpa7-windows/sdpa.exe\"})\n",
        "        print(sdp.primal, sdp.dual, sdp.status)\n",
        "\n",
        "        if(sdp.status != 'infeasible'):\n",
        "            print('ok.')\n",
        "            est_noise = []\n",
        "            for i in range(T):\n",
        "                est_noise.append(sdp[n[i]])\n",
        "            print(est_noise)\n",
        "            return est_noise\n",
        "        else:\n",
        "            print('Cannot find feasible solution.')\n",
        "            return\n",
        "\n",
        "    def estimate_2hidden(self, X, Y):\n",
        "        \"\"\"Fit Estimator based on NCPOP Regressor model with 2*2 hidden state matrix and predict y or produce residuals.\n",
        "        The module converts a noncommutative optimization problem provided in SymPy\n",
        "        format to an SDPA semidefinite programming problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array\n",
        "            Variable seen as cause\n",
        "        Y: array\n",
        "            Variable seen as effect\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predict: array\n",
        "            regression predict values of y or residuals\n",
        "        \"\"\"\n",
        "\n",
        "        T = len(Y)\n",
        "        level = 1\n",
        "\n",
        "        # Decision Variables\n",
        "        G = generate_operators(\"G\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        H = generate_operators(\"H\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        Fdash = generate_operators(\"Fdash\", n_vars=2, hermitian=True, commutative=False)\n",
        "        m = generate_operators(\"m\", n_vars=T, hermitian=True, commutative=False)\n",
        "        q = generate_operators(\"q\", n_vars=T, hermitian=True, commutative=False)\n",
        "        p = generate_operators(\"p\", n_vars=T, hermitian=True, commutative=True)\n",
        "        f = generate_operators(\"f\", n_vars=T, hermitian=True, commutative=True)\n",
        "        n = generate_operators(\"n\", n_vars=T, hermitian=True, commutative=False)\n",
        "        u = generate_operators(\"u\", n_vars=T, hermitian=True, commutative=False)\n",
        "\n",
        "        # Objective\n",
        "        obj = sum((Y[i]-f[i])**2 for i in range(T)) + 0.0005*sum(p[i]**2 for i in range(T)) + 0.001*sum(q[i]**2 for i in range(T))\n",
        "\n",
        "        # Constraints\n",
        "        ine1 = [ f[i] - Fdash[0]*m[i]  - Fdash[1]*m[i-1] - p[i] for i in range(1,T)]\n",
        "        ine2 = [-f[i] + Fdash[0]*m[i] +Fdash[1]*m[i-1] + p[i] for i in range(1,T)]\n",
        "        ine3 = [ m[i] - G*m[i-1]  - H*m[i-2] - q[i] for i in range(2,T)]\n",
        "        ine4 = [-m[i] + G*m[i-1] + H*m[i-2] + q[i] for i in range(2,T)]\n",
        "        ine5 = [p[i] + n[i] for i in range(T)]\n",
        "        ine6 = [p[i] - n[i] for i in range(T)]\n",
        "        ine7 = [q[i] + u[i] for i in range(T)]\n",
        "        ine8 = [q[i] - u[i] for i in range(T)]\n",
        "\n",
        "        ines = ine1+ine2+ine3+ine4\n",
        "\n",
        "        # Solve the NCPO\n",
        "        sdp = SdpRelaxation(variables = flatten([G,H,Fdash,f,p,m,q]),verbose = 1)\n",
        "        sdp.get_relaxation(level, objective=obj, inequalities=ines)\n",
        "        sdp.solve(solver='mosek')\n",
        "        #sdp.solve(solver='sdpa', solverparameters={\"executable\":\"sdpa_gmp\",\"executable\": \"C:/Users/zhouq/Documents/sdpa7-windows/sdpa.exe\"})\n",
        "        print(sdp.primal, sdp.dual, sdp.status)\n",
        "\n",
        "        if(sdp.status != 'infeasible'):\n",
        "            print('ok.')\n",
        "            est_noise = []\n",
        "            for i in range(T):\n",
        "                est_noise.append(sdp[Y[i]-f[i]+q[i]])\n",
        "            print(est_noise)\n",
        "            return est_noise\n",
        "        else:\n",
        "            print('Cannot find feasible solution.')\n",
        "            return\n",
        "\n",
        "\n",
        "\n",
        "class ANM_NCPO(BaseLearner):\n",
        "    \"\"\"\n",
        "    Nonlinear causal discovery with additive noise models\n",
        "\n",
        "    Use Estimator based on NCPOP Regressor and independent Gaussian noise,\n",
        "    For the independence test, we implemented the HSIC with a Gaussian kernel,\n",
        "    where we used the gamma distribution as an approximation for the\n",
        "    distribution of the HSIC under the null hypothesis of independence\n",
        "    in order to calculate the p-value of the test result.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Hoyer, Patrik O and Janzing, Dominik and Mooij, Joris M and Peters,\n",
        "    Jonas and Schölkopf, Bernhard,\n",
        "    \"Nonlinear causal discovery with additive noise models\", NIPS 2009\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float, default 0.05\n",
        "        significance level be used to compute threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : array like shape of (n_features, n_features)\n",
        "        Learned causal structure matrix.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.05):\n",
        "        super(ANM_NCPO, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def learn(self, data,causalmodelling, columns=None, regressor=NCPOLR(),test_method=hsic_test, **kwargs):\n",
        "        \"\"\"Set up and run the ANM_NCPOP algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: numpy.ndarray or Tensor\n",
        "            Training data.\n",
        "        causalmodelling: Modelling type(hidden_state1,hidden_state2,ARMA,Discrete)\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is NCPOLR.\n",
        "            If user defined, must implement `estimate` self.method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test self.method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "        \"\"\"\n",
        "\n",
        "        self.regressor = regressor\n",
        "\n",
        "        # create learning model and ground truth model\n",
        "        data = Tensor(data, columns=columns)\n",
        "\n",
        "        node_num = data.shape[1]\n",
        "        self.causal_matrix = Tensor(np.zeros((node_num, node_num)),\n",
        "                                    index=data.columns,\n",
        "                                    columns=data.columns)\n",
        "\n",
        "        for i, j in combinations(range(node_num), 2):\n",
        "            x = data[:, i]\n",
        "            y = data[:, j]\n",
        "            xx = x.reshape((-1, 1))\n",
        "            yy = y.reshape((-1, 1))\n",
        "\n",
        "            flag = test_method(xx, yy, alpha=self.alpha)\n",
        "            if flag == 1:\n",
        "                continue\n",
        "            # test x-->y\n",
        "            flag = self.anmNCPO_estimate(x, y, causalmodelling, regressor = regressor, test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[i, j] = 1\n",
        "            # test y-->x\n",
        "            flag = self.anmNCPO_estimate(y, x, causalmodelling, regressor = regressor, test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[j, i] = 1\n",
        "\n",
        "    def anmNCPO_estimate(self, x, y, causalmodelling, regressor=NCPOLR(), test_method=hsic_test):\n",
        "        \"\"\"Compute the fitness score of the ANM_NCPOP Regression model in the x->y direction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: array\n",
        "            Variable seen as cause\n",
        "        y: array\n",
        "            Variable seen as effect\n",
        "        causalmodelling: Modelling type(hidden_state1,hidden_state2,ARMA,Discrete)\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is NCPOP.\n",
        "            If user defined, must implement `estimate` self.method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test self.method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "        Returns\n",
        "        -------\n",
        "        out: int, 0 or 1\n",
        "            If 1, residuals n is independent of x, then accept x --> y\n",
        "            If 0, residuals n is not independent of x, then reject x --> y\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        x = scale(x)\n",
        "        y = scale(y)\n",
        "        if causalmodelling == 'hidden_state1':\n",
        "            y_res = regressor.estimate_1hidden(x, y)\n",
        "        elif causalmodelling == 'hidden_state2':\n",
        "            y_res = regressor.estimate_2hidden(x, y)\n",
        "        else:\n",
        "            print('WRONG MODELLING TYPE')\n",
        "        #if causalmodelling='ARMA':\n",
        "        #if causalmodelling='Discrete':\n",
        "\n",
        "\n",
        "        flag = test_method(np.asarray(y_res).reshape((-1, 1)), np.asarray(x).reshape((-1, 1)), alpha=self.alpha)\n",
        "        print(flag)\n",
        "\n",
        "        return flag"
      ],
      "metadata": {
        "id": "JED-iUu2tW8f"
      },
      "id": "JED-iUu2tW8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anmNCPO = ANM_NCPO(alpha=0.05)\n",
        "anmNCPO.learn(data=X,causalmodelling='hidden_state1')\n",
        "\n",
        "# plot predict_dag and true_dag\n",
        "GraphDAG(anmNCPO.causal_matrix, true_dag, save_name = 'Result_anm_ncpop')\n",
        "met = MetricsDAG(anmNCPO.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_anm_ncpop.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uGzaobUGuHnA",
        "outputId": "599f3e1b-1842-40d7-ba38-4ddafc82b432"
      },
      "id": "uGzaobUGuHnA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 14/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 2.42            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  2.47  \n",
            "1   1.8e+00  4.3e-01  6.4e-01  2.84e-02   1.083610388e+01   1.031990542e+01   4.3e-01  10.34 \n",
            "2   3.2e-01  7.7e-02  6.0e-02  9.43e-01   1.761534128e+01   1.754790457e+01   7.7e-02  16.82 \n",
            "3   4.0e-02  9.6e-03  2.9e-03  8.62e-01   2.424488908e+01   2.423672759e+01   9.6e-03  19.60 \n",
            "4   2.9e-02  7.0e-03  1.8e-03  7.77e-01   2.455017438e+01   2.454431557e+01   7.0e-03  21.94 \n",
            "5   1.9e-03  4.6e-04  2.5e-05  1.04e+00   2.527256457e+01   2.527188505e+01   4.6e-04  24.43 \n",
            "6   2.7e-04  6.4e-05  1.5e-06  9.95e-01   2.531619312e+01   2.531612200e+01   6.4e-05  28.24 \n",
            "7   1.8e-05  4.4e-06  2.7e-08  1.01e+00   2.532296563e+01   2.532296093e+01   4.4e-06  31.49 \n",
            "8   5.0e-08  1.2e-08  3.3e-12  1.01e+00   2.532349504e+01   2.532349502e+01   1.2e-08  34.17 \n",
            "9   4.6e-11  1.2e-08  4.7e-18  1.00e+00   2.532349656e+01   2.532349656e+01   1.5e-12  36.78 \n",
            "Optimizer terminated. Time: 36.82   \n",
            "\n",
            "9.67650344215048 9.676503442152434 optimal\n",
            "ok.\n",
            "[1.2835698404467257, 0.33879813429246164, 0.19242703883203252, -0.8022602615063442, -0.7533686493730867, 0.4619831666691007, -0.30115597974451114, 3.090509612346359e-10, 1.631994319195498, 3.3536360225610087e-09, -2.2757754629013048e-09, -0.0599408616078968, 2.3508899556388823e-09, -4.162221081440463e-09, -3.023523023386899e-09, 0.2735196189902984, 0.44991220932694215, -6.529971637437913e-10, 1.0249104742721244, -1.111990938376518, -2.2087099806384716e-09, -0.26427493571012184, 1.3248872336433355, -0.49684598561275495, -1.0571391280484403, 1.0483315653511517e-09, -0.2246177182096943, 0.33836716412284573, -0.32857934762499813, -0.7884996567130781, 0.19865234433016188, -1.5528368352664038e-10, -1.4561302557264233, 0.12214007106712603, -0.3996756445150534]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 10/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 35.00000000000001 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.51            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.6e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.56  \n",
            "1   2.0e+00  4.3e-01  6.4e-01  2.84e-02   1.083335501e+01   1.031702135e+01   4.3e-01  5.34  \n",
            "2   6.1e-01  1.3e-01  1.2e-01  9.48e-01   1.484998686e+01   1.471583118e+01   1.3e-01  9.01  \n",
            "3   9.8e-02  2.1e-02  9.3e-03  7.59e-01   2.327326577e+01   2.325486330e+01   2.1e-02  12.87 \n",
            "4   6.8e-02  1.5e-02  5.6e-03  7.44e-01   2.402209936e+01   2.400997984e+01   1.5e-02  16.54 \n",
            "5   4.2e-03  9.0e-04  7.4e-05  1.05e+00   2.553310835e+01   2.553186943e+01   9.0e-04  19.79 \n",
            "6   5.9e-04  1.3e-04  4.0e-06  9.89e-01   2.562612452e+01   2.562596715e+01   1.3e-04  22.29 \n",
            "7   7.8e-05  1.7e-05  2.0e-07  1.01e+00   2.563756427e+01   2.563754828e+01   1.7e-05  24.85 \n",
            "8   9.8e-06  2.1e-06  9.2e-09  1.01e+00   2.563925256e+01   2.563925071e+01   2.1e-06  27.30 \n",
            "9   4.8e-07  1.0e-07  9.8e-11  1.01e+00   2.563946865e+01   2.563946856e+01   1.0e-07  30.29 \n",
            "10  8.0e-09  6.2e-10  1.3e-14  1.01e+00   2.563948119e+01   2.563948119e+01   3.0e-10  34.02 \n",
            "Optimizer terminated. Time: 34.07   \n",
            "\n",
            "9.360518811338416 9.360518811588115 optimal\n",
            "ok.\n",
            "[-8.854744519160732e-09, 8.159566012674667e-09, -0.054651320405414115, 0.5639182114003466, 2.322583279504122e-10, 0.17894272242948397, 4.762362036231117e-10, 0.8937412157592999, -2.344393021988922, 0.4141198923694593, -1.3326370747748763e-09, 1.2970278631815145e-09, 9.027028025221707e-10, -0.8247522839751947, -0.2503587092576796, -5.329796733461026e-09, -0.01630992848666663, 0.4257076789958353, -0.6716315570018458, 1.0994349148092462, -2.310739105416901e-10, -0.33273901642707326, -0.4741973045630553, 0.7626567563420347, 2.4428784615874475e-10, -4.523958491448617e-09, 0.7621292938553278, 0.28170262404194807, 0.6351848379542682, 0.512438292589014, -1.507448129878055, 0.3551091092102266, 0.6714171315517722, -0.666957521766164, -0.5169782501236259]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 25/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 3.04            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.3e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  3.11  \n",
            "1   1.9e+00  4.5e-01  6.9e-01  -8.97e-03  1.105937610e+01   1.053794653e+01   4.5e-01  7.36  \n",
            "2   3.2e-01  7.5e-02  5.8e-02  9.53e-01   1.899431491e+01   1.893030927e+01   7.5e-02  12.02 \n",
            "3   4.1e-02  9.6e-03  2.7e-03  9.08e-01   2.520821108e+01   2.519943521e+01   9.6e-03  16.25 \n",
            "4   2.5e-02  5.9e-03  1.4e-03  8.76e-01   2.559588549e+01   2.559054250e+01   5.9e-03  19.93 \n",
            "5   1.8e-03  4.2e-04  2.2e-05  1.03e+00   2.613490237e+01   2.613431796e+01   4.2e-04  23.01 \n",
            "6   7.7e-04  1.8e-04  6.5e-06  9.94e-01   2.615802628e+01   2.615779967e+01   1.8e-04  26.92 \n",
            "7   1.1e-04  2.6e-05  3.5e-07  1.00e+00   2.617377076e+01   2.617374101e+01   2.6e-05  29.67 \n",
            "8   1.1e-05  2.5e-06  1.1e-08  1.01e+00   2.617590475e+01   2.617590234e+01   2.5e-06  32.59 \n",
            "9   7.8e-06  1.8e-06  7.0e-09  1.00e+00   2.617597445e+01   2.617597271e+01   1.8e-06  35.01 \n",
            "10  1.3e-06  2.9e-07  4.6e-10  1.01e+00   2.617613151e+01   2.617613127e+01   2.9e-07  37.70 \n",
            "11  5.5e-08  1.3e-08  4.1e-12  1.01e+00   2.617615832e+01   2.617615831e+01   1.3e-08  41.80 \n",
            "12  7.6e-11  9.6e-09  5.7e-19  1.00e+00   2.617615956e+01   2.617615956e+01   3.7e-13  44.69 \n",
            "Optimizer terminated. Time: 44.74   \n",
            "\n",
            "8.823840439770642 8.823840439770283 optimal\n",
            "ok.\n",
            "[-0.6433349038215403, -0.003833753959644037, 1.1079101243775127e-09, 1.0356794299366912, 0.493629976506493, -0.9183705303481321, 5.472413336711654e-10, 0.7189220684032255, -1.1452149986226834, -0.029206495400087882, 1.3308495449321422e-09, 0.9668388104564948, -4.1272238662882313e-10, 0.11168548205669382, -0.5624806989119316, -1.028552315193903, -0.7453695616951153, 0.7283495549322362, -0.287688522526426, 0.6964545102349584, -3.002918112757741e-10, 0.08329884594367039, -0.47662621580464615, -3.0822512399623603e-09, 4.00194263045183e-09, -0.08960682909744588, -4.470173022155737e-09, 0.24755657106137466, 0.24648472526861542, 1.5016023798767446, 0.038217046955237366, -0.2709355987715618, 6.200023345015156e-10, -0.9587941874138944, 7.127953803823523e-09]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 17/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 35.00000000000001 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 3.27            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.6e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  3.32  \n",
            "1   2.1e+00  4.5e-01  6.9e-01  -8.97e-03  1.105845918e+01   1.053698598e+01   4.5e-01  9.26  \n",
            "2   3.8e-01  8.2e-02  6.5e-02  9.55e-01   1.915180739e+01   1.907519107e+01   8.2e-02  13.25 \n",
            "3   4.1e-02  8.8e-03  2.4e-03  9.32e-01   2.564660495e+01   2.563869360e+01   8.8e-03  17.46 \n",
            "4   2.6e-02  5.6e-03  1.2e-03  8.84e-01   2.600023544e+01   2.599522492e+01   5.6e-03  21.11 \n",
            "5   2.1e-03  4.6e-04  2.7e-05  1.04e+00   2.649888272e+01   2.649831674e+01   4.6e-04  25.22 \n",
            "6   2.7e-04  5.9e-05  1.3e-06  9.95e-01   2.654018730e+01   2.654013052e+01   5.9e-05  28.42 \n",
            "7   3.0e-05  6.4e-06  4.6e-08  1.01e+00   2.654519104e+01   2.654518496e+01   6.4e-06  31.15 \n",
            "8   5.0e-08  1.1e-08  2.9e-12  1.01e+00   2.654583050e+01   2.654583049e+01   1.1e-08  33.91 \n",
            "9   6.2e-11  3.9e-09  1.6e-18  1.00e+00   2.654583160e+01   2.654583160e+01   8.5e-13  36.54 \n",
            "Optimizer terminated. Time: 36.57   \n",
            "\n",
            "8.454168395374655 8.4541683953734 optimal\n",
            "ok.\n",
            "[-3.0279136915674437e-10, -1.7688199583001095e-10, 6.643647858313457e-11, 0.7716136802574796, 2.9270260239097294e-10, -3.288939348549959e-10, -0.08415831411235764, 1.2747017569230803, -1.763831155461865, -1.9516039543004945e-10, 1.0033706442722043e-10, 0.5179266908441218, -5.064668028675265e-11, -0.17477876236031292, -0.5759648293722793, -0.5431378258651488, -0.3323662118965974, 0.9162573946989119, -0.2642446018129221, 0.799613847027964, 2.271357352373838e-11, -0.12602990034417064, -0.13991913178887033, 0.24292900137529827, -0.13069578893777337, -0.14140257004363435, 0.31410146971800174, 0.6716541492033309, 0.4747242582746731, 1.0512560702731355, -0.605563487900739, -2.1291364956683032e-10, 7.252556855435329e-11, -1.048805504884612, -0.4793234552238487]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 22/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.50            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   5.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.54  \n",
            "1   2.3e+00  4.6e-01  7.4e-01  -7.03e-02  1.158228922e+01   1.106406218e+01   4.6e-01  5.45  \n",
            "2   3.2e-01  6.2e-02  4.2e-02  9.88e-01   2.297018279e+01   2.291466250e+01   6.2e-02  10.64 \n",
            "3   1.8e-01  3.5e-02  1.8e-02  8.28e-01   2.570136409e+01   2.566923801e+01   3.5e-02  15.23 \n",
            "4   9.1e-03  1.8e-03  1.6e-04  1.12e+00   2.856419986e+01   2.856148929e+01   1.8e-03  19.71 \n",
            "5   5.5e-03  1.1e-03  7.9e-05  9.52e-01   2.862996028e+01   2.862839756e+01   1.1e-03  23.35 \n",
            "6   4.1e-04  7.9e-05  1.6e-06  1.01e+00   2.870234447e+01   2.870223526e+01   7.9e-05  26.15 \n",
            "7   5.0e-05  9.8e-06  8.0e-08  1.01e+00   2.870792534e+01   2.870791570e+01   9.8e-06  28.72 \n",
            "8   2.1e-06  4.1e-07  6.7e-10  1.01e+00   2.870861483e+01   2.870861441e+01   4.1e-07  31.86 \n",
            "9   7.9e-09  8.8e-10  4.4e-14  1.01e+00   2.870864631e+01   2.870864631e+01   6.8e-10  35.94 \n",
            "Optimizer terminated. Time: 35.98   \n",
            "\n",
            "6.291353687432871 6.291353688242996 optimal\n",
            "ok.\n",
            "[-0.07157327216632998, -0.12538937274252224, 3.733929579522586e-09, 0.6451804350626272, 0.23758298657689056, -0.3138025230533746, -0.3834966870574821, 0.7731235703302436, -0.05819511717811431, -0.36585077652261805, 1.7347730336688223e-09, 0.7820589960288882, -0.02039360885988525, 0.34240674701788204, -0.5916628374698693, -0.748409559602234, -0.39996357018221523, 0.4083766661117945, -1.9236637642692617e-09, -2.342142636710965e-09, 1.0871742388339278e-09, 1.5673350223796365e-08, 4.3924642974126035e-09, -3.5293680405967295e-09, -2.082118152011256e-09, -5.312300399141503e-09, 2.7267252082058144e-09, 0.4413119094042793, 1.6193369554768637e-09, 1.1104521388426936, 0.3868903176704091, -0.11968087941712854, -0.3316909640445582, -0.7533414961805858, -1.5381487682198883e-09]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 24/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 35.00000000000001 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.59            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.6e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.64  \n",
            "1   2.1e+00  4.6e-01  7.4e-01  -7.03e-02  1.158207922e+01   1.106384262e+01   4.6e-01  5.68  \n",
            "2   2.9e-01  6.3e-02  4.2e-02  9.88e-01   2.316368361e+01   2.310361682e+01   6.3e-02  9.88  \n",
            "3   1.1e-01  2.3e-02  7.6e-03  8.94e-01   2.718893404e+01   2.715689096e+01   2.3e-02  13.58 \n",
            "4   1.4e-02  3.0e-03  4.0e-04  8.54e-01   2.889824521e+01   2.889425475e+01   3.0e-03  17.71 \n",
            "5   9.5e-04  2.1e-04  7.2e-06  1.03e+00   2.909903233e+01   2.909878905e+01   2.1e-04  21.78 \n",
            "6   1.2e-04  2.6e-05  3.2e-07  1.01e+00   2.911403240e+01   2.911400308e+01   2.6e-05  25.34 \n",
            "7   2.1e-05  4.4e-06  2.4e-08  1.01e+00   2.911555428e+01   2.911554984e+01   4.4e-06  29.00 \n",
            "8   1.7e-06  3.8e-07  6.1e-10  1.00e+00   2.911589960e+01   2.911589926e+01   3.8e-07  31.33 \n",
            "9   9.8e-10  2.7e-10  2.2e-15  1.00e+00   2.911593021e+01   2.911593021e+01   1.0e-10  34.62 \n",
            "Optimizer terminated. Time: 34.68   \n",
            "\n",
            "5.8840697916593285 5.884069791807065 optimal\n",
            "ok.\n",
            "[-7.34537755107005e-12, -1.868583292033692e-10, 2.171307611767926e-10, 0.5906601103699766, 9.155665029796015e-10, -2.9816931680947844e-10, -0.3816873778807224, 1.1378596520772435, -0.6755788036951025, -0.012508390129698468, 8.71763883357373e-12, 0.5748818486167094, -4.009898694492264e-10, 1.7595311243790227e-10, -0.6203463906212836, -0.5373273465023671, -0.23627102256283578, 0.6114966099008843, -9.938515472903148e-10, 6.51609462324467e-10, 1.9629448932744087e-10, -1.3339234752947635e-09, 1.0896850891179358e-09, 4.0649770503451236e-10, -0.13393428763052542, -4.5095008211042113e-10, 0.17403965338148222, 0.6852907827575835, 0.06676262294368228, 0.9553863326624217, -3.0239635637495513e-10, -6.085585860681116e-11, -0.23369465082967816, -0.8829996649467352, -0.29918946415034836]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 28/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 3.19            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.3e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  3.26  \n",
            "1   1.9e+00  4.6e-01  7.3e-01  -4.98e-02  1.139012796e+01   1.086947653e+01   4.6e-01  7.62  \n",
            "2   2.7e-01  6.4e-02  4.5e-02  9.74e-01   2.138366889e+01   2.133235152e+01   6.4e-02  11.93 \n",
            "3   7.4e-02  1.7e-02  5.7e-03  8.90e-01   2.586834124e+01   2.584698545e+01   1.7e-02  15.72 \n",
            "4   4.9e-03  1.1e-03  8.6e-05  9.32e-01   2.710438782e+01   2.710255279e+01   1.1e-03  19.62 \n",
            "5   1.7e-03  4.0e-04  1.9e-05  1.03e+00   2.713943095e+01   2.713888378e+01   4.0e-04  23.14 \n",
            "6   2.3e-04  5.5e-05  1.1e-06  1.01e+00   2.715689887e+01   2.715684604e+01   5.5e-05  26.66 \n",
            "7   1.6e-05  3.7e-06  1.9e-08  1.00e+00   2.716041308e+01   2.716040926e+01   3.7e-06  30.58 \n",
            "8   5.5e-07  1.3e-07  1.2e-10  1.01e+00   2.716065476e+01   2.716065461e+01   1.3e-07  33.83 \n",
            "9   2.9e-08  1.6e-09  6.6e-14  1.01e+00   2.716066398e+01   2.716066398e+01   8.8e-10  36.49 \n",
            "Optimizer terminated. Time: 36.53   \n",
            "\n",
            "7.839336018922548 7.839336021040108 optimal\n",
            "ok.\n",
            "[-3.1741831766349213e-09, -2.3142330948577357e-09, 0.22738711350812912, 0.15759073148519442, 0.08089228724717544, -0.8738498186499578, -0.23253807076894267, 0.17039280187328715, 0.5640970300673965, -0.4691929825094377, 0.15182950661985112, 0.7817707693043849, -0.0756391861064076, 0.7980714385594931, -0.2168598002961014, -0.7053985956711745, -0.31562804604333694, 0.3993310147623845, 0.2400074742283508, -0.10612978119637881, 1.4779623797323043e-09, 0.20424931714148253, 0.06045385233930628, -0.506139596384623, -0.13418945206778807, -0.015942380838276447, -0.4438922736319058, 0.18491477860448682, -3.079814174791882e-08, 0.6579066991548941, 1.20389024286371, -0.5807977420601582, -0.7413333037184291, -0.29530365580175777, 3.0708194862170227e-07]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 31/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 2.99            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  3.07  \n",
            "1   1.9e+00  4.6e-01  7.3e-01  -4.98e-02  1.138998107e+01   1.086932286e+01   4.6e-01  7.04  \n",
            "2   2.8e-01  6.8e-02  5.0e-02  9.74e-01   2.135744119e+01   2.129929728e+01   6.8e-02  11.10 \n",
            "3   3.9e-02  9.5e-03  2.4e-03  9.56e-01   2.676627868e+01   2.675577914e+01   9.5e-03  14.90 \n",
            "4   5.0e-03  1.2e-03  1.1e-04  9.97e-01   2.734597671e+01   2.734460967e+01   1.2e-03  18.85 \n",
            "5   3.9e-04  9.5e-05  2.3e-06  1.01e+00   2.742237766e+01   2.742225733e+01   9.5e-05  22.91 \n",
            "6   2.5e-04  6.0e-05  1.1e-06  1.01e+00   2.742439802e+01   2.742432548e+01   6.0e-05  26.55 \n",
            "7   2.9e-05  7.0e-06  4.8e-08  1.01e+00   2.742796419e+01   2.742795691e+01   7.0e-06  29.16 \n",
            "8   3.9e-07  9.5e-08  7.5e-11  1.00e+00   2.742845652e+01   2.742845642e+01   9.5e-08  31.67 \n",
            "9   6.6e-09  8.1e-10  1.4e-14  1.01e+00   2.742846405e+01   2.742846405e+01   3.2e-10  34.46 \n",
            "Optimizer terminated. Time: 34.50   \n",
            "\n",
            "7.57153595227895 7.57153595276046 optimal\n",
            "ok.\n",
            "[0.7991470842979399, 0.2473487024083976, 0.265848847527681, -3.6290092837616656e-09, -0.35229545416807817, 7.420934848440807e-10, -0.4833839838878014, 0.7585463375235899, 0.5485203819173302, -0.02048419673142056, 2.9678062025298263e-08, 0.2003423524558189, -0.08176534363174397, 0.06225327615448649, -0.15410626608851893, -0.02275705459351857, 7.721505058986256e-10, 0.5888293749291634, 0.6562813477527283, -0.43613651920855323, 2.5973931864444214e-09, -0.16558150962108295, 0.8837144558963671, -0.3484256184988727, -1.0521515231145258, -0.08064765868119446, -0.08676688624856574, 0.7627694588454055, -2.456120583655665e-09, 8.007963712737141e-10, 0.18490375694720465, -1.8505558207665688e-09, -1.3254841002310942, -0.2332494690064137, -0.4999310964726319]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 37/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.53            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.2e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.59  \n",
            "1   1.9e+00  4.5e-01  6.9e-01  -1.20e-02  1.108161397e+01   1.056004288e+01   4.5e-01  5.48  \n",
            "2   2.8e-01  6.7e-02  5.0e-02  9.54e-01   1.921783955e+01   1.916568631e+01   6.7e-02  9.70  \n",
            "3   4.1e-02  9.8e-03  2.7e-03  8.57e-01   2.514385421e+01   2.513325349e+01   9.8e-03  13.56 \n",
            "4   4.8e-03  1.1e-03  1.1e-04  1.00e+00   2.588950434e+01   2.588840175e+01   1.1e-03  17.49 \n",
            "5   3.0e-04  7.1e-05  1.6e-06  1.01e+00   2.598236276e+01   2.598228336e+01   7.1e-05  21.66 \n",
            "6   4.1e-05  9.7e-06  8.4e-08  1.01e+00   2.598769624e+01   2.598768652e+01   9.7e-06  25.46 \n",
            "7   7.0e-06  1.7e-06  6.0e-09  1.01e+00   2.598839293e+01   2.598839129e+01   1.7e-06  27.98 \n",
            "8   3.6e-08  8.4e-09  1.9e-12  1.00e+00   2.598854451e+01   2.598854450e+01   8.4e-09  31.29 \n",
            "Optimizer terminated. Time: 31.36   \n",
            "\n",
            "9.011455487805684 9.01145549940388 optimal\n",
            "ok.\n",
            "[-0.17233249424920805, -1.0113246615556662e-08, 0.20272579379428227, 0.2719470420887235, 0.9410749556937815, -0.6456806178284402, -0.6058489581477738, 0.7139811825882263, 0.19013937416457144, -0.8694746445313268, -0.11217487095043514, 0.6094253246711935, -0.32221064617644896, 0.6372460560392476, -0.7446188741038587, -0.6039742297775157, -0.41177115554019733, 0.35395695577255715, 0.13571191994894682, -0.12277403113677879, -7.361111984454952e-09, 8.014835346486938e-09, 0.18575449198704436, -0.38429351220166474, -1.8646177003280453e-08, 1.042191308281863e-07, -1.404443556911174e-07, 0.784953391285954, -2.5068191109234664e-08, 1.2242812542745134, 1.1309288867325806, -0.03645251733572872, -0.8064046099899912, -1.1111917161291818, 3.289329267935292e-08]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 22/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.56            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.61  \n",
            "1   1.9e+00  4.5e-01  6.9e-01  -1.20e-02  1.108107212e+01   1.055947532e+01   4.5e-01  5.58  \n",
            "2   3.1e-01  7.5e-02  5.8e-02  9.55e-01   1.934101671e+01   1.927531553e+01   7.5e-02  9.77  \n",
            "3   4.0e-02  9.6e-03  2.7e-03  9.16e-01   2.555089509e+01   2.554192017e+01   9.6e-03  13.69 \n",
            "4   3.5e-02  8.5e-03  2.3e-03  6.53e-01   2.563197402e+01   2.562418696e+01   8.5e-03  17.43 \n",
            "5   3.9e-03  9.5e-04  7.6e-05  1.03e+00   2.638986421e+01   2.638860566e+01   9.5e-04  21.14 \n",
            "6   1.5e-03  3.6e-04  1.8e-05  9.84e-01   2.644308276e+01   2.644265339e+01   3.6e-04  25.02 \n",
            "7   6.0e-05  1.5e-05  1.3e-07  1.01e+00   2.647526629e+01   2.647524391e+01   1.5e-05  29.33 \n",
            "8   6.2e-06  1.5e-06  5.1e-09  1.01e+00   2.647649208e+01   2.647649064e+01   1.5e-06  33.64 \n",
            "9   3.3e-08  8.2e-09  2.0e-12  1.01e+00   2.647663783e+01   2.647663782e+01   8.1e-09  36.58 \n",
            "Optimizer terminated. Time: 36.63   \n",
            "\n",
            "8.523362172739844 8.523362181621877 optimal\n",
            "ok.\n",
            "[0.7811385494004208, 0.3487397083812338, 0.2629598126641125, -2.2616220407553047e-07, -1.0345899170680498e-07, 0.0961679149804184, -0.7458063145416016, 1.1633513610773594, 0.3340030866546309, -0.2180677376415333, -2.7789432117138988e-06, 0.041975844097045495, -0.23587955240715613, 3.906331188081794e-08, -0.47128570500687733, -3.993519321242258e-08, 1.9199855936737915e-08, 0.5943692220887157, 0.6514046882553004, -0.4915653536974868, 3.261326351095357e-08, -0.47698535151443766, 1.063547943217817, -0.26753856803320697, -1.0806507629536377, -1.6654379535341184e-08, 2.2548426816476606e-09, 1.2005247361190243, -1.7369830210951095e-08, 0.09851910657169792, 0.03982680457010577, 4.4768703457272603e-07, -1.450520340274854, -0.7251187022186455, -0.6626120695320781]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 4/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.69            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   5.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.74  \n",
            "1   2.2e+00  4.3e-01  6.4e-01  3.21e-02   1.081856430e+01   1.030323485e+01   4.3e-01  5.73  \n",
            "2   4.4e-01  8.5e-02  7.0e-02  9.44e-01   1.729475699e+01   1.721600184e+01   8.5e-02  9.62  \n",
            "3   4.3e-02  8.4e-03  2.3e-03  8.79e-01   2.426115656e+01   2.425419574e+01   8.4e-03  13.72 \n",
            "4   2.6e-02  5.0e-03  1.1e-03  9.20e-01   2.469242953e+01   2.468828368e+01   5.0e-03  17.59 \n",
            "5   2.0e-03  3.8e-04  2.1e-05  1.03e+00   2.520505239e+01   2.520461522e+01   3.8e-04  21.76 \n",
            "6   2.9e-04  5.8e-05  1.3e-06  9.99e-01   2.524048231e+01   2.524042828e+01   5.8e-05  25.89 \n",
            "7   4.2e-05  8.2e-06  6.8e-08  1.01e+00   2.524545540e+01   2.524544722e+01   8.2e-06  29.00 \n",
            "8   3.4e-06  6.6e-07  1.5e-09  1.01e+00   2.524626937e+01   2.524626873e+01   6.6e-07  31.67 \n",
            "9   4.3e-07  8.4e-08  7.3e-11  1.01e+00   2.524632520e+01   2.524632513e+01   8.4e-08  34.12 \n",
            "10  1.7e-09  2.6e-09  2.6e-15  1.00e+00   2.524633354e+01   2.524633354e+01   1.0e-10  37.05 \n",
            "Optimizer terminated. Time: 37.11   \n",
            "\n",
            "9.753666455904668 9.753666456051537 optimal\n",
            "ok.\n",
            "[2.480448861329101e-11, -0.23114125469398086, 0.1862794942504002, 5.832815037807381e-10, 0.11612526456458953, -0.5811125050162707, -0.47794449759957436, 3.353529845531276e-10, 1.7123423944621807, -0.9139527086362634, 0.05149616780156143, 0.6625928726973155, -0.1079297305027868, 1.2226406905696603, -0.17498801249760054, -0.5315954461677336, -0.11336611265609332, 4.0253871228225793e-10, 0.5194840063012008, -1.0896583054572557, 3.7241367944647086e-10, 0.36092180851215916, 0.6486941665279412, -0.783106807473892, -0.008441263367395611, -5.492131897472486e-11, -0.48539612028864915, 0.07888357942888007, -0.4296759653020297, 0.3919040166639016, 1.8376764236975687, -0.5950704799836299, -1.2477165728349835, -4.4191104701907466e-08, 0.21052731550908335]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 23/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.61            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.67  \n",
            "1   1.8e+00  4.3e-01  6.4e-01  3.21e-02   1.081921955e+01   1.030392244e+01   4.3e-01  5.78  \n",
            "2   3.5e-01  8.3e-02  6.8e-02  9.43e-01   1.734942614e+01   1.727160191e+01   8.3e-02  10.24 \n",
            "3   3.5e-02  8.5e-03  2.4e-03  8.77e-01   2.419544380e+01   2.418839867e+01   8.5e-03  14.61 \n",
            "4   2.0e-02  4.9e-03  1.1e-03  9.31e-01   2.464055612e+01   2.463653284e+01   4.9e-03  18.43 \n",
            "5   1.8e-03  4.3e-04  2.5e-05  1.03e+00   2.511848037e+01   2.511797277e+01   4.3e-04  22.32 \n",
            "6   2.7e-04  6.4e-05  1.5e-06  9.96e-01   2.516061794e+01   2.516055332e+01   6.4e-05  26.22 \n",
            "7   1.7e-04  4.1e-05  7.6e-07  1.01e+00   2.516288477e+01   2.516284621e+01   4.1e-05  29.83 \n",
            "8   2.1e-05  5.2e-06  3.5e-08  1.01e+00   2.516639288e+01   2.516638810e+01   5.2e-06  32.97 \n",
            "9   2.5e-07  6.0e-08  4.3e-11  1.00e+00   2.516695427e+01   2.516695421e+01   6.0e-08  35.60 \n",
            "10  6.2e-09  9.3e-10  4.2e-15  1.00e+00   2.516696089e+01   2.516696089e+01   1.3e-10  38.30 \n",
            "Optimizer terminated. Time: 38.35   \n",
            "\n",
            "9.833039109434548 9.833039109727086 optimal\n",
            "ok.\n",
            "[1.3062935851218438, 0.3132175411992603, 0.24281515501150747, -0.31797828868660516, -0.6745603069528042, 0.42889609915344923, -0.6679699344881135, 0.8159112297355247, 1.0483561290981063, -0.04445874169014469, 2.8569924663421266e-10, 5.375146340057583e-10, -0.08711802860219368, 1.8436462956381616e-09, -0.09566911669625298, 1.6239319844500005e-10, 0.17962362095437212, 0.3905846952365811, 0.914372095472617, -0.9968564931857801, 1.665569440926927e-08, -0.4135821474287628, 1.434619280308244, -0.39475735897434006, -1.3009370171354542, -5.69821768127905e-09, -1.1554778200871876e-09, 0.8998455034083608, -0.1275608896603344, -0.09983151370072679, 0.07061371072316609, 1.6808140179092713e-10, -1.7298134646596672, -0.04567682056928233, -0.7533887723613055]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 34/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 35.00000000000001 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.50            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.4e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.54  \n",
            "1   2.0e+00  4.5e-01  6.9e-01  -1.52e-02  1.110425612e+01   1.058253444e+01   4.5e-01  5.45  \n",
            "2   3.5e-01  7.8e-02  6.1e-02  9.56e-01   1.932757384e+01   1.925833530e+01   7.8e-02  9.54  \n",
            "3   4.1e-02  9.3e-03  2.6e-03  9.30e-01   2.550960347e+01   2.550159886e+01   9.3e-03  13.66 \n",
            "4   1.7e-02  3.9e-03  6.8e-04  9.21e-01   2.605338225e+01   2.604923392e+01   3.9e-03  17.44 \n",
            "5   2.5e-03  5.6e-04  3.8e-05  9.57e-01   2.639232334e+01   2.639177267e+01   5.6e-04  21.73 \n",
            "6   2.2e-04  5.1e-05  1.0e-06  1.01e+00   2.643613016e+01   2.643608122e+01   5.1e-05  25.61 \n",
            "7   8.5e-06  1.9e-06  7.0e-09  1.01e+00   2.644111328e+01   2.644111100e+01   1.9e-06  29.76 \n",
            "8   1.8e-06  4.1e-07  7.1e-10  1.01e+00   2.644124565e+01   2.644124522e+01   4.1e-07  33.40 \n",
            "9   4.8e-08  1.1e-08  3.1e-12  1.00e+00   2.644128604e+01   2.644128603e+01   1.1e-08  36.90 \n",
            "10  1.5e-11  6.4e-09  1.1e-18  1.00e+00   2.644128704e+01   2.644128704e+01   5.2e-13  39.75 \n",
            "Optimizer terminated. Time: 39.79   \n",
            "\n",
            "8.558712963356193 8.558712963356612 optimal\n",
            "ok.\n",
            "[-1.5744623642564718e-09, 2.0670810560485644e-09, -5.93286295105054e-10, -0.32303363160684123, 0.07824577966381074, 0.9899069491932608, -1.5945033750132003e-10, -0.45899503416444576, 1.695000105356503, -0.39864337562879, -0.8652543857382967, -0.6106621379148262, -9.398886538763503e-11, -0.18641078819827137, -0.22481069648743338, 0.4192808289871219, 1.1382122758175328, -0.5758078813690962, -1.8238801971199326e-09, -1.3291305406711522, 1.721928197974748e-09, -1.062856322722572e-09, 0.8640061758727209, 9.568396647462125e-10, -1.5965895084318137e-09, 0.1880329062653814, 0.424231479989702, -0.21048856287453258, -0.7333865765292863, -0.3601876745893127, 0.6622493669162482, -5.452948896368998e-10, -0.42938118423416266, 0.2782315025297951, 0.20833066184908017]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 14/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 2.79            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  2.87  \n",
            "1   1.9e+00  4.5e-01  6.9e-01  -1.52e-02  1.110470046e+01   1.058299981e+01   4.5e-01  11.53 \n",
            "2   2.7e-01  6.6e-02  4.8e-02  9.55e-01   1.963995362e+01   1.958704806e+01   6.6e-02  20.73 \n",
            "3   4.1e-02  1.0e-02  2.7e-03  8.44e-01   2.548010661e+01   2.546842588e+01   1.0e-02  27.32 \n",
            "4   6.7e-03  1.6e-03  1.8e-04  9.63e-01   2.620109489e+01   2.619930274e+01   1.6e-03  34.93 \n",
            "5   8.9e-04  2.2e-04  8.8e-06  1.00e+00   2.632652827e+01   2.632629865e+01   2.2e-04  40.62 \n",
            "6   1.7e-04  4.2e-05  7.6e-07  1.00e+00   2.634032335e+01   2.634027903e+01   4.2e-05  44.13 \n",
            "7   3.9e-07  9.4e-08  6.7e-11  1.01e+00   2.634442287e+01   2.634442272e+01   9.4e-08  47.79 \n",
            "8   1.8e-09  4.2e-10  2.9e-15  1.00e+00   2.634443301e+01   2.634443301e+01   1.1e-10  54.22 \n",
            "Optimizer terminated. Time: 54.32   \n",
            "\n",
            "8.655566986045624 8.655566986255916 optimal\n",
            "ok.\n",
            "[1.0899566473016957, 0.6122968855264747, 1.09748623455315e-09, -2.19855755445406e-09, -0.9405999727503177, 3.2653523008629384e-09, -0.30289272628826025, 1.0990378529229898, -0.6353089586478683, 0.38264009876330146, 0.4359188186116126, 0.050860296191622106, -1.6862021104719164e-09, -1.4662286575952251e-09, 1.9709753329697387e-09, 9.5174197543271e-11, -0.09048877751003633, 0.7921322928085754, 0.3699851332321755, 0.1897611551426402, 0.04402583959269475, -0.7077166672608431, 0.1817521097928358, -8.809548522015527e-11, -1.1834237091940623, -0.224307188134368, -0.0537669371757979, 0.9358642610346066, 0.3172855432902336, -2.533579000390364e-09, -0.9121258744892496, 0.15787863906938843, -0.4711618877523783, -0.2525913039547919, -1.1331097944842754]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 23/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.01            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 2.98            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.2e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  3.05  \n",
            "1   1.9e+00  4.6e-01  7.7e-01  -1.16e-01  1.206485069e+01   1.155630786e+01   4.6e-01  8.72  \n",
            "2   2.6e-01  6.1e-02  3.7e-02  1.03e+00   2.665386061e+01   2.658218661e+01   6.1e-02  13.20 \n",
            "3   1.5e-01  3.5e-02  1.6e-02  9.59e-01   2.915413874e+01   2.911331426e+01   3.5e-02  16.72 \n",
            "4   5.1e-03  1.2e-03  6.7e-05  1.18e+00   3.157023718e+01   3.156814086e+01   1.2e-03  20.45 \n",
            "5   7.5e-04  1.8e-04  4.9e-06  1.00e+00   3.163462913e+01   3.163439750e+01   1.8e-04  23.03 \n",
            "6   3.4e-05  8.0e-06  4.7e-08  1.01e+00   3.164401402e+01   3.164400392e+01   8.0e-06  27.05 \n",
            "7   4.7e-06  1.1e-06  2.7e-09  1.01e+00   3.164430146e+01   3.164430035e+01   1.1e-06  29.70 \n",
            "8   5.8e-09  1.2e-09  8.1e-14  1.00e+00   3.164437012e+01   3.164437012e+01   1.1e-09  32.38 \n",
            "Optimizer terminated. Time: 32.43   \n",
            "\n",
            "3.3556298805890457 3.3556298818599224 optimal\n",
            "ok.\n",
            "[-0.023848572425328906, 8.304097401212278e-10, 6.204262623437732e-10, 4.598612326859858e-09, 0.6369016599451875, 3.314173819955871e-09, -0.17628452888293494, 0.3429632396726831, -0.03840064527434554, -0.2220861122370374, -0.47316740421644027, -1.6580705119353436e-09, -0.030881620069511305, -1.3416621224064508e-09, -0.32636172907323513, 9.571199908967378e-11, -4.5153733664171315e-09, 5.890412312213236e-10, -1.1677787143535063e-09, -1.3603996902988054e-09, -3.965803350332886e-09, -0.06370609836516307, 7.630299646840814e-09, 1.0234359000041114e-09, 2.564571572263014e-09, 0.1407973865234074, 0.2102471542982482, 0.4008511082888022, 4.004356719133573e-09, 0.4048342337029958, 1.7626209109608228e-09, 0.21265492335079833, -1.502988790797596e-08, -0.622743005547462, -1.4304839210433728e-09]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 37/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.01            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.04    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.76            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.3e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.90  \n",
            "1   2.0e+00  4.6e-01  7.7e-01  -1.16e-01  1.206491531e+01   1.155637536e+01   4.6e-01  10.61 \n",
            "2   2.6e-01  6.1e-02  3.7e-02  1.03e+00   2.660441678e+01   2.653363389e+01   6.1e-02  21.54 \n",
            "3   1.5e-01  3.4e-02  1.5e-02  9.66e-01   2.912974175e+01   2.909035628e+01   3.4e-02  28.95 \n",
            "4   4.7e-03  1.1e-03  4.4e-05  1.17e+00   3.134594349e+01   3.134373171e+01   1.1e-03  36.72 \n",
            "5   6.8e-04  1.6e-04  4.1e-06  1.00e+00   3.139596392e+01   3.139575071e+01   1.6e-04  42.55 \n",
            "6   6.8e-05  1.6e-05  1.4e-07  1.00e+00   3.140506545e+01   3.140504809e+01   1.6e-05  46.10 \n",
            "7   9.7e-06  2.3e-06  7.5e-09  1.01e+00   3.140576971e+01   3.140576715e+01   2.3e-06  51.88 \n",
            "8   6.2e-07  1.5e-07  1.3e-10  1.00e+00   3.140588381e+01   3.140588367e+01   1.5e-07  57.51 \n",
            "9   1.4e-08  9.5e-10  4.5e-14  1.00e+00   3.140589221e+01   3.140589221e+01   7.4e-10  61.74 \n",
            "Optimizer terminated. Time: 61.86   \n",
            "\n",
            "3.594107787855677 3.5941077884421055 optimal\n",
            "ok.\n",
            "[1.52175835370721e-09, -3.1730167795702098e-09, -3.01360374105587e-11, -5.591083009163506e-10, -0.4724599035907945, -0.10911718816087673, 0.17849970486758807, -0.39541160561796684, 0.08580118565179236, 0.10905368904224932, 0.5121876428549991, 1.0445716320886911e-08, 0.009000083076181702, 1.4749820542835237e-08, 0.2826452461745315, -4.726921828666258e-09, 2.7778634937780085e-10, -1.9428280363880997e-10, 2.6924493045397076e-10, 1.5132554555808838e-09, 2.795561825869033e-09, 0.21215780236365114, -0.013033243053007524, -2.2895368034795937e-09, 2.3393517967704677e-10, -0.1427263687600687, -0.29754193895565195, -0.4502657751036638, -4.120344008082325e-09, -0.225651158382584, 4.98653314571202e-09, -0.3515550743954455, 7.681010387395696e-09, 0.5709277853019274, 1.1232702392330739e-08]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 33/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 7.65            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   5.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  7.73  \n",
            "1   2.4e+00  4.6e-01  7.7e-01  -1.07e-01  1.196368636e+01   1.145277477e+01   4.6e-01  17.27 \n",
            "2   3.2e-01  6.2e-02  3.8e-02  1.02e+00   2.607238525e+01   2.600070668e+01   6.2e-02  23.22 \n",
            "3   1.8e-01  3.6e-02  1.7e-02  9.84e-01   2.855976720e+01   2.851912919e+01   3.6e-02  27.05 \n",
            "4   7.7e-03  1.5e-03  1.0e-04  1.14e+00   3.121427897e+01   3.121167345e+01   1.5e-03  29.94 \n",
            "5   3.8e-03  7.5e-04  3.9e-05  9.99e-01   3.126768713e+01   3.126650461e+01   7.5e-04  32.20 \n",
            "6   4.9e-04  9.6e-05  2.1e-06  1.01e+00   3.130494614e+01   3.130482703e+01   9.6e-05  34.72 \n",
            "7   5.6e-05  1.1e-05  8.5e-08  1.01e+00   3.131123080e+01   3.131121940e+01   1.1e-05  39.00 \n",
            "8   3.0e-06  5.9e-07  1.1e-09  1.01e+00   3.131190772e+01   3.131190711e+01   5.9e-07  41.77 \n",
            "9   3.9e-08  7.2e-09  1.3e-12  1.01e+00   3.131194898e+01   3.131194897e+01   6.9e-09  44.42 \n",
            "Optimizer terminated. Time: 44.48   \n",
            "\n",
            "3.688051024526928 3.688051032226781 optimal\n",
            "ok.\n",
            "[0.0354289731441297, -0.04579587037565747, 7.157677270720684e-09, -0.020724045238539546, -8.90837138977138e-10, 3.6131281491228883e-07, -0.1142094890534576, -2.7733331765155444e-08, 1.0470518657378076, -0.27504899737776817, -5.807692036752625e-09, 1.1351610667980195e-09, -1.4364575721096036e-08, 0.2819339806362862, -5.10971153287779e-09, 1.0230229084590277e-08, 4.7542892234278047e-08, -0.12489785503857494, 0.16912393823870162, -0.8382489089746777, 3.0068949977878135e-09, 2.8324360172173553e-08, 0.4925288085207028, -0.14833109084542254, -6.6399682827828155e-09, 7.14894230260477e-08, -2.393860838883657e-08, 5.33947606272778e-09, -0.23597448859323888, -7.496827701112788e-08, 0.5329063115045448, -1.4627444974309377e-08, -0.5085209648067115, 2.1023373884540517e-07, 3.2382979450933504e-08]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 39/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.81            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.3e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.87  \n",
            "1   2.0e+00  4.6e-01  7.7e-01  -1.07e-01  1.196384669e+01   1.145294225e+01   4.6e-01  5.87  \n",
            "2   2.6e-01  6.2e-02  3.8e-02  1.02e+00   2.593593651e+01   2.586598726e+01   6.2e-02  11.59 \n",
            "3   1.5e-01  3.6e-02  1.7e-02  9.83e-01   2.839028700e+01   2.835037923e+01   3.6e-02  15.43 \n",
            "4   2.2e-02  5.2e-03  9.2e-04  1.14e+00   3.067765267e+01   3.067223969e+01   5.2e-03  19.26 \n",
            "5   3.3e-03  7.7e-04  4.8e-05  1.02e+00   3.096543220e+01   3.096452814e+01   7.7e-04  23.40 \n",
            "6   4.1e-04  9.7e-05  2.2e-06  1.01e+00   3.100651397e+01   3.100640372e+01   9.7e-05  27.37 \n",
            "7   6.3e-05  1.5e-05  1.3e-07  1.00e+00   3.101101409e+01   3.101099782e+01   1.5e-05  31.37 \n",
            "8   3.9e-06  9.2e-07  2.1e-09  1.00e+00   3.101189376e+01   3.101189287e+01   9.2e-07  34.74 \n",
            "9   4.0e-08  9.4e-09  2.1e-12  1.00e+00   3.101195028e+01   3.101195027e+01   9.3e-09  38.23 \n",
            "Optimizer terminated. Time: 38.31   \n",
            "\n",
            "3.9880497239107306 3.9880497331103903 optimal\n",
            "ok.\n",
            "[-0.3197081370717107, 2.8530094592279224e-08, -2.1334013807035064e-09, 0.19243299399514244, 0.014834604467742473, -0.35864698027236, 0.10786674902883954, 6.538418777514838e-09, -0.8987851646300333, 0.07269669532255461, 3.07381509631798e-08, 3.806872926964719e-07, 1.1789731499502857e-08, -2.3128588300996854e-07, -5.139094451246317e-09, -0.07769238176661479, -0.12454949467766095, 0.12494766002635235, -0.19827459240964038, 0.8266446008696622, -6.9722205613728e-09, 3.0450925639393857e-08, -0.6189253870653659, 0.017249569880277555, 2.5695847135915865e-07, -8.956953019632513e-08, -6.51596228607333e-09, -3.6198150259989476e-08, 0.17944066370633238, 0.2150971478546817, -0.11147220322357722, -4.579208090671339e-08, 0.5112116265476805, -0.02680497173406153, 3.01487188309653e-08]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 34/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 35.00000000000001 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.71            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.4e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.75  \n",
            "1   1.9e+00  4.3e-01  6.4e-01  2.83e-02   1.083343665e+01   1.031706436e+01   4.3e-01  5.91  \n",
            "2   4.0e-01  9.1e-02  7.5e-02  9.49e-01   1.804053950e+01   1.794479761e+01   9.1e-02  9.89  \n",
            "3   3.5e-02  8.0e-03  2.1e-03  8.98e-01   2.537890916e+01   2.537094238e+01   8.0e-03  14.01 \n",
            "4   1.9e-02  4.4e-03  8.2e-04  9.60e-01   2.579410262e+01   2.578948923e+01   4.4e-03  17.93 \n",
            "5   3.1e-03  7.0e-04  5.2e-05  9.77e-01   2.628262597e+01   2.628186269e+01   7.0e-04  21.99 \n",
            "6   9.3e-05  2.1e-05  2.2e-07  1.01e+00   2.634763592e+01   2.634759953e+01   2.1e-05  26.38 \n",
            "7   3.3e-05  7.4e-06  5.1e-08  1.00e+00   2.634895982e+01   2.634894912e+01   7.4e-06  30.11 \n",
            "8   1.5e-06  3.4e-07  5.3e-10  1.00e+00   2.634962073e+01   2.634962032e+01   3.4e-07  33.72 \n",
            "9   2.0e-07  4.5e-08  2.6e-11  1.00e+00   2.634965613e+01   2.634965608e+01   4.5e-08  36.85 \n",
            "10  6.1e-08  1.4e-08  4.5e-12  1.00e+00   2.634965956e+01   2.634965955e+01   1.4e-08  40.79 \n",
            "11  1.7e-08  6.4e-09  4.8e-13  1.00e+00   2.634966065e+01   2.634966064e+01   3.1e-09  43.11 \n",
            "Optimizer terminated. Time: 43.17   \n",
            "\n",
            "8.650339354233786 8.650339357121648 optimal\n",
            "ok.\n",
            "[4.140481125341437e-08, -1.1392658588269654e-07, 0.12318497592475257, -0.16075981969869188, 0.13443061484708244, 0.4553964472793601, -0.06114666642843346, -0.0774515836937152, 2.2319857099577853, -0.7969692452824647, -0.6275836130733888, -0.028314963285292833, -4.877533995801004e-08, 1.7496384835378154e-08, -0.5233438852605949, 2.4699479740133514e-08, 0.8772701670635193, -0.11219734502710886, 0.3111663544265449, -1.6229331341438542, -1.2463476601662691e-08, 0.06764014634691555, 1.2137791440203805, -0.3238666336028449, -3.627595842268819e-08, 0.0004209432315530315, 0.02665832779002881, -7.616385918958032e-09, -0.8800435684737385, -2.3157579226753565e-08, 1.4479213979393641, -0.15392070164458277, -1.188732454393745, 3.5347500076519844e-08, 0.19487654005111588]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 8/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 2.08            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.3e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  2.15  \n",
            "1   1.8e+00  4.3e-01  6.4e-01  2.83e-02   1.083575868e+01   1.031950059e+01   4.3e-01  6.42  \n",
            "2   5.3e-01  1.2e-01  1.1e-01  9.45e-01   1.486164383e+01   1.474002674e+01   1.2e-01  10.20 \n",
            "3   8.7e-02  2.0e-02  8.9e-03  7.35e-01   2.302823790e+01   2.301045628e+01   2.0e-02  14.08 \n",
            "4   5.4e-02  1.3e-02  4.5e-03  8.02e-01   2.387799478e+01   2.386733080e+01   1.3e-02  17.88 \n",
            "5   2.6e-03  6.2e-04  3.9e-05  1.05e+00   2.512905184e+01   2.512805213e+01   6.2e-04  22.13 \n",
            "6   3.6e-04  8.4e-05  2.3e-06  1.00e+00   2.518962122e+01   2.518953074e+01   8.4e-05  26.40 \n",
            "7   4.9e-05  1.2e-05  1.2e-07  1.01e+00   2.519758858e+01   2.519757830e+01   1.2e-05  30.43 \n",
            "8   6.8e-07  1.6e-07  1.7e-10  1.01e+00   2.519892491e+01   2.519892470e+01   1.6e-07  34.65 \n",
            "9   3.0e-08  6.6e-09  1.5e-12  1.01e+00   2.519894225e+01   2.519894225e+01   6.3e-09  38.74 \n",
            "Optimizer terminated. Time: 38.83   \n",
            "\n",
            "9.801057748976792 9.801057754578178 optimal\n",
            "ok.\n",
            "[-0.7144037196723789, -0.4596644455066622, 0.0957980243281663, 0.18148796242517848, 0.870961686835962, -0.7610317240460343, -4.168019265691027e-08, -0.2843463935523749, 1.4519689888129315, -0.9727543475860786, -1.8249284184960062e-07, 0.44477086428711354, -2.1500236878075616e-07, 0.8115628726215945, -0.40384939424842775, -0.5730359923577828, -3.145637138677493e-08, -4.399987289554368e-08, 4.418358918945657e-08, -0.6554103164016924, -2.8380087215699552e-08, 0.8229234408646553, 4.882007043107449e-06, -0.4725236310528157, 0.28893121809117867, 2.7857485737145614e-08, -0.138030165599047, -0.12559735457853252, -0.5043497715385085, 0.6709458653897715, 2.0795822467916114, -0.8260274742502997, -0.41785497701199975, -4.768300217283516e-08, 0.9225290751698021]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 32/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 3.45            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   5.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  3.52  \n",
            "1   2.3e+00  4.6e-01  7.5e-01  -7.57e-02  1.163516009e+01   1.111776454e+01   4.6e-01  8.02  \n",
            "2   3.5e-01  6.8e-02  4.7e-02  9.92e-01   2.331173459e+01   2.324523098e+01   6.8e-02  12.34 \n",
            "3   4.8e-02  9.3e-03  2.3e-03  1.03e+00   2.832530385e+01   2.831526405e+01   9.3e-03  18.04 \n",
            "4   1.9e-02  3.8e-03  5.9e-04  9.89e-01   2.862876248e+01   2.862477652e+01   3.8e-03  21.80 \n",
            "5   2.2e-03  4.3e-04  2.3e-05  9.98e-01   2.887068914e+01   2.887021536e+01   4.3e-04  26.00 \n",
            "6   5.3e-05  1.0e-05  6.3e-08  1.00e+00   2.889334367e+01   2.889332605e+01   1.0e-05  30.48 \n",
            "7   6.8e-06  1.3e-06  3.5e-09  1.01e+00   2.889384650e+01   2.889384484e+01   1.3e-06  34.92 \n",
            "8   5.2e-07  1.0e-07  7.6e-11  1.00e+00   2.889392083e+01   2.889392072e+01   1.0e-07  39.47 \n",
            "9   7.8e-08  1.2e-08  3.0e-12  1.00e+00   2.889392676e+01   2.889392674e+01   1.2e-08  43.79 \n",
            "10  1.7e-09  1.3e-08  7.5e-15  1.00e+00   2.889392745e+01   2.889392745e+01   2.2e-10  46.84 \n",
            "Optimizer terminated. Time: 46.89   \n",
            "\n",
            "6.106072548558707 6.106072548825388 optimal\n",
            "ok.\n",
            "[0.025847299483442297, -0.19854796015323026, 5.977249203245514e-10, -0.00036915844793552186, -0.2788588315713951, -2.309536368954261e-09, -2.1469683857802581e-10, -0.4371493074787757, 1.341051189167113, -0.03258375223196192, 0.34190070817001983, 1.583033687314969e-08, 9.670450583843554e-10, 0.5382667137828256, 0.1269995885429173, -1.488612404386319e-09, 1.1401163545366994e-09, -0.13013017829424067, 0.1699618574696906, -0.7598826213992387, 1.2038461566107796e-09, 0.3295733088785313, 0.23555790589398645, -0.26811595622968243, 2.606320844212662e-10, -1.2407943873531392e-09, -0.361666680136489, -0.2989745302102052, -0.3709280257338288, -0.252206758208575, 0.7682764129927135, -0.3908101218576816, -0.33804584461851617, 0.6150092472035803, 0.14213188235357382]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 34/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.93            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.2e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.99  \n",
            "1   1.9e+00  4.6e-01  7.5e-01  -7.57e-02  1.163528321e+01   1.111789325e+01   4.6e-01  6.19  \n",
            "2   2.6e-01  6.2e-02  4.1e-02  9.92e-01   2.352160484e+01   2.346167118e+01   6.2e-02  10.25 \n",
            "3   1.4e-01  3.2e-02  1.3e-02  8.69e-01   2.648448367e+01   2.644298354e+01   3.2e-02  13.76 \n",
            "4   2.0e-02  4.7e-03  7.8e-04  9.06e-01   2.864229397e+01   2.863693154e+01   4.7e-03  18.00 \n",
            "5   2.2e-03  5.3e-04  2.9e-05  1.03e+00   2.892936087e+01   2.892877039e+01   5.3e-04  22.24 \n",
            "6   2.4e-04  5.7e-05  1.0e-06  1.01e+00   2.895865354e+01   2.895858719e+01   5.7e-05  26.61 \n",
            "7   7.5e-05  1.8e-05  1.8e-07  1.01e+00   2.896117116e+01   2.896115181e+01   1.8e-05  30.29 \n",
            "8   2.2e-06  5.2e-07  8.7e-10  1.01e+00   2.896245421e+01   2.896245360e+01   5.2e-07  34.45 \n",
            "9   3.0e-07  7.2e-08  4.7e-11  1.01e+00   2.896248314e+01   2.896248306e+01   7.2e-08  37.92 \n",
            "10  5.7e-08  1.4e-08  2.7e-12  1.01e+00   2.896248714e+01   2.896248713e+01   1.1e-08  41.62 \n",
            "11  3.5e-09  7.1e-09  1.6e-14  1.00e+00   2.896248771e+01   2.896248771e+01   3.6e-10  44.12 \n",
            "Optimizer terminated. Time: 44.19   \n",
            "\n",
            "6.03751229228342 6.037512292662157 optimal\n",
            "ok.\n",
            "[-0.5914414486000484, 5.230608386052297e-10, -1.675910105385361e-10, 0.32915797519017437, 0.9336203921299195, -0.25761850964020966, -9.29275090943106e-10, 0.34397496680720674, -0.9844085784177123, -0.0005618476180700351, -0.3483824112731168, 5.1626158714757755e-09, -2.9707056198983978e-08, -2.7707458772789476e-09, -0.36511853161461805, -0.09355794141708897, -0.27680041326676574, 0.09669248567352157, -0.19215952426928792, 0.6146114472206113, -3.025465377671341e-09, -1.6602030604592102e-09, -0.3660276857737874, 2.142518184743704e-08, 0.09407145792581147, 1.0893785681257728e-08, 0.14046169223830618, 0.260656007041573, 0.24150965759332588, 0.8789532857619401, -1.169975663820642e-09, 0.016879596880038983, 0.23010920567851503, -0.876192622893407, 5.602204758763942e-10]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 32/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 35.00000000000001 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.18    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 2.48            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.4e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  2.80  \n",
            "1   1.9e+00  4.2e-01  6.1e-01  5.09e-02   1.075355233e+01   1.024431379e+01   4.2e-01  6.96  \n",
            "2   3.9e-01  8.9e-02  7.3e-02  9.45e-01   1.697264678e+01   1.688285292e+01   8.9e-02  11.31 \n",
            "3   3.5e-02  7.9e-03  2.2e-03  8.67e-01   2.431544583e+01   2.430853023e+01   7.9e-03  15.42 \n",
            "4   1.5e-02  3.4e-03  6.0e-04  9.64e-01   2.490619889e+01   2.490283070e+01   3.4e-03  19.33 \n",
            "5   2.4e-03  5.4e-04  3.7e-05  9.62e-01   2.531946289e+01   2.531893702e+01   5.4e-04  23.37 \n",
            "6   1.3e-04  3.0e-05  4.5e-07  1.01e+00   2.537167978e+01   2.537164502e+01   3.0e-05  27.74 \n",
            "7   3.3e-05  7.4e-06  5.8e-08  1.00e+00   2.537447406e+01   2.537446630e+01   7.4e-06  31.72 \n",
            "8   3.6e-07  8.0e-08  4.9e-11  1.00e+00   2.537526967e+01   2.537526951e+01   8.0e-08  35.74 \n",
            "9   3.3e-08  6.4e-09  7.5e-13  1.01e+00   2.537527866e+01   2.537527865e+01   5.0e-09  39.64 \n",
            "Optimizer terminated. Time: 39.73   \n",
            "\n",
            "9.624721336181217 9.624721348221293 optimal\n",
            "ok.\n",
            "[0.014320558436931714, -1.0450818713778656e-08, 0.10893443526687639, -0.21848376762716934, 0.4065072253622182, 0.8063511631958377, -0.2896539783524366, 6.351428233975753e-09, 2.0221854364961374, -0.9111131942582793, -1.0576440726760175, -0.2384089323431306, -0.06724728681513331, -2.45635271491611e-08, -0.7761045623728902, 0.09541382338298372, 0.9533466729105952, -0.11334893719318312, 0.32269174153363517, -1.6838489522648488, -7.730922577004466e-08, -7.133382223849156e-09, 1.419017967675471, -0.21982125170734665, -0.009473360886917056, 0.20977662147777182, 0.3754983019378599, 0.08313391877979381, -0.7851883716159769, -1.018417631615432e-08, 1.204041999905285, 3.823257967644409e-09, -1.3085679728790467, -1.4783321573003494e-06, 1.89106872365066e-06]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 21/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.01    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 1.71            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.2e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  1.75  \n",
            "1   1.8e+00  4.2e-01  6.1e-01  5.09e-02   1.075516576e+01   1.024600840e+01   4.2e-01  6.05  \n",
            "2   3.6e-01  8.6e-02  7.1e-02  9.43e-01   1.631004526e+01   1.623196004e+01   8.6e-02  10.57 \n",
            "3   3.3e-02  7.9e-03  2.2e-03  8.56e-01   2.331621590e+01   2.331028048e+01   7.9e-03  14.96 \n",
            "4   2.4e-02  5.6e-03  1.3e-03  8.83e-01   2.363567675e+01   2.363142081e+01   5.6e-03  18.61 \n",
            "5   1.4e-03  3.3e-04  1.7e-05  1.03e+00   2.426851492e+01   2.426812916e+01   3.3e-04  28.45 \n",
            "6   5.8e-04  1.4e-04  4.7e-06  9.95e-01   2.429159117e+01   2.429144588e+01   1.4e-04  36.30 \n",
            "7   2.8e-05  6.6e-06  5.1e-08  1.00e+00   2.430628604e+01   2.430627937e+01   6.6e-06  40.34 \n",
            "8   2.4e-07  5.6e-08  3.6e-11  1.01e+00   2.430703385e+01   2.430703377e+01   5.6e-08  44.81 \n",
            "9   2.0e-09  3.3e-10  1.9e-15  1.00e+00   2.430704065e+01   2.430704065e+01   7.9e-11  48.70 \n",
            "Optimizer terminated. Time: 48.74   \n",
            "\n",
            "10.692959347310168 10.692959347417634 optimal\n",
            "ok.\n",
            "[-0.9875004966610869, -0.37705773935134135, 0.06518037758515181, 0.37489752140610483, 1.6801202263495063, -0.7051375942633327, -0.26635349876254494, 2.743983489179986e-10, 0.802700666439111, -1.2797517468234973, -0.5722881418536776, 0.3938554230076634, -0.21942843815110533, 0.7109972262931455, -0.8646320689057382, -0.5651129885692558, -0.15243930734646943, -1.5176591957381244e-10, 3.994859939427368e-11, -0.4497909442976243, 1.4237123248819136e-10, 0.4412426611826135, 2.3561245046821286e-09, -0.33705594346184425, 0.3967312442450971, 0.20919992716988525, 1.1950323397234254e-10, 0.04392337953912249, -0.2024031232358718, 1.3216103755282487, 1.8742211892823355, -0.2842816336350017, -0.39131251533080774, -0.7847829573001386, 0.7754364363369485]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 18/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 35.00000000000001 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.03    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 4.26            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   4.4e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  4.34  \n",
            "1   1.5e+00  3.5e-01  4.3e-01  1.17e-01   1.113556117e+01   1.068752118e+01   3.5e-01  11.41 \n",
            "2   3.6e-01  8.1e-02  6.6e-02  1.00e+00   1.529782581e+01   1.521959949e+01   8.1e-02  17.76 \n",
            "3   2.7e-02  6.1e-03  1.5e-03  7.96e-01   2.293645106e+01   2.293098872e+01   6.1e-03  26.09 \n",
            "4   1.3e-02  2.9e-03  5.3e-04  9.62e-01   2.350107910e+01   2.349874832e+01   2.9e-03  33.25 \n",
            "5   1.3e-03  3.0e-04  1.7e-05  9.95e-01   2.395395020e+01   2.395369733e+01   3.0e-04  39.20 \n",
            "6   9.3e-05  2.1e-05  3.1e-07  1.01e+00   2.399536030e+01   2.399534279e+01   2.1e-05  45.29 \n",
            "7   9.7e-07  2.2e-07  3.0e-10  1.01e+00   2.399853356e+01   2.399853329e+01   2.2e-07  50.22 \n",
            "8   1.1e-10  1.7e-10  2.6e-18  1.00e+00   2.399857028e+01   2.399857028e+01   9.8e-13  53.86 \n",
            "Optimizer terminated. Time: 53.92   \n",
            "\n",
            "11.001429723931462 11.001429723932802 optimal\n",
            "ok.\n",
            "[0.5395393529465415, -3.2941401585966945e-11, 0.076183764110692, -0.5660260322766494, 2.614719849093917e-11, 1.2009898139869768, -0.20185634307410763, -2.181279807642314e-10, 2.5188641225020656, -0.6705363201676398, -0.7754765394261873, -0.43260454509343926, -6.465630820828968e-11, -4.718626919341286e-11, -0.432623773913884, 0.3641324515680511, 1.2910252234875699, -0.26824848398929874, 0.5352114746523253, -2.0558604533043914, -7.450349942089332e-12, 2.7292421923198693e-11, 1.7380783487730391, -0.2662236620185491, -0.25129233441542903, 0.08203039921591275, 0.24340725342446995, 1.337544077062818e-12, -0.9858637042001334, -0.4436848936616427, 1.055338774679584, -3.540594007143961e-11, -1.5088508213349525, 0.11605639465245701, 4.1985400471354367e-11]\n",
            "1\n",
            "The problem has 106 noncommuting Hermitian variables\n",
            "Calculating block structure...\n",
            "Estimated number of SDP variables: 5777\n",
            "Generating moment matrix...\n",
            "Reduced number of SDP variables: 5777\n",
            "\u001b[KProcessing 15/140 constraints..."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The objective function has a non-zero 34.99999999999999 constant term. It is not included in the SDP objective.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KProcessing 140/140 constraints...\n",
            "Problem\n",
            "  Name                   :                 \n",
            "  Objective sense        : minimize        \n",
            "  Type                   : CONIC (conic optimization problem)\n",
            "  Constraints            : 5777            \n",
            "  Affine conic cons.     : 0               \n",
            "  Disjunctive cons.      : 0               \n",
            "  Cones                  : 0               \n",
            "  Scalar variables       : 0               \n",
            "  Matrix variables       : 1 (scalarized: 30628)\n",
            "  Integer variables      : 0               \n",
            "\n",
            "Optimizer started.\n",
            "Presolve started.\n",
            "Linear dependency checker started.\n",
            "Linear dependency checker terminated.\n",
            "Eliminator started.\n",
            "Freed constraints in eliminator : 0\n",
            "Eliminator terminated.\n",
            "Eliminator - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - tries                  : 1                 time                   : 0.00            \n",
            "Lin. dep.  - primal attempts        : 1                 successes              : 1               \n",
            "Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
            "Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
            "Presolve terminated. Time: 0.02    \n",
            "GP based matrix reordering started.\n",
            "GP based matrix reordering terminated.\n",
            "Optimizer  - threads                : 1               \n",
            "Optimizer  - solved problem         : the primal      \n",
            "Optimizer  - Constraints            : 5777            \n",
            "Optimizer  - Cones                  : 0               \n",
            "Optimizer  - Scalar variables       : 0                 conic                  : 0               \n",
            "Optimizer  - Semi-definite variables: 1                 scalarized             : 30628           \n",
            "Factor     - setup time             : 5.64            \n",
            "Factor     - dense det. time        : 0.00              GP order time          : 0.00            \n",
            "Factor     - nonzeros before factor : 1.67e+07          after factor           : 1.67e+07        \n",
            "Factor     - dense dim.             : 0                 flops                  : 6.44e+10        \n",
            "ITE PFEAS    DFEAS    GFEAS    PRSTATUS   POBJ              DOBJ              MU       TIME  \n",
            "0   5.1e+00  1.0e+00  2.0e+00  0.00e+00   1.000000000e+00   0.000000000e+00   1.0e+00  5.72  \n",
            "1   1.8e+00  3.5e-01  4.3e-01  1.17e-01   1.113805238e+01   1.069014806e+01   3.5e-01  12.72 \n",
            "2   4.2e-01  8.3e-02  6.8e-02  9.99e-01   1.468044139e+01   1.460294805e+01   8.3e-02  17.22 \n",
            "3   3.0e-02  5.9e-03  1.5e-03  7.95e-01   2.207676338e+01   2.207222687e+01   5.9e-03  21.68 \n",
            "4   2.6e-02  5.2e-03  1.2e-03  8.37e-01   2.221608549e+01   2.221217257e+01   5.2e-03  27.22 \n",
            "5   3.3e-03  6.5e-04  5.4e-05  1.01e+00   2.296617865e+01   2.296563995e+01   6.5e-04  35.46 \n",
            "6   5.0e-04  9.8e-05  3.1e-06  9.77e-01   2.304499861e+01   2.304490843e+01   9.8e-05  43.68 \n",
            "7   4.8e-05  9.5e-06  9.0e-08  1.01e+00   2.305606159e+01   2.305605187e+01   9.5e-06  48.32 \n",
            "8   4.3e-06  8.3e-07  2.3e-09  1.01e+00   2.305720610e+01   2.305720520e+01   8.3e-07  52.64 \n",
            "9   5.4e-07  1.1e-07  1.1e-10  1.01e+00   2.305731089e+01   2.305731080e+01   1.1e-07  57.93 \n",
            "10  1.7e-07  3.1e-08  1.7e-11  1.02e+00   2.305732190e+01   2.305732187e+01   3.1e-08  64.08 \n",
            "11  5.7e-08  9.5e-09  3.0e-14  1.01e+00   2.305732611e+01   2.305732611e+01   4.5e-10  68.02 \n",
            "12  5.1e-10  5.4e-07  7.3e-19  1.00e+00   2.305732614e+01   2.305732614e+01   4.2e-13  76.47 \n",
            "Optimizer terminated. Time: 76.56   \n",
            "\n",
            "11.942673857203587 11.942673857202973 optimal\n",
            "ok.\n",
            "[-0.4603748990401849, -0.6988054499412714, 0.0620014855299335, 2.8157157082296804e-10, 0.7618855475394304, -0.679733973271295, -0.17767937846076456, -0.34602300936317587, 2.1859376433725655, -1.2552512096469604, -1.9228217331940135e-12, 0.5024214130414882, -0.01727012442529116, 1.3043740700178337, -0.26431077849026774, -0.5216708886682008, -1.0711590291216957e-11, -0.1811467670329429, 0.1819210841036669, -1.3225361918847844, -1.4595571230619441e-12, 0.8807419364484247, 0.40974757985387567, -0.7373837433423797, 0.26963919048709406, 6.25575675789356e-11, -0.32292399804972155, -0.06379255793867436, -0.734163076315896, 0.5010335180747065, 2.4657044652968665, -0.816854990691958, -0.8531727262404398, 1.0337635975650966e-12, 0.9604053560017591]\n",
            "1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.0769, 'tpr': 0.9231, 'fpr': 0.5, 'shd': 2, 'nnz': 13, 'precision': 0.4615, 'recall': 0.9231, 'F1': 0.6154, 'gscore': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c9eba70",
      "metadata": {
        "id": "1c9eba70"
      },
      "source": [
        "##__M 3: DirectLiNGAM__\n",
        "* A direct learning algorithm for linear non-Gaussian acyclic model (LiNGAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038699cb",
      "metadata": {
        "id": "038699cb",
        "outputId": "ae12c1fe-0830-4b41-857c-b5d9a57867a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.5, 'tpr': 0.4615, 'fpr': 3.0, 'shd': 9, 'nnz': 12, 'precision': 0.5, 'recall': 0.4615, 'F1': 0.48, 'gscore': 0.0}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import DirectLiNGAM\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "n = DirectLiNGAM()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_DirectLiNGAM')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_DirectLiNGAM.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c3c3f0",
      "metadata": {
        "id": "24c3c3f0"
      },
      "source": [
        "##__M 4: ICALiNGAM__\n",
        "* An ICA-based learning algorithm for linear non-Gaussian acyclic model (LiNGAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a81c35f7",
      "metadata": {
        "id": "a81c35f7",
        "outputId": "5c652330-8ac5-41e7-9125-8d81c7038390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.2, 'tpr': 0.9231, 'fpr': 1.5, 'shd': 3, 'nnz': 15, 'precision': 0.8, 'recall': 0.9231, 'F1': 0.8571, 'gscore': 0.6923}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import ICALiNGAM\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "n = ICALiNGAM()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_ICALiNGAM')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_ICALiNGAM.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1146f7f",
      "metadata": {
        "id": "a1146f7f"
      },
      "source": [
        "##__M 5: PNL*__\n",
        "* Causal discovery based on the post-nonlinear causal assumption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6f4976",
      "metadata": {
        "id": "1d6f4976",
        "outputId": "eb0a207f-8874-46b3-c0a1-7f6d779af2f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'castle.algorithms.gradient.pnl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f0fcdd9de8df>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpnl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPNL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# X, true_dag, _ = load_dataset('IID_Test')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'castle.algorithms.gradient.pnl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from castle.algorithms.gradient.pnl.torch.pnl import PNL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = PNL()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_PNL')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_PNL.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IID/Score-based"
      ],
      "metadata": {
        "id": "r3gL_gWhTd9H"
      },
      "id": "r3gL_gWhTd9H"
    },
    {
      "cell_type": "markdown",
      "id": "f9ae531e",
      "metadata": {
        "id": "f9ae531e"
      },
      "source": [
        "##__M 6: GES*__\n",
        "*\tA classical Greedy Equivalence Search algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8799aa85",
      "metadata": {
        "id": "8799aa85",
        "outputId": "ed46413a-c07a-462d-b5a0-16ac3c54d9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'GES' from 'castle.algorithms' (/usr/local/lib/python3.10/dist-packages/castle/algorithms/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-ba4b294b09e7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcastle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'GES' from 'castle.algorithms' (/usr/local/lib/python3.10/dist-packages/castle/algorithms/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from castle.algorithms import GES\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import load_dataset\n",
        "\n",
        "# X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "algo = GES()\n",
        "algo.learn(X)\n",
        "GraphDAG(algo.causal_matrix, true_dag, save_name='result_ges')\n",
        "met = MetricsDAG(algo.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_ges.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IID/Gradient-based"
      ],
      "metadata": {
        "id": "CO10saR0UVEq"
      },
      "id": "CO10saR0UVEq"
    },
    {
      "cell_type": "markdown",
      "id": "3da9f736",
      "metadata": {
        "id": "3da9f736"
      },
      "source": [
        "##__M 7: NOTEARS_Linear__\n",
        "* A gradient-based algorithm for linear data models (typically with least-squares loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87af24f",
      "metadata": {
        "id": "d87af24f",
        "outputId": "72f9e127-74b0-4ace-e2c3-e175368b16bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20, 'precision': 1.0, 'recall': 1.0, 'F1': 1.0, 'gscore': 1.0}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import Notears\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = Notears()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_Notearslinear')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_Notearslinear.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52690be3",
      "metadata": {
        "id": "52690be3"
      },
      "source": [
        "##__M 8: NOTEARS_Nonlinear_MLP_SOB__\n",
        "* A gradient-based algorithm using neural network modeling for non-linear causal relationships\n",
        "* A gradient-based algorithm using Sobolev space modeling for non-linear causal relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "477b4a85",
      "metadata": {
        "id": "477b4a85",
        "outputId": "9534af94-7532-464b-c365-bf4d8cab77b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fdr': 0.2941, 'tpr': 0.6, 'fpr': 0.2, 'shd': 9, 'nnz': 17, 'precision': 0.7059, 'recall': 0.6, 'F1': 0.6486, 'gscore': 0.35}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import NotearsNonlinear\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = NotearsNonlinear()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_NotearsNonlinear')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_NotearsNonlinear.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcee51d0",
      "metadata": {
        "id": "dcee51d0"
      },
      "source": [
        "##__M 9: NOTEARS-lOW-RANK__\n",
        "* Adapting NOTEARS for large problems with low-rank causal graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7e18d9",
      "metadata": {
        "id": "bb7e18d9",
        "outputId": "906a483f-c723-4c28-fd5d-39b50538686a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 17:40:21,178 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py[line:205] - INFO: [start]: n=2000, d=10, iter_=15, h_=1e-06, rho_=1e+20\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:153: RuntimeWarning: overflow encountered in double_scalars\n",
            "  return loss + 0.5 * rho * h * h + alpha * h\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:172: RuntimeWarning: overflow encountered in multiply\n",
            "  obj_grad = loss_grad + (rho * (np.trace(E) - d) + alpha) * 2 * \\\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:184: RuntimeWarning: overflow encountered in multiply\n",
            "  obj_grad = loss_grad + (rho * (np.trace(E) - d) + alpha) * 2 * \\\n",
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\_matfuncs.py:375: RuntimeWarning: overflow encountered in matmul\n",
            "  eAw = eAw @ eAw\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:173: RuntimeWarning: invalid value encountered in matmul\n",
            "  np.matmul(E.T * W, v)\n",
            "C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py:185: RuntimeWarning: invalid value encountered in matmul\n",
            "  np.matmul(E.T * W, u)\n",
            "2023-04-05 17:40:29,676 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\low_rank.py[line:241] - INFO: FINISHED\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20, 'precision': 1.0, 'recall': 1.0, 'F1': 1.0, 'gscore': 1.0}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from castle.algorithms import NotearsLowRank\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset('IID_Test')\n",
        "rank = np.linalg.matrix_rank(true_dag)\n",
        "n = NotearsLowRank()\n",
        "n.learn(X, rank=rank)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_NotearsLowRank')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_NotearsLowRank.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9bdf5f",
      "metadata": {
        "id": "ca9bdf5f"
      },
      "source": [
        "##__M 10: DAG-GNN__\n",
        "* DAG Structure Learning with Graph Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63caa17",
      "metadata": {
        "id": "a63caa17",
        "outputId": "89f67472-23d5-4d3a-b0d8-970e4b331c01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 17:44:01,665 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:165] - INFO: GPU is available.\n",
            "2023-04-05 17:44:45,446 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 0, epoch: 299, h_new: 0.06046934230157852\n",
            "2023-04-05 17:46:02,916 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 1, epoch: 299, h_new: 0.0037533286602506877\n",
            "2023-04-05 17:47:19,168 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 2, epoch: 299, h_new: 0.0006320721989414579\n",
            "2023-04-05 17:48:34,799 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 3, epoch: 299, h_new: 0.00032116626072564713\n",
            "2023-04-05 17:49:16,357 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 4, epoch: 299, h_new: 4.6692305740947404e-05\n",
            "2023-04-05 17:51:16,879 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 5, epoch: 299, h_new: 5.371589342217931e-06\n",
            "2023-04-05 17:52:45,245 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 6, epoch: 299, h_new: 1.2432905975856556e-06\n",
            "2023-04-05 17:54:11,972 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 7, epoch: 299, h_new: 2.6637384742400627e-07\n",
            "2023-04-05 17:55:37,735 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 8, epoch: 299, h_new: 6.238566108152099e-08\n",
            "2023-04-05 17:57:02,300 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\dag_gnn\\torch\\dag_gnn.py[line:253] - INFO: Iter: 9, epoch: 299, h_new: 7.610358565557362e-09\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.1429, 'tpr': 0.9, 'fpr': 0.12, 'shd': 5, 'nnz': 21, 'precision': 0.8571, 'recall': 0.9, 'F1': 0.878, 'gscore': 0.75}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms.gradient.dag_gnn.torch import DAG_GNN\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset('IID_Test')\n",
        "m = DAG_GNN()\n",
        "m.learn(X)\n",
        "GraphDAG(m.causal_matrix, true_dag, save_name='Result_DAG_GNN')\n",
        "met = MetricsDAG(m.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_DAG_GNN.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ab858a",
      "metadata": {
        "id": "06ab858a"
      },
      "source": [
        "##__M 11: GOLEM__\n",
        "* A more efficient version of NOTEARS that can reduce the number of optimization iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "259185a9",
      "metadata": {
        "id": "259185a9",
        "outputId": "3d0e88d1-2ec7-4f01-9d68-5d285f361c93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 18:00:32,616 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:120] - INFO: GPU is available.\n",
            "2023-04-05 18:00:32,624 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:207] - INFO: Started training for 100000 iterations.\n",
            "2023-04-05 18:00:32,627 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 0] score=67.199, likelihood=67.199, h=0.0e+00\n",
            "2023-04-05 18:00:47,584 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 5000] score=50.079, likelihood=49.589, h=5.0e-04\n",
            "2023-04-05 18:01:01,878 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 10000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:01:13,937 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 15000] score=50.070, likelihood=49.577, h=3.7e-04\n",
            "2023-04-05 18:01:24,871 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 20000] score=50.070, likelihood=49.577, h=3.7e-04\n",
            "2023-04-05 18:01:34,960 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 25000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:01:44,636 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 30000] score=50.070, likelihood=49.577, h=3.7e-04\n",
            "2023-04-05 18:01:54,100 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 35000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:02:03,063 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 40000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:02:11,670 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 45000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:02:20,137 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 50000] score=50.070, likelihood=49.577, h=3.7e-04\n",
            "2023-04-05 18:02:28,354 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 55000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:02:35,835 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 60000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:02:43,372 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 65000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:02:50,787 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 70000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:02:58,407 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 75000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:03:05,905 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 80000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:03:13,348 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 85000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:03:20,865 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 90000] score=50.070, likelihood=49.577, h=3.8e-04\n",
            "2023-04-05 18:03:28,311 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 95000] score=50.070, likelihood=49.577, h=3.7e-04\n",
            "2023-04-05 18:03:35,812 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\notears\\torch\\golem.py[line:220] - INFO: [Iter 100000] score=50.070, likelihood=49.577, h=3.8e-04\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.0, 'tpr': 1.0, 'fpr': 0.0, 'shd': 0, 'nnz': 20, 'precision': 1.0, 'recall': 1.0, 'F1': 1.0, 'gscore': 1.0}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import GOLEM\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _= load_dataset(name='IID_Test')\n",
        "n = GOLEM()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='GOLEM')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_GOLEM.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "771a8c01",
      "metadata": {
        "id": "771a8c01"
      },
      "source": [
        "##__M 12: GraNDAG_Mindspore__\n",
        "* A gradient-based algorithm using neural network modeling for non-linear additive noise data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48aff001",
      "metadata": {
        "id": "48aff001",
        "outputId": "f6e0eac5-c2ab-4504-d8b1-251001eeb586"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 18:05:33,704 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gran_dag\\torch\\gran_dag.py[line:269] - INFO: GPU is available.\n",
            "Training Iterations: 100%|██████████████████████████████████████████████████████| 10000/10000 [01:29<00:00, 111.28it/s]\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAApMAAAEhCAYAAAAj/CseAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyuElEQVR4nO3deXxU5b3H8e9kSCZsCcoSCIYQWlQgopKoZdOqGC8gitZKRQERrCgKMVYRsRKomrpxcQOL4spiqhUFi0quyqLAFSKoV7guFSTFYC6oE0QJJHnuH76Sdsg2Z8icmTz5vF+v80cennPmOYH58nvO6jHGGAEAAAAhiIn0AAAAANB0UUwCAAAgZBSTAAAACBnFJAAAAEJGMQkAAICQUUwCAAAgZBSTAAAACBnFJAAAAEJGMQkAAICQUUxabv369crNzdX3338f6aE0mtWrV8vj8eill16K9FAARAkbs85NV111ldq0aRPpYaCJopi03Pr16zVr1iwCFoDVyDogcigmERbGGP3000+RHgYA1NBUs+mnn36SMSbSwwBqoJiMYp9//rlGjx6tTp06yefzqVevXnrssceq/7yyslJ33XWXTjjhBLVs2VLt2rVT37599dBDD0mScnNzdcstt0iS0tLS5PF45PF4tHr16qDH8Oqrr6pv377y+Xzq0aOHHnroIeXm5srj8QT083g8uuGGG/T444+rV69e8vl8evbZZyVJs2bN0hlnnKFjjz1WCQkJ6tevnxYuXFgjFLt3764LLrhAy5YtU9++fRUfH68ePXro4YcfrnVshw8f1owZM5ScnKyEhAQNGTJEn376adD7BsAO9WVdVa68/PLLOvXUUxUfH69Zs2Zp586d8ng8euaZZ2psz+PxKDc3N6CtoTwOVllZmW6++WZ17txZrVq10plnnqnCwkJ1795dV111VXW/Z555Rh6PR6tWrdLVV1+tjh07qlWrViorK9MXX3yh8ePHq2fPnmrVqpW6du2qESNG6OOPPw74rKpLghYtWqScnBx17txZLVu21FlnnaUtW7bUOr4vvvhCw4YNU5s2bZSSkqKbb75ZZWVljvcTzUuLSA8Atdu2bZsGDBigbt266cEHH1Tnzp315ptvasqUKdq7d69mzpyp++67T7m5ubrjjjt05pln6vDhw/rf//3f6tM8EydO1LfffqtHHnlEL7/8srp06SJJ6t27d1BjeOONN3TJJZfozDPPVH5+vsrLy/XAAw/om2++qbX/K6+8onXr1unOO+9U586d1alTJ0nSzp07de2116pbt26SpI0bN+rGG2/U7t27deeddwZsY+vWrcrOzlZubq46d+6sxYsXa+rUqTp06JD+8Ic/BPS9/fbbNXDgQD355JMqLS3VtGnTNGLECG3fvl1erzfo3zWApq2hrPvggw+0fft23XHHHUpLS1Pr1q0dbT+YPA7W+PHjlZ+fr1tvvVXnnHOOtm3bposvvlilpaW19r/66qs1fPhwPf/88zpw4IBiY2P19ddfq3379vrzn/+sjh076ttvv9Wzzz6rM844Q1u2bNEJJ5wQsI3bb79d/fr105NPPim/36/c3Fz9+te/1pYtW9SjR4/qfocPH9aFF16oCRMm6Oabb9batWv1pz/9SYmJiTWyGghgEJXOP/98c9xxxxm/3x/QfsMNN5j4+Hjz7bffmgsuuMCccsop9W7n/vvvN5LMjh07HI/htNNOMykpKaasrKy6bf/+/aZ9+/bmyH86kkxiYqL59ttv691mRUWFOXz4sJk9e7Zp3769qaysrP6z1NRU4/F4zNatWwPWOe+880xCQoI5cOCAMcaYd955x0gyw4YNC+j317/+1UgyGzZscLyvAJq2urIuNTXVeL1e8+mnnwa079ixw0gyTz/9dI1tSTIzZ86s/jmYPA7GJ598YiSZadOmBbQvXbrUSDLjxo2rbnv66aeNJDN27NgGt1teXm4OHTpkevbsaW666abq9qqs7NevX0DW7ty508TGxpqJEydWt40bN85IMn/9618Dtj1s2DBzwgknBLV/aL44zR2FDh48qLfeeksXX3yxWrVqpfLy8upl2LBhOnjwoDZu3KjTTz9dH374oa6//nq9+eabdc5sQ3HgwAFt3rxZI0eOVFxcXHV7mzZtNGLEiFrXOeecc3TMMcfUaH/77bc1ZMgQJSYmyuv1KjY2Vnfeeaf27dunkpKSgL59+vTRySefHNA2evRolZaW6oMPPghov/DCCwN+7tu3ryTpq6++Cn5HAVivb9++Ov7440NaN9g8DsaaNWskSZdddllA+6WXXqoWLWo/Ufib3/ymRlt5ebnuuece9e7dW3FxcWrRooXi4uL0+eefa/v27TX6jx49OuDSpNTUVA0YMEDvvPNOQD+Px1Mj3/v27UumokEUk1Fo3759Ki8v1yOPPKLY2NiAZdiwYZKkvXv3avr06XrggQe0ceNGDR06VO3bt9e5556rzZs3H/UYvvvuOxljlJSUVOPPamuTVH1q6d+9//77ysrKkiQ98cQTeu+997Rp0ybNmDFDUs0L4Tt37lxjG1Vt+/btC2hv3759wM8+n6/WbQJo3mrLpmAFm8fBbkuqmaEtWrSokWf1jT0nJ0d//OMfNXLkSK1YsUL//d//rU2bNunkk0+uNf/qytUjM7VVq1aKj48PaPP5fDp48GD9O4Zmj2smo9Axxxwjr9erMWPGaPLkybX2SUtLU4sWLZSTk6OcnBx9//33+q//+i/dfvvtOv/881VUVKRWrVod1Rg8Hk+t10fu2bOn1nWOvClHkl544QXFxsbqtddeCwipV155pdZt1Lbtqra6whYA6lNbNlXl0ZE3lxxZYAWbx8GoyrBvvvlGXbt2rW4vLy+v8bn1jX3RokUaO3as7rnnnoD2vXv3ql27djX615WrZCoaC8VkFGrVqpXOPvtsbdmyRX379g04zVyXdu3a6dJLL9Xu3buVnZ2tnTt3qnfv3iEfrWvdurUyMzP1yiuv6IEHHqgeww8//KDXXnst6O14PB61aNEi4IaYn376Sc8//3yt/T/55BN9+OGHAae6lyxZorZt26pfv36O9gFA8+E065KSkhQfH6+PPvoooP3VV18N+DmUPK7LmWeeKUnKz88PyLOXXnpJ5eXlQW/H4/FU72+Vv//979q9e7d++ctf1ui/dOlS5eTkVBemX331ldavX6+xY8eGshtADRSTUeqhhx7SoEGDNHjwYF133XXq3r279u/fry+++EIrVqzQ22+/rREjRig9PV2ZmZnq2LGjvvrqK82dO1epqanq2bOnJOmkk06q3t64ceMUGxurE044QW3btm1wDLNnz9bw4cN1/vnna+rUqaqoqND999+vNm3a6Ntvvw1qP4YPH645c+Zo9OjR+v3vf699+/bpgQceqBGEVZKTk3XhhRcqNzdXXbp00aJFi1RQUKB77733qI60ArBbXVlXF4/HoyuvvFJPPfWUfvGLX+jkk0/W+++/ryVLltToG0weB6NPnz66/PLL9eCDD8rr9eqcc87RJ598ogcffFCJiYmKiQnuyrMLLrhAzzzzjE488UT17dtXhYWFuv/++3XcccfV2r+kpEQXX3yxrrnmGvn9fs2cOVPx8fGaPn16UJ8HNCjSdwChbjt27DBXX3216dq1q4mNjTUdO3Y0AwYMMHfddZcxxpgHH3zQDBgwwHTo0MHExcWZbt26mQkTJpidO3cGbGf69OkmOTnZxMTEGEnmnXfeCXoMy5YtMyeddFL19v/85z+bKVOmmGOOOSagnyQzefLkWrfx1FNPmRNOOMH4fD7To0cPk5eXZxYuXFjjzsvU1FQzfPhw89JLL5k+ffqYuLg40717dzNnzpyA7VXdofjiiy/W+H2pjrszAdivtqyrypXa+P1+M3HiRJOUlGRat25tRowYYXbu3Fnjbm5jGs7jYB08eNDk5OSYTp06mfj4ePOrX/3KbNiwwSQmJgbciV11N/emTZtqbOO7774zEyZMMJ06dTKtWrUygwYNMuvWrTNnnXWWOeuss6r7VWXl888/b6ZMmWI6duxofD6fGTx4sNm8eXPANseNG2dat25d47NmzpxZ4+kdwJE8xvA4fQTv8OHDOuWUU9S1a1etWrWqUbfdvXt3paenOzqNDgBN3fr16zVw4EAtXrxYo0ePbrTtrl69WmeffbZefPFFXXrppY22XeBI3M2Nek2YMEEvvPCC1qxZo/z8fGVlZWn79u269dZbIz00WGbt2rUaMWKEkpOT5fF46rxJ69+tWbNGGRkZ1W9Levzxx8M/UOAoFBQUaPbs2fr73/+ut99+W//5n/+piy++WD179tQll1wS6eGhiYtUjnLNZDNUWVmpysrKevtUPfNs//79+sMf/qD/+7//U2xsrPr166eVK1dqyJAhbgwVzciBAwd08skna/z48bU+W+9IO3bs0LBhw3TNNddo0aJFeu+993T99derY8eOQa0PNKaKiop635vt8Xjk9XqVkJCgVatWae7cudq/f786dOigoUOHKi8vr8ZjeQCnIpWjnOZuhnJzczVr1qx6++zYsUPdu3d3Z0DAETwej5YtW6aRI0fW2WfatGlavnx5wEOaJ02apA8//FAbNmxwYZTAv3Tv3r3eh3ufddZZWr16tXsDQrPnZo5yZLIZ+v3vf68LLrig3j7JyckujQZNycGDB3Xo0KGg+xtjajwnz+fz1Xk3vxMbNmyofiB+lfPPP18LFy7U4cOHFRsbe9SfAQRrxYoVNZ5Z+e+CeYIGmgcbc5RishlKTk6mWIRjBw8eVMuWLR2t06ZNG/3www8BbTNnzlRubu5Rj2fPnj013iSSlJSk8vJy7d2796jeegI4VfVoIqA+tuYoxSSAoDiZSVf54YcfVFRUpISEhOq2xphNVzlytl511U5tbw0BgEizNUddLyYrKyv19ddfq23btgQ+EAHGGO3fv1/JyclBPyT533k8nqC+u8YYGWOUkJAQEIKNpXPnzjVeE1dSUlLve45tQY4CkUWOBnK9mPz666+VkpLi9scCOEJRUVGdb8yoT7AhKKneu1uPVv/+/bVixYqAtlWrVikzM9P66yXJUSA6kKM/c72YrLoI+chDtgDcUVpaqpSUlJBvCIiJiQl6Rt3QI6j+3Q8//KAvvvii+ucdO3Zo69atOvbYY9WtWzdNnz5du3fv1nPPPSfp5zsOH330UeXk5Oiaa67Rhg0btHDhQi1dutT5TjUx5KgdEhMTw7Ztv98ftm03NeH8PZOjP3O9mKz65YXrkC2A4IR6etRJCDqxefNmnX322dU/5+TkSJLGjRunZ555RsXFxdq1a1f1n6elpWnlypW66aab9Nhjjyk5OVkPP/xws3jGJDmKhvDvwh3k6M9cf85kaWmpEhMT5ff7+ccORECo38Gq9Xw+X9AhWFZWxnc9DMhRO4TzelceIf0v4fw9k6M/425uAI44udYHAFCTbTlKMQnAEdtCEADcZluOUkwCcCRc1/oAQHNhW446fziSpHnz5iktLU3x8fHKyMjQunXrGntcAKJU1Yw6mAV1I0eB5su2HHVcTObn5ys7O1szZszQli1bNHjwYA0dOjTg7iAA9rItBCOBHAWaN9ty1HExOWfOHE2YMEETJ05Ur169NHfuXKWkpGj+/PnhGB+AKGNbCEYCOQo0b7blqKNi8tChQyosLFRWVlZAe1ZWltavX1/rOmVlZSotLQ1YADRdtoWg28hRALblqKNicu/evaqoqFBSUlJAe1JSUo13O1bJy8tTYmJi9cIrwICmLSYmRl6vt8EllPfVNgfkKADbcjSkUR5ZKRtj6qyep0+fLr/fX70UFRWF8pEAooRtM+pIIUeB5su2HHX0aKAOHTrI6/XWmD2XlJTUmGVX8fl88vl8oY8QQFQJNuCaSgi6jRwFYFuOOjoyGRcXp4yMDBUUFAS0FxQUaMCAAY06MADRybYZtdvIUQC25ajjh5bn5ORozJgxyszMVP/+/bVgwQLt2rVLkyZNCsf4AEQZ22bUkUCOAs2bbTnquJgcNWqU9u3bp9mzZ6u4uFjp6elauXKlUlNTwzE+AFHGthCMBHIUaN5sy1GPcfldPaWlpUpMTJTf71dCQoKbHw1AoX8Hq9br3LlzUHcYVlZWas+ePXzXw4ActUM4C4Wm8ho+N4Tz90yO/ox3cwNwxLYZNQC4zbYcpZgE4IhtIQgAbrMtRykmAThiWwgCgNtsy1GKSQCOxMTENJm3MgBANLItRykmAThiWwgC9WkqR4Zs0Jx+17blKMUkAEdsOz0DAG6zLUcpJgE4YlsIAoDbbMtRikkAjtgWggDgNttylGISgGNNJeAAIFrZlKMUkwAcCfbCcd7AAQC1sy1HKSYBOGLb6RkAcJttOUoxCcAR20IQANxmW45STAJwxOv1yuv1RnoYANBk2ZajFJMAHLHtWh8AcJttOUoxCcAR207PAIDbbMtRikkAjtgWggDgNttylGISgCO2nZ4BALfZlqMUkwAcsW1GDQBusy1HKSYBOGLbjBoA3GZbjlJMAnDEthk1ALjNthylmATgiMfjCWpGXVlZ6cJoAKDpsS1HG94TAPg3VadnglmcmjdvntLS0hQfH6+MjAytW7eu3v6LFy/WySefrFatWqlLly4aP3689u3bF+quAYArbMtRikkAjoQrBPPz85Wdna0ZM2Zoy5YtGjx4sIYOHapdu3bV2v/dd9/V2LFjNWHCBH3yySd68cUXtWnTJk2cOLExdhMAwsa2HKWYBOBI1bU+wSxOzJkzRxMmTNDEiRPVq1cvzZ07VykpKZo/f36t/Tdu3Kju3btrypQpSktL06BBg3Tttddq8+bNjbGbABA2tuUoxSQAR5zOqEtLSwOWsrKyGts8dOiQCgsLlZWVFdCelZWl9evX1zqOAQMG6J///KdWrlwpY4y++eYbvfTSSxo+fHjj7zQANCLbcpRiEoAjTmfUKSkpSkxMrF7y8vJqbHPv3r2qqKhQUlJSQHtSUpL27NlT6zgGDBigxYsXa9SoUYqLi1Pnzp3Vrl07PfLII42/0wDQiGzLUe7mBuCI00daFBUVKSEhobrd5/M1uE4VY0ydn7Vt2zZNmTJFd955p84//3wVFxfrlltu0aRJk7Rw4cJgdqXJS0xMDMt2m8qz7WCXpvTvrrS09Ki+f7blKMUkAEeCvSi8qk9CQkJACNamQ4cO8nq9NWbPJSUlNWbZVfLy8jRw4EDdcsstkqS+ffuqdevWGjx4sO666y516dIlmN0BANfZlqOc5gbgSDguHI+Li1NGRoYKCgoC2gsKCjRgwIBa1/nxxx9rhLHX65XUtI5wAGh+bMtRjkwCcMTpjDpYOTk5GjNmjDIzM9W/f38tWLBAu3bt0qRJkyRJ06dP1+7du/Xcc89JkkaMGKFrrrlG8+fPrz49k52drdNPP13JycnOdwwAXGJbjlJMAnAkXCE4atQo7du3T7Nnz1ZxcbHS09O1cuVKpaamSpKKi4sDnpV21VVXaf/+/Xr00Ud18803q127djrnnHN07733OtshAHCZbTnqMS6fD6q6aNXv9zd4/h9A4wv1O1i13nnnnafY2NgG+x8+fFgFBQV818PgaC/+bwiXCfxLU3k38pH4OwwvcjQQRyYBOOL0LkQAQCDbcpRiEoAj4To9AwDNhW056miUeXl5Ou2009S2bVt16tRJI0eO1KeffhqusQGIQuF6DVhzQY4CsC1HHRWTa9as0eTJk7Vx40YVFBSovLxcWVlZOnDgQLjGByDKOH0NGAKRowBsy1FHp7nfeOONgJ+ffvppderUSYWFhTrzzDMbdWAAopNt1/q4jRwFYFuOHtU1k36/X5J07LHH1tmnrKws4IXkpaWlR/ORACLMthCMNHIUaH5sy9GQj58aY5STk6NBgwYpPT29zn55eXkBLydPSUkJ9SMBRAHbrvWJJHIUaJ5sy9GQi8kbbrhBH330kZYuXVpvv+nTp8vv91cvRUVFoX4kgChgWwhGEjkKNE+25WhIp7lvvPFGLV++XGvXrtVxxx1Xb1+fzyefzxfS4ABEH9seaREp5CjQfNmWo46KSWOMbrzxRi1btkyrV69WWlpauMYFIErZdq2P28hRALblqKNicvLkyVqyZIleffVVtW3bVnv27JEkJSYmqmXLlmEZIIDoYlsIuo0cBWBbjjo6fjp//nz5/X79+te/VpcuXaqX/Pz8cI0PQJSx7flobiNHAdiWo45PcwNo3mybUbuNHAVgW47ybm4AjjWVgAOAaGVTjlJMAnDEthk1ALjNthylmATgiG0hCABusy1HKSYBOGJbCAKA22zLUYpJIIpFY5DY9rDdpszv9yshISHSw7AaN0w1feRo+FFMAnDEthk1ALjNthylmATgiG0hCABusy1HKSYBOGJbCAKA22zLUYpJAI7YFoIA4DbbcpRiEoAjtoUgALjNthylmATgiG0hCABusy1HKSYBOGJbCAKA22zLUYpJAI7YFoIA4DbbcpRiEoAjtj1sFwDcZluOUkwCcMS2GTUAuM22HKWYBOCIbSEIAG6zLUcpJgE41lQCDgCilU05SjEJwBHbZtQA4DbbcpRiEoAjtoUgALjNthylmATgiG0hCABusy1HKSYBOGJbCAKA22zLUYpJAI7YFoIA4DbbcrRpPA0TQNTwer1BL07NmzdPaWlpio+PV0ZGhtatW1dv/7KyMs2YMUOpqany+Xz6xS9+oaeeeirUXQMAV9iWoxyZBOBIuGbU+fn5ys7O1rx58zRw4ED95S9/0dChQ7Vt2zZ169at1nUuu+wyffPNN1q4cKF++ctfqqSkROXl5Y4+FwDcZluOUkwCcCRcIThnzhxNmDBBEydOlCTNnTtXb775pubPn6+8vLwa/d944w2tWbNGX375pY499lhJUvfu3R19JgBEgm05ymluAI5UhWAwiySVlpYGLGVlZTW2eejQIRUWFiorKyugPSsrS+vXr691HMuXL1dmZqbuu+8+de3aVccff7z+8Ic/6Keffmr8nQaARmRbjnJkMgqE6wJbY0xYtgv3hOPvsLS0VImJiSGv73RGnZKSEtA+c+ZM5ebmBrTt3btXFRUVSkpKCmhPSkrSnj17at3+l19+qXfffVfx8fFatmyZ9u7dq+uvv17ffvst1002Q+QomhLbcpRiEoAjTkOwqKhICQkJ1e0+n6/BdaoYY+r8rMrKSnk8Hi1evLi6OJ4zZ44uvfRSPfbYY2rZsmWDYwSASLAtRykmATjiNAQTEhICQrA2HTp0kNfrrTF7LikpqTHLrtKlSxd17do14Chrr169ZIzRP//5T/Xs2bPBMQJAJNiWo1wzCcARp9f6BCMuLk4ZGRkqKCgIaC8oKNCAAQNqXWfgwIH6+uuv9cMPP1S3ffbZZ4qJidFxxx0X2s4BgAtsy1GKSQCOhCMEJSknJ0dPPvmknnrqKW3fvl033XSTdu3apUmTJkmSpk+frrFjx1b3Hz16tNq3b6/x48dr27ZtWrt2rW655RZdffXVnOIGENVsy1FOcwNwJFyPtBg1apT27dun2bNnq7i4WOnp6Vq5cqVSU1MlScXFxdq1a1d1/zZt2qigoEA33nijMjMz1b59e1122WW66667nO0QALjMthz1GJdvVau6k9Tv9zd4/r+54C5EuCnU72DVerfffrvi4+Mb7H/w4EHdc889fNfDgBytiRxFXcL5SkJy9GccmQTgiG3vlAUAt9mWo0d1zWReXp48Ho+ys7MbaTgAol24rvVprshRoPmxLUdDPjK5adMmLViwQH379m3M8QCIcrbNqCOJHAWaJ9tyNKQjkz/88IOuuOIKPfHEEzrmmGMae0wAophtM+pIIUeB5su2HA2pmJw8ebKGDx+uIUOGNNi3rKysxjslATRdtoVgpJCjQPNlW446Ps39wgsvqLCwUJs3bw6qf15enmbNmuV4YACik22nZyKBHAWaN9ty1NGRyaKiIk2dOlWLFy8O6pZ26ecHZPr9/uqlqKgopIECiA62zajdRo4CsC1HHR2ZLCwsVElJiTIyMqrbKioqtHbtWj366KMqKyuT1+sNWMfn89X7QnIATYttM2q3kaMAbMtRR8Xkueeeq48//jigbfz48TrxxBM1bdq0GgEIwD5erzeo7zp5UDtyFIBtOeqomGzbtq3S09MD2lq3bq327dvXaAdgJ9tm1G4jRwHYlqO8AQeAI7aFIAC4zbYcPepicvXq1Y0wDABNhW0hGA3IUaB5sS1HOTIJwBHbQhAA3GZbjlJMAnCsqQQcAEQrm3KUYhKAI7bNqAHAbbblKMUkAEdsC0EAcJttOUoxGQWMMZEeAqJUNAaJbSEIO5CjaEpsy1GKSQCO2PawXQBwm205SjEJwBHbZtQA4DbbcpRiEoAjtoUgALjNthylmATgSExMjGJiYoLqBwCoybYcpZgE4IhtM2oAcJttOUoxCcAR20IQANxmW45STAJwxLYQBAC32ZajFJMAHLEtBAHAbbblKMUkAEdsu3AcANxmW45STAJwxOPxBBVwTWVGDQBusy1HKSYBOGLb6RkAcJttOUoxCcAR207PAIDbbMtRikkAjtg2owYAt9mWoxSTAByxLQQBwG225SjFJABHbAtBAHCbbTlKMQnAEdtCEADcZluOUkwCcMS2C8cBwG225SjFJABHbJtRA4DbbMtRikkAjtg2owYAt9mWo01jlACiRlUIBrM4NW/ePKWlpSk+Pl4ZGRlat25dUOu99957atGihU455RTHnwkAbrMtRykmAThSdXommMWJ/Px8ZWdna8aMGdqyZYsGDx6soUOHateuXfWu5/f7NXbsWJ177rlHs1sA4BrbcpRiEoAj4QrBOXPmaMKECZo4caJ69eqluXPnKiUlRfPnz693vWuvvVajR49W//79j2a3AMA1tuUoxSQQxYwxjb74/f6jGpPTECwtLQ1YysrKamzz0KFDKiwsVFZWVkB7VlaW1q9fX+dYnn76af3jH//QzJkzj2qfEMjJ33G4/lMEGgs5Gv4cpZgE4IjTEExJSVFiYmL1kpeXV2Obe/fuVUVFhZKSkgLak5KStGfPnlrH8fnnn+u2227T4sWL1aIF9xICaDpsy1ESGIAjHo8nqIvCq0KwqKhICQkJ1e0+n6/BdaoYY2o9olVRUaHRo0dr1qxZOv7444MdOgBEBdtylGISgCNOn4+WkJAQEIK16dChg7xeb43Zc0lJSY1ZtiTt379fmzdv1pYtW3TDDTdIkiorK2WMUYsWLbRq1Sqdc845we4SALjKthylmATgSDgethsXF6eMjAwVFBTo4osvrm4vKCjQRRddVKN/QkKCPv7444C2efPm6e2339ZLL72ktLS0oD8bANxmW45STAJwJBwhKEk5OTkaM2aMMjMz1b9/fy1YsEC7du3SpEmTJEnTp0/X7t279dxzzykmJkbp6ekB63fq1Enx8fE12gEg2tiWoxSTABzxer3yer1B9XNi1KhR2rdvn2bPnq3i4mKlp6dr5cqVSk1NlSQVFxc3+Kw0AGgKbMtRjzHGOFlh9+7dmjZtml5//XX99NNPOv7447Vw4UJlZGQEtX5paakSExPl9/sbPP8PoPGF+h2sWm/FihVq3bp1g/0PHDigESNG8F2vRbTnaLge4+PwvxsgapGjgRwdmfzuu+80cOBAnX322Xr99dfVqVMn/eMf/1C7du3CNDwA0SZcp2eaC3IUgG056qiYvPfee5WSkqKnn366uq179+6NPSYAUcy2EHQbOQrAthx19NDy5cuXKzMzU7/97W/VqVMnnXrqqXriiSfqXaesrKzGk9sBNF288eTokKMAbMtRR8Xkl19+qfnz56tnz5568803NWnSJE2ZMkXPPfdcnevk5eUFPLU9JSXlqAcNIHJsC0G3kaMAbMtRRzfgxMXFKTMzM+Adj1OmTNGmTZu0YcOGWtcpKysLeIdkaWmpUlJSov5iUsBWR3vh+BtvvBH0heP/8R//wXf9CE0hR7kBB6gfORrI0TWTXbp0Ue/evQPaevXqpb/97W91ruPz+ep97Q+ApsW2a33cRo4CsC1HHRWTAwcO1KeffhrQ9tlnn1U/vwiA/WwLQbeRowBsy1FH10zedNNN2rhxo+655x598cUXWrJkiRYsWKDJkyeHa3wAooxt1/q4jRwFYFuOOiomTzvtNC1btkxLly5Venq6/vSnP2nu3Lm64oorwjU+AFHGthB0GzkKwLYcdfw6xQsuuEAXXHBBOMYCoAmw7fRMJJCjQPNmW47ybm4AjjWVgAOAaGVTjlJMAnDEthk1ALjNthylmATgiG0hCABusy1HKSYBOGJbCAKA22zLUYpJAECAcL2ppqn8x2gD3jYEN1FMAnDEthk1ALjNthylmATgSExMjGJiGn5EbTB9AKA5si1HKSYBOGLbjBoA3GZbjlJMAnDEthAEALfZlqMUkwAcsS0EAcBttuUoxSQAR2wLQQBwm205SjEJwBHbQhAA3GZbjjaN24QAAAAQlTgyCcAR22bUAOA223KUYhKAI7aFIAC4zbYcpZgE4IhtD9sFALfZlqMUkwAcsW1GDQBusy1HKSYBOGJbCAKA22zL0aZx/BQAAABRiSOTABxrKrNlAIhWNuUoxSQAR2w7PQMAbrMtRznNDQAAgJBxZBKAI7bNqAHAbbblKMUkAEdsC0EAcJttOUoxCcAR20IQANxmW45yzSQAR6pCMJjFqXnz5iktLU3x8fHKyMjQunXr6uz78ssv67zzzlPHjh2VkJCg/v3768033zyaXQMAV9iWoxSTABwJVwjm5+crOztbM2bM0JYtWzR48GANHTpUu3btqrX/2rVrdd5552nlypUqLCzU2WefrREjRmjLli2NsZsAEDa25ajHGGMcrXGUSktLlZiYKL/fr4SEBDc/GoBC/w5Wrfc///M/atu2bYP99+/fr/T09KA/54wzzlC/fv00f/786rZevXpp5MiRysvLC2qMffr00ahRo3TnnXcG1b+pIkftEM5TmC7/1x7Vwvl7Jkd/xpFJAI44nVGXlpYGLGVlZTW2eejQIRUWFiorKyugPSsrS+vXrw9qXJWVldq/f7+OPfbYo99JAAgj23KUYhKAI05DMCUlRYmJidVLbbPjvXv3qqKiQklJSQHtSUlJ2rNnT1DjevDBB3XgwAFddtllR7+TABBGtuUod3MDcMTpXYhFRUUBp2d8Pl+D61QxxgT1WUuXLlVubq5effVVderUqcH+ABBJtuUoxSSAsEpISGjwWp8OHTrI6/XWmD2XlJTUmGUfKT8/XxMmTNCLL76oIUOGHPV4ASDaRHuOcpobgCPhuAsxLi5OGRkZKigoCGgvKCjQgAED6lxv6dKluuqqq7RkyRINHz485H0CADfZlqMcmQTgSLgetpuTk6MxY8YoMzNT/fv314IFC7Rr1y5NmjRJkjR9+nTt3r1bzz33nKSfA3Ds2LF66KGH9Ktf/ap6Nt6yZUslJiY63CsAcI9tOeroyGR5ebnuuOMOpaWlqWXLlurRo4dmz56tyspKJ5sB0ISF6/loo0aN0ty5czV79mydcsopWrt2rVauXKnU1FRJUnFxccCz0v7yl7+ovLxckydPVpcuXaqXqVOnNur+NjZyFIBtOeroyOS9996rxx9/XM8++6z69OmjzZs3a/z48UpMTIz6AAfQOMI1o5ak66+/Xtdff32tf/bMM88E/Lx69WrH248G5CgA23LUUTG5YcMGXXTRRdXn1Lt3766lS5dq8+bNjTIYANEvnCHYHJCjAGzLUUenuQcNGqS33npLn332mSTpww8/1Lvvvqthw4bVuU5ZWVmNh20CQHNFjgKwjaMjk9OmTZPf79eJJ54or9eriooK3X333br88svrXCcvL0+zZs066oECiA62zajdRo4CsC1HHR2ZzM/P16JFi7RkyRJ98MEHevbZZ/XAAw/o2WefrXOd6dOny+/3Vy9FRUVHPWgAkROuC8ebC3IUgG056ujI5C233KLbbrtNv/vd7yRJJ510kr766ivl5eVp3Lhxta7j8/nqfVI7gKbFthm128hRALblqKMjkz/++KNiYgJX8Xq9PNICAIJEjgKwjaMjkyNGjNDdd9+tbt26qU+fPtqyZYvmzJmjq6++OlzjAxCFmspsORqRowAku3LUUTH5yCOP6I9//KOuv/56lZSUKDk5Wddee63uvPPOcI0PQJSx7fSM28hRALblqKNism3btpo7d67mzp0bpuEAiHa2haDbyFEAtuUo7+YG4IhtIQgAbrMtRx3dgAMAAAD8O45MAnDEthk1ALjNthzlyCQAAABCxpFJAI7YNqMGIsUYE+khNAvh+D2XlpYqMTEx5PVty1GOTAIAACBkHJkE4IhtM2oAcJttOUoxCcAR20IQANxmW45ymhsAAAAh48gkAEdsm1EDgNtsy1GOTAIAACBkHJkE4IhtM2oAcJttOcqRSQAAAISMI5MAHLFtRg0AbrMtRzkyCQAAgJBRTAIAACBknOYG4Ihtp2cAwG225SjFJABHbAtBAHCbbTnKaW4AAACEjCOTAByxbUYNAG6zLUc5MgkAAICQcWQSgCO2zagBwG225ShHJgEAABAyjkwCcMS2GTUAuM22HOXIJAAAAELGkUkAjtg2owYAt9mWo64Xk8YYSVJpaanbHw1A//ruVX0XnQpnCM6bN0/333+/iouL1adPH82dO1eDBw+us/+aNWuUk5OjTz75RMnJybr11ls1adIkx5/b1JCjQGSRo0cwLisqKjKSWFhYIrwUFRU5+u76/X4jyXz//femsrKyweX77783kozf7w9q+y+88IKJjY01TzzxhNm2bZuZOnWqad26tfnqq69q7f/ll1+aVq1amalTp5pt27aZJ554wsTGxpqXXnrJ0X41ReQoC0t0LOTozzzGhFhWh6iyslJff/212rZt22DFXVpaqpSUFBUVFSkhIcGlER4dxuwOxhw6Y4z279+v5ORkxcQEf9l0aWmpEhMT5ff7gxq/0/5nnHGG+vXrp/nz51e39erVSyNHjlReXl6N/tOmTdPy5cu1ffv26rZJkybpww8/1IYNG4Lcq6aJHI0+jNkd0TJmcjSQ66e5Y2JidNxxxzlaJyEhocn8Q6/CmN3BmEOTmJgY8rrBnlqt6ndkf5/PJ5/PF9B26NAhFRYW6rbbbgtoz8rK0vr162vd/oYNG5SVlRXQdv7552vhwoU6fPiwYmNjgxpnU0SORi/G7I5oGDM5+i/cgAMgKHFxcercubNSUlKCXqdNmzY1+s+cOVO5ubkBbXv37lVFRYWSkpIC2pOSkrRnz55at71nz55a+5eXl2vv3r3q0qVL0OMEADfYmqMUkwCCEh8frx07dujQoUNBr2OMqXEa9sjZ9L87sm9t6zfUv7Z2AIgGtuZoVBeTPp9PM2fOrPeXFm0YszsYc2TEx8crPj6+0bfboUMHeb3eGrPnkpKSGrPmKp07d661f4sWLdS+fftGH2NT1RT/3TFmdzDmyLAxR12/AQcAanPGGWcoIyND8+bNq27r3bu3LrroojovHF+xYoW2bdtW3Xbddddp69at1t+AAwC1iViOOrr3GwDCpOqRFgsXLjTbtm0z2dnZpnXr1mbnzp3GGGNuu+02M2bMmOr+VY+0uOmmm8y2bdvMwoULm82jgQCgNpHK0ag+zQ2g+Rg1apT27dun2bNnq7i4WOnp6Vq5cqVSU1MlScXFxdq1a1d1/7S0NK1cuVI33XSTHnvsMSUnJ+vhhx/Wb37zm0jtAgBEVKRylNPcAAAACFnwT9oEAAAAjkAxCQAAgJBFbTE5b948paWlKT4+XhkZGVq3bl2kh1SvvLw8nXbaaWrbtq06deqkkSNH6tNPP430sIKWl5cnj8ej7OzsSA+lQbt379aVV16p9u3bq1WrVjrllFNUWFgY6WHVqby8XHfccYfS0tLUsmVL9ejRQ7Nnz1ZlZWWkhwbLkaPuIkfDhxyNblFZTObn5ys7O1szZszQli1bNHjwYA0dOjTgotFos2bNGk2ePFkbN25UQUGBysvLlZWVpQMHDkR6aA3atGmTFixYoL59+0Z6KA367rvvNHDgQMXGxur111/Xtm3b9OCDD6pdu3aRHlqd7r33Xj3++ON69NFHtX37dt133326//779cgjj0R6aLAYOeoucjS8yNEo14h3pDea008/3UyaNCmg7cQTTzS33XZbhEbkXElJiZFk1qxZE+mh1Gv//v2mZ8+epqCgwJx11llm6tSpkR5SvaZNm2YGDRoU6WE4Mnz4cHP11VcHtF1yySXmyiuvjNCI0ByQo+4hR8OPHI1uUXdksupF5Ue+eLy+F5VHI7/fL0k69thjIzyS+k2ePFnDhw/XkCFDIj2UoCxfvlyZmZn67W9/q06dOunUU0/VE088Eelh1WvQoEF666239Nlnn0mSPvzwQ7377rsaNmxYhEcGW5Gj7iJHw48cjW5R95zJUF5UHm2MMcrJydGgQYOUnp4e6eHU6YUXXlBhYaE2b94c6aEE7csvv9T8+fOVk5Oj22+/Xe+//76mTJkin8+nsWPHRnp4tZo2bZr8fr9OPPFEeb1eVVRU6O6779bll18e6aHBUuSoe8hRd5Cj0S3qiskqTl9UHk1uuOEGffTRR3r33XcjPZQ6FRUVaerUqVq1alVY3hEaLpWVlcrMzNQ999wjSTr11FP1ySefaP78+VEbgvn5+Vq0aJGWLFmiPn36aOvWrcrOzlZycrLGjRsX6eHBYuRoeJGj7iFHo1vUFZOhvKg8mtx4441avny51q5dq+OOOy7Sw6lTYWGhSkpKlJGRUd1WUVGhtWvX6tFHH1VZWZm8Xm8ER1i7Ll26qHfv3gFtvXr10t/+9rcIjahht9xyi2677Tb97ne/kySddNJJ+uqrr5SXl0cIIizIUXeQo+4hR6Nb1F0zGRcXp4yMDBUUFAS0FxQUaMCAAREaVcOMMbrhhhv08ssv6+2331ZaWlqkh1Svc889Vx9//LG2bt1avWRmZuqKK67Q1q1bozIAJWngwIE1HhXy2WefVb8qKhr9+OOPiokJ/Kp5vV4eaYGwIUfdQY66hxyNcpG8+6cuDb2oPBpdd911JjEx0axevdoUFxdXLz/++GOkhxa0pnAX4vvvv29atGhh7r77bvP555+bxYsXm1atWplFixZFemh1GjdunOnatat57bXXzI4dO8zLL79sOnToYG699dZIDw0WI0cjgxwND3I0ukVlMWmMMY899phJTU01cXFxpl+/flH/aAhJtS5PP/10pIcWtKYQgsYYs2LFCpOenm58Pp858cQTzYIFCyI9pHqVlpaaqVOnmm7dupn4+HjTo0cPM2PGDFNWVhbpocFy5Kj7yNHwIEejm8cYYyJzTBQAAABNXdRdMwkAAICmg2ISAAAAIaOYBAAAQMgoJgEAABAyikkAAACEjGISAAAAIaOYBAAAQMgoJgEAABAyikkAAACEjGISAAAAIaOYBAAAQMj+H/d6YpBz48hYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.0, 'tpr': 0.15, 'fpr': 0.0, 'shd': 17, 'nnz': 3, 'precision': 1.0, 'recall': 0.15, 'F1': 0.2609, 'gscore': 0.15}\n"
          ]
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import DAG, IIDSimulation\n",
        "from castle.algorithms import GraNDAG\n",
        "\n",
        "# load data\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=10, n_edges=20,\n",
        "                                      weight_range=(0.5, 2.0), seed=1)\n",
        "# dataset = IIDSimulation(W=weighted_random_dag, n=2000, method='nonlinear',sem_type='mlp')\n",
        "# dag, x = dataset.B, dataset.X\n",
        "dag, x = true_dag,X\n",
        "\n",
        "# Instantiation algorithm\n",
        "d = {'model_name': 'NonLinGauss', 'nonlinear': 'leaky-relu', 'optimizer': 'sgd', 'norm_prod': 'paths', 'device_type': 'gpu'}\n",
        "gnd = GraNDAG(input_dim=x.shape[1], )\n",
        "gnd.learn(data=x)\n",
        "\n",
        "# plot predict_dag and true_dag\n",
        "GraphDAG(gnd.causal_matrix, dag, save_name='Result_GraNDAG')\n",
        "met = MetricsDAG(gnd.causal_matrix, dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_GraNDAG.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7170aaee",
      "metadata": {
        "id": "7170aaee"
      },
      "source": [
        "##__M 13: MCSL__\n",
        "* A gradient-based algorithm for non-linear additive noise data by learning the binary adjacency matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faacde44",
      "metadata": {
        "id": "faacde44",
        "outputId": "45b864ec-f1ca-4f7e-979e-b552c5553b8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 18:16:46,236 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\mcsl.py[line:142] - INFO: GPU is available.\n",
            "2023-04-05 18:16:46,248 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 1==================\n",
            "2023-04-05 18:16:46,310 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 172.21960756363447\n",
            "2023-04-05 18:16:56,996 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 2.0705390402262447\n",
            "2023-04-05 18:17:07,154 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.7566800846478867\n",
            "2023-04-05 18:17:17,265 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.6667374272769206\n",
            "2023-04-05 18:17:27,381 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.6184198320065792\n",
            "2023-04-05 18:17:37,495 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 14.612440766258683\n",
            "2023-04-05 18:17:37,496 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 30.275212386045027\n",
            "2023-04-05 18:17:37,497 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 2==================\n",
            "2023-04-05 18:17:37,558 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.5807159895817755\n",
            "2023-04-05 18:17:47,735 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.5458887798272134\n",
            "2023-04-05 18:17:57,872 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.5130923219609467\n",
            "2023-04-05 18:18:07,976 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.506705689656188\n",
            "2023-04-05 18:18:18,113 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.5604003720813322\n",
            "2023-04-05 18:18:28,335 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.518087448933749\n",
            "2023-04-05 18:18:38,832 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.5298201861180967\n",
            "2023-04-05 18:18:49,210 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.4466128896598012\n",
            "2023-04-05 18:18:59,656 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.4058026102518244\n",
            "2023-04-05 18:19:09,946 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.3749506512236302\n",
            "2023-04-05 18:19:20,123 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.3830310670574082\n",
            "2023-04-05 18:19:30,485 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.3495845294340878\n",
            "2023-04-05 18:19:40,885 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.3526826555131335\n",
            "2023-04-05 18:19:51,125 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.3425991582521977\n",
            "2023-04-05 18:20:01,297 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.2989619956835354\n",
            "2023-04-05 18:20:11,421 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 2.0148059691714444\n",
            "2023-04-05 18:20:11,422 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 24.293849003898544\n",
            "2023-04-05 18:20:11,423 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 3==================\n",
            "2023-04-05 18:20:11,480 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.311475332408541\n",
            "2023-04-05 18:20:21,656 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.2812017029344036\n",
            "2023-04-05 18:20:31,887 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.2864250398156116\n",
            "2023-04-05 18:20:42,141 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.3126732860785195\n",
            "2023-04-05 18:20:52,362 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.2545727867953274\n",
            "2023-04-05 18:21:02,599 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.583558840465297\n",
            "2023-04-05 18:21:12,950 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.354065147955072\n",
            "2023-04-05 18:21:23,205 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.2695162830768278\n",
            "2023-04-05 18:21:33,934 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.2444688381895108\n",
            "2023-04-05 18:21:44,501 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.2215719978375603\n",
            "2023-04-05 18:21:55,244 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.2376815151882092\n",
            "2023-04-05 18:22:05,525 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1965036521139742\n",
            "2023-04-05 18:22:15,738 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1841456198084033\n",
            "2023-04-05 18:22:26,029 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.198043652340504\n",
            "2023-04-05 18:22:36,204 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.184331290779044\n",
            "2023-04-05 18:22:46,340 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.21507671584605603\n",
            "2023-04-05 18:22:46,341 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 16.468165330531697\n",
            "2023-04-05 18:22:46,341 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 4==================\n",
            "2023-04-05 18:22:46,400 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1712305303169612\n",
            "2023-04-05 18:22:56,663 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1999678156655438\n",
            "2023-04-05 18:23:06,925 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1663252619151006\n",
            "2023-04-05 18:23:17,343 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.103643648027766\n",
            "2023-04-05 18:23:27,991 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.1020331791224833\n",
            "2023-04-05 18:23:38,191 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.106581806969012\n",
            "2023-04-05 18:23:48,369 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.2166124893347647\n",
            "2023-04-05 18:23:58,709 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1474754732174741\n",
            "2023-04-05 18:24:09,164 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.1139217470987808\n",
            "2023-04-05 18:24:19,522 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.5487465165769358\n",
            "2023-04-05 18:24:29,609 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.05322761133114895\n",
            "2023-04-05 18:24:29,610 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 13.292174026609366\n",
            "2023-04-05 18:24:29,611 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 5==================\n",
            "2023-04-05 18:24:29,666 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1603818612509365\n",
            "2023-04-05 18:24:39,853 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1085466343120003\n",
            "2023-04-05 18:24:50,036 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.1839831068981015\n",
            "2023-04-05 18:25:00,197 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.16005640463496\n",
            "2023-04-05 18:25:10,281 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.1216654203214254\n",
            "2023-04-05 18:25:20,531 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1313175972842378\n",
            "2023-04-05 18:25:31,387 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1047317515950885\n",
            "2023-04-05 18:25:41,610 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0923847515252607\n",
            "2023-04-05 18:25:51,771 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0921034125722644\n",
            "2023-04-05 18:26:01,950 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0549921703917318\n",
            "2023-04-05 18:26:12,192 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0800556490017355\n",
            "2023-04-05 18:26:22,355 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0995437517748872\n",
            "2023-04-05 18:26:32,618 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.164045143218508\n",
            "2023-04-05 18:26:43,072 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0347331131287119\n",
            "2023-04-05 18:26:53,645 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0576820582064685\n",
            "2023-04-05 18:27:04,169 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.008013265075899056\n",
            "2023-04-05 18:27:04,170 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 9.256776188028088\n",
            "2023-04-05 18:27:04,171 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 6==================\n",
            "2023-04-05 18:27:04,228 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0953363498077666\n",
            "2023-04-05 18:27:14,796 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0224019446048604\n",
            "2023-04-05 18:27:25,289 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0564212342470167\n",
            "2023-04-05 18:27:36,231 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0321342293132036\n",
            "2023-04-05 18:27:46,651 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0966505527697845\n",
            "2023-04-05 18:27:56,838 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.1021688633269988\n",
            "2023-04-05 18:28:06,955 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0223571494850765\n",
            "2023-04-05 18:28:17,103 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.034189704916487\n",
            "2023-04-05 18:28:27,230 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.054969412745555\n",
            "2023-04-05 18:28:37,367 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0206343533360183\n",
            "2023-04-05 18:28:47,398 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0678304725657346\n",
            "2023-04-05 18:28:57,555 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.026220549152489\n",
            "2023-04-05 18:29:07,758 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.005390328819679\n",
            "2023-04-05 18:29:17,999 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0439136950373198\n",
            "2023-04-05 18:29:28,201 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0194924841112452\n",
            "2023-04-05 18:29:38,260 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.0009675705444340821\n",
            "2023-04-05 18:29:38,261 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 5.87755064117812\n",
            "2023-04-05 18:29:38,261 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 7==================\n",
            "2023-04-05 18:29:38,320 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0744826336075808\n",
            "2023-04-05 18:29:48,421 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.2074835985192922\n",
            "2023-04-05 18:29:58,475 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0177144207722275\n",
            "2023-04-05 18:30:08,631 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0677168198578022\n",
            "2023-04-05 18:30:18,763 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.051432291826676\n",
            "2023-04-05 18:30:28,868 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.073159703295811\n",
            "2023-04-05 18:30:38,989 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0556807035084292\n",
            "2023-04-05 18:30:49,082 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9880833031455373\n",
            "2023-04-05 18:30:59,303 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9567749514175276\n",
            "2023-04-05 18:31:09,493 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9814158806896922\n",
            "2023-04-05 18:31:19,574 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0833208209398144\n",
            "2023-04-05 18:31:29,762 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0928390129487386\n",
            "2023-04-05 18:31:39,946 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0475160478948\n",
            "2023-04-05 18:31:50,193 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9571189941786714\n",
            "2023-04-05 18:32:00,338 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.008603475041625\n",
            "2023-04-05 18:32:10,442 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 0.00011634858939402193\n",
            "2023-04-05 18:32:10,443 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 3.9612114019225295\n",
            "2023-04-05 18:32:10,444 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 8==================\n",
            "2023-04-05 18:32:10,504 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9900077899179242\n",
            "2023-04-05 18:32:20,684 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0327241313841031\n",
            "2023-04-05 18:32:30,818 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0131505276497659\n",
            "2023-04-05 18:32:41,007 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9467272197611095\n",
            "2023-04-05 18:32:51,178 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9801182936714373\n",
            "2023-04-05 18:33:01,409 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9854831449417669\n",
            "2023-04-05 18:33:11,524 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 0.9583304605525461\n",
            "2023-04-05 18:33:21,610 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0298974441032995\n",
            "2023-04-05 18:33:31,801 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0025865888193268\n",
            "2023-04-05 18:33:41,933 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9586380888355656\n",
            "2023-04-05 18:33:52,120 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9946704095891908\n",
            "2023-04-05 18:34:02,306 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0652079011271303\n",
            "2023-04-05 18:34:12,466 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9858893190620042\n",
            "2023-04-05 18:34:22,823 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.965533049843913\n",
            "2023-04-05 18:34:32,962 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9852854681722283\n",
            "2023-04-05 18:34:43,247 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 1.3334510741813688e-05\n",
            "2023-04-05 18:34:43,248 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 3.0124698350162245\n",
            "2023-04-05 18:34:43,248 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 9==================\n",
            "2023-04-05 18:34:43,306 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9557159435579088\n",
            "2023-04-05 18:34:53,654 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 0.9756925177480793\n",
            "2023-04-05 18:35:03,952 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0096483009130905\n",
            "2023-04-05 18:35:14,220 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9725686287662292\n",
            "2023-04-05 18:35:24,363 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0629068501898349\n",
            "2023-04-05 18:35:34,507 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0116407547758226\n",
            "2023-04-05 18:35:44,723 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0265773342193836\n",
            "2023-04-05 18:35:54,882 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9627255424450575\n",
            "2023-04-05 18:36:05,047 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9713448704459346\n",
            "2023-04-05 18:36:15,261 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0317331759895616\n",
            "2023-04-05 18:36:25,432 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0494775952310096\n",
            "2023-04-05 18:36:35,667 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0338698443139336\n",
            "2023-04-05 18:36:45,791 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0285551774876098\n",
            "2023-04-05 18:36:56,089 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.08752061371429\n",
            "2023-04-05 18:37:06,352 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0406068022995172\n",
            "2023-04-05 18:37:16,502 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 1.4110290820212867e-06\n",
            "2023-04-05 18:37:16,503 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.5543306990433194\n",
            "2023-04-05 18:37:16,504 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 10==================\n",
            "2023-04-05 18:37:16,564 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9889562482170019\n",
            "2023-04-05 18:37:26,803 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 0.9856685568569447\n",
            "2023-04-05 18:37:37,016 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9880953718284811\n",
            "2023-04-05 18:37:47,135 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9598232278117923\n",
            "2023-04-05 18:37:57,294 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 1.0251240682361626\n",
            "2023-04-05 18:38:07,406 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9823966239635461\n",
            "2023-04-05 18:38:17,553 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0763049176267936\n",
            "2023-04-05 18:38:27,714 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 0.9726160998501228\n",
            "2023-04-05 18:38:37,940 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9522393969836469\n",
            "2023-04-05 18:38:48,105 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.943765750030381\n",
            "2023-04-05 18:38:58,293 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.0303267520101134\n",
            "2023-04-05 18:39:08,485 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1041110867454753\n",
            "2023-04-05 18:39:18,632 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0207753778453108\n",
            "2023-04-05 18:39:28,770 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9931443029407504\n",
            "2023-04-05 18:39:38,938 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9843512492962126\n",
            "2023-04-05 18:39:49,011 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 1.590258023043134e-07\n",
            "2023-04-05 18:39:49,012 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.248520555411682\n",
            "2023-04-05 18:39:49,013 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 11==================\n",
            "2023-04-05 18:39:49,072 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 1.021248724643395\n",
            "2023-04-05 18:39:59,318 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.0383377082389171\n",
            "2023-04-05 18:40:09,522 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0310495332179908\n",
            "2023-04-05 18:40:19,696 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 0.9951553244940885\n",
            "2023-04-05 18:40:29,839 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9846928575141407\n",
            "2023-04-05 18:40:39,987 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 0: 0.9838522442661524\n",
            "2023-04-05 18:40:50,256 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 200: 1.1350669164979896\n",
            "2023-04-05 18:41:00,434 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 400: 1.0463378639250958\n",
            "2023-04-05 18:41:10,568 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 600: 1.0259996479865996\n",
            "2023-04-05 18:41:20,662 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:97] - INFO: Current loss in step 800: 0.9483615503404881\n",
            "2023-04-05 18:41:30,760 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,761 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,762 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 12==================\n",
            "2023-04-05 18:41:30,763 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,763 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,764 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 13==================\n",
            "2023-04-05 18:41:30,765 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,765 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,766 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 14==================\n",
            "2023-04-05 18:41:30,767 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,767 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,767 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 15==================\n",
            "2023-04-05 18:41:30,768 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,769 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,769 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 16==================\n",
            "2023-04-05 18:41:30,770 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,771 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,771 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 17==================\n",
            "2023-04-05 18:41:30,772 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,774 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,774 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 18==================\n",
            "2023-04-05 18:41:30,775 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,776 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,776 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 19==================\n",
            "2023-04-05 18:41:30,777 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,777 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,778 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 20==================\n",
            "2023-04-05 18:41:30,779 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,780 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,780 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 21==================\n",
            "2023-04-05 18:41:30,781 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,782 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,782 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 22==================\n",
            "2023-04-05 18:41:30,784 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,784 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,785 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 23==================\n",
            "2023-04-05 18:41:30,787 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,787 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,788 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 24==================\n",
            "2023-04-05 18:41:30,789 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,789 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n",
            "2023-04-05 18:41:30,790 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 25==================\n",
            "2023-04-05 18:41:30,791 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:72] - INFO: Current        h: 4.7059129926196874e-08\n",
            "2023-04-05 18:41:30,792 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\mcsl\\torch\\trainers\\al_trainer.py[line:73] - INFO: Current h_logits: 2.1040485275928233\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.8571, 'tpr': 0.15, 'fpr': 0.72, 'shd': 32, 'nnz': 21, 'precision': 0.1304, 'recall': 0.15, 'F1': 0.1395, 'gscore': 0.0}\n"
          ]
        }
      ],
      "source": [
        "from castle.algorithms import MCSL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "\n",
        "# X, true_dag, _ = load_dataset(name='IID_Test')\n",
        "\n",
        "n = MCSL(iter_step=1000, init_rho=1e-5,rho_multiply=10, l1_graph_penalty=2e-3)\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_mcsl')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_MCSL.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1160d1a6",
      "metadata": {
        "id": "1160d1a6"
      },
      "source": [
        "##__M 14: GAE__\n",
        "*\tA gradient-based algorithm using graph autoencoder to model non-linear causal relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f80f92",
      "metadata": {
        "id": "50f80f92",
        "outputId": "2d1f4dae-1d28-45af-d320-0da9c54d875e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-04 16:18:38,921 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-04 16:18:38,923 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\gae.py[line:144] - INFO: GPU is available.\n",
            "2023-04-04 16:18:38,934 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 1==================\n",
            "2023-04-04 16:18:38,954 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 173.0023464933411\n",
            "2023-04-04 16:18:40,553 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 89.04092049739704\n",
            "2023-04-04 16:18:41,982 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 22.93267801980251\n",
            "2023-04-04 16:18:43,348 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 6.796850138547454\n",
            "2023-04-04 16:18:44,752 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 4.775933437043005\n",
            "2023-04-04 16:18:46,179 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 3.70934874612499\n",
            "2023-04-04 16:18:47,683 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 3.0870756868242215\n",
            "2023-04-04 16:18:49,047 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 2.662250093881209\n",
            "2023-04-04 16:18:50,559 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 2.3593934112824515\n",
            "2023-04-04 16:18:51,988 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 2.144494072073573\n",
            "2023-04-04 16:18:53,347 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.990906461476466\n",
            "2023-04-04 16:18:54,808 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.8807857128140937\n",
            "2023-04-04 16:18:56,155 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.7994749356751485\n",
            "2023-04-04 16:18:57,976 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.7368965445297335\n",
            "2023-04-04 16:18:59,655 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.6862878713131124\n",
            "2023-04-04 16:19:01,152 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 0.04574173294915873\n",
            "2023-04-04 16:19:01,153 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 2==================\n",
            "2023-04-04 16:19:01,163 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.6470036330542739\n",
            "2023-04-04 16:19:02,766 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.6142053534466723\n",
            "2023-04-04 16:19:04,151 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.588785711404043\n",
            "2023-04-04 16:19:05,630 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.569774378302179\n",
            "2023-04-04 16:19:07,468 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5563095846559418\n",
            "2023-04-04 16:19:09,603 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5472716712431394\n",
            "2023-04-04 16:19:11,252 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.541459428610131\n",
            "2023-04-04 16:19:12,710 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5380788811657513\n",
            "2023-04-04 16:19:14,190 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.536196336863467\n",
            "2023-04-04 16:19:15,551 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5352253710812915\n",
            "2023-04-04 16:19:16,986 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5346195294714828\n",
            "2023-04-04 16:19:18,419 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5343680808283973\n",
            "2023-04-04 16:19:19,953 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5341911516213076\n",
            "2023-04-04 16:19:21,384 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5340915277368055\n",
            "2023-04-04 16:19:22,805 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5340332866121764\n",
            "2023-04-04 16:19:24,159 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.534575806730267\n",
            "2023-04-04 16:19:25,544 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5345126617793168\n",
            "2023-04-04 16:19:26,974 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5343937920745496\n",
            "2023-04-04 16:19:28,363 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5342958407404503\n",
            "2023-04-04 16:19:29,719 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5342406229760783\n",
            "2023-04-04 16:19:31,110 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5341565163449047\n",
            "2023-04-04 16:19:32,599 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5340709406321709\n",
            "2023-04-04 16:19:34,242 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5339717091909697\n",
            "2023-04-04 16:19:35,697 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.533929377472418\n",
            "2023-04-04 16:19:37,162 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5338142210363237\n",
            "2023-04-04 16:19:38,653 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5337039662426566\n",
            "2023-04-04 16:19:40,063 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.533619909254581\n",
            "2023-04-04 16:19:41,445 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5335243420175173\n",
            "2023-04-04 16:19:42,885 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5335808707795044\n",
            "2023-04-04 16:19:44,261 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5333294764956023\n",
            "2023-04-04 16:19:45,665 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5338256148385856\n",
            "2023-04-04 16:19:47,207 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5335745453077712\n",
            "2023-04-04 16:19:48,555 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5333913173525477\n",
            "2023-04-04 16:19:49,961 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5331727000810524\n",
            "2023-04-04 16:19:51,320 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5329335787099623\n",
            "2023-04-04 16:19:52,776 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5327216738576857\n",
            "2023-04-04 16:19:54,184 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5325720574542334\n",
            "2023-04-04 16:19:55,516 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5324078924990328\n",
            "2023-04-04 16:19:57,189 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5322952730315489\n",
            "2023-04-04 16:19:58,941 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.532247363717031\n",
            "2023-04-04 16:20:00,674 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.532139788370083\n",
            "2023-04-04 16:20:02,223 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5320425116597443\n",
            "2023-04-04 16:20:03,741 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5320053948024057\n",
            "2023-04-04 16:20:05,219 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.531914428693173\n",
            "2023-04-04 16:20:06,581 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5318173892351632\n",
            "2023-04-04 16:20:07,949 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5320069412416528\n",
            "2023-04-04 16:20:09,352 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.531929658597094\n",
            "2023-04-04 16:20:10,759 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5320699425155597\n",
            "2023-04-04 16:20:12,210 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5318718888554697\n",
            "2023-04-04 16:20:13,619 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5318185849484078\n",
            "2023-04-04 16:20:15,010 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5317485762568341\n",
            "2023-04-04 16:20:16,400 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5316746206692562\n",
            "2023-04-04 16:20:17,828 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5317340415609841\n",
            "2023-04-04 16:20:19,189 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5315911947584266\n",
            "2023-04-04 16:20:20,572 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5315615139789502\n",
            "2023-04-04 16:20:21,955 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5316221605237028\n",
            "2023-04-04 16:20:23,373 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5315243948812822\n",
            "2023-04-04 16:20:24,789 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5315231782871568\n",
            "2023-04-04 16:20:26,167 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5316392544002702\n",
            "2023-04-04 16:20:27,679 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5314635228686793\n",
            "2023-04-04 16:20:29,040 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 0.0068851214821119555\n",
            "2023-04-04 16:20:29,042 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 3==================\n",
            "2023-04-04 16:20:29,050 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5318126674442807\n",
            "2023-04-04 16:20:30,465 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5317838036950213\n",
            "2023-04-04 16:20:31,861 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5317349613022084\n",
            "2023-04-04 16:20:33,404 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5317947782804002\n",
            "2023-04-04 16:20:34,847 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5316835640375173\n",
            "2023-04-04 16:20:36,307 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.531731172758511\n",
            "2023-04-04 16:20:37,735 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5317490555796212\n",
            "2023-04-04 16:20:39,074 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5317761322225878\n",
            "2023-04-04 16:20:40,452 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.531594700286962\n",
            "2023-04-04 16:20:41,835 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5315569533625586\n",
            "2023-04-04 16:20:43,214 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5315580572102456\n",
            "2023-04-04 16:20:44,559 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5315279844800667\n",
            "2023-04-04 16:20:45,901 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5316254985328728\n",
            "2023-04-04 16:20:47,409 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5315041610050386\n",
            "2023-04-04 16:20:48,855 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5316304278751847\n",
            "2023-04-04 16:20:50,242 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5316670966760155\n",
            "2023-04-04 16:20:51,650 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5322268539646724\n",
            "2023-04-04 16:20:53,013 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5315303485112453\n",
            "2023-04-04 16:20:54,407 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5315623754000847\n",
            "2023-04-04 16:20:55,770 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5315453835167905\n",
            "2023-04-04 16:20:57,205 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5315096099196566\n",
            "2023-04-04 16:20:58,620 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5314675455771234\n",
            "2023-04-04 16:21:00,014 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5315129029742978\n",
            "2023-04-04 16:21:01,448 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5314095984742389\n",
            "2023-04-04 16:21:02,842 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5314579836803082\n",
            "2023-04-04 16:21:04,252 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5320041098273525\n",
            "2023-04-04 16:21:05,666 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5315668970610092\n",
            "2023-04-04 16:21:07,140 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.531392806693874\n",
            "2023-04-04 16:21:08,517 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5313801947925496\n",
            "2023-04-04 16:21:09,971 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5314283531410466\n",
            "2023-04-04 16:21:11,343 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.531524869943435\n",
            "2023-04-04 16:21:12,818 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5314228905932425\n",
            "2023-04-04 16:21:14,191 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5314751533433262\n",
            "2023-04-04 16:21:15,571 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5314754394724315\n",
            "2023-04-04 16:21:17,039 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5314142745827528\n",
            "2023-04-04 16:21:18,615 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5313930996363434\n",
            "2023-04-04 16:21:20,031 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5313780196160467\n",
            "2023-04-04 16:21:21,395 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5313653675775551\n",
            "2023-04-04 16:21:22,849 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313587686734331\n",
            "2023-04-04 16:21:24,206 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5313968313605206\n",
            "2023-04-04 16:21:25,560 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.531322659903837\n",
            "2023-04-04 16:21:26,926 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5313960062462988\n",
            "2023-04-04 16:21:28,289 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5313075045921525\n",
            "2023-04-04 16:21:29,683 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.531308271026475\n",
            "2023-04-04 16:21:31,079 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5313397326288227\n",
            "2023-04-04 16:21:32,472 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5313985394245633\n",
            "2023-04-04 16:21:33,844 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5316857173597427\n",
            "2023-04-04 16:21:35,265 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5313567726281534\n",
            "2023-04-04 16:21:36,687 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5313476724637873\n",
            "2023-04-04 16:21:38,162 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5314335035147042\n",
            "2023-04-04 16:21:39,545 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5313192928511647\n",
            "2023-04-04 16:21:40,908 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.531325014268483\n",
            "2023-04-04 16:21:42,289 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5315029532117828\n",
            "2023-04-04 16:21:43,875 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313109315568236\n",
            "2023-04-04 16:21:45,418 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5312805989783036\n",
            "2023-04-04 16:21:46,861 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5312850446289756\n",
            "2023-04-04 16:21:48,337 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5312581797908478\n",
            "2023-04-04 16:21:49,737 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5312750907315857\n",
            "2023-04-04 16:21:51,077 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5314366750380315\n",
            "2023-04-04 16:21:52,450 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5316318787582681\n",
            "2023-04-04 16:21:53,810 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5313624132536632\n",
            "2023-04-04 16:21:55,176 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5314030486591284\n",
            "2023-04-04 16:21:56,606 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5313753657855664\n",
            "2023-04-04 16:21:58,044 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5313902707263898\n",
            "2023-04-04 16:21:59,438 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5317814494535003\n",
            "2023-04-04 16:22:00,915 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5313848734351392\n",
            "2023-04-04 16:22:02,480 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5312985981347014\n",
            "2023-04-04 16:22:03,901 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5317314614447985\n",
            "2023-04-04 16:22:05,298 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313185811231063\n",
            "2023-04-04 16:22:06,727 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5315112054731284\n",
            "2023-04-04 16:22:08,119 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.531390664132341\n",
            "2023-04-04 16:22:09,453 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.531281653023004\n",
            "2023-04-04 16:22:10,785 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5312892321421523\n",
            "2023-04-04 16:22:12,176 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5312557361795238\n",
            "2023-04-04 16:22:13,562 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5312884293091797\n",
            "2023-04-04 16:22:14,954 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 0.001524373362874698\n",
            "2023-04-04 16:22:14,956 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 4==================\n",
            "2023-04-04 16:22:14,971 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5316749366652402\n",
            "2023-04-04 16:22:16,433 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5324446080977003\n",
            "2023-04-04 16:22:17,881 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.531573102520751\n",
            "2023-04-04 16:22:19,295 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5316429843484813\n",
            "2023-04-04 16:22:20,666 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5317398022033266\n",
            "2023-04-04 16:22:22,024 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5314995445248534\n",
            "2023-04-04 16:22:23,410 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5314732797917068\n",
            "2023-04-04 16:22:24,746 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5314218625467821\n",
            "2023-04-04 16:22:26,095 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313925650923552\n",
            "2023-04-04 16:22:27,526 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5313956647998623\n",
            "2023-04-04 16:22:28,902 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5315195135403583\n",
            "2023-04-04 16:22:30,515 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.531449185995395\n",
            "2023-04-04 16:22:32,149 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5315726625132857\n",
            "2023-04-04 16:22:33,873 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5314029539491658\n",
            "2023-04-04 16:22:35,256 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.531418312150252\n",
            "2023-04-04 16:22:36,626 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5314851159096\n",
            "2023-04-04 16:22:38,067 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.531399576077827\n",
            "2023-04-04 16:22:39,419 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.531510933090625\n",
            "2023-04-04 16:22:40,750 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5313963899757903\n",
            "2023-04-04 16:22:42,134 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5319710519207226\n",
            "2023-04-04 16:22:43,608 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.531580200587063\n",
            "2023-04-04 16:22:44,990 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5313858709170352\n",
            "2023-04-04 16:22:46,327 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5314016228067586\n",
            "2023-04-04 16:22:47,743 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5314408102968997\n",
            "2023-04-04 16:22:49,156 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5316119186981152\n",
            "2023-04-04 16:22:50,541 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.531353761515321\n",
            "2023-04-04 16:22:51,926 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5313436502101891\n",
            "2023-04-04 16:22:53,371 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5313104014798902\n",
            "2023-04-04 16:22:54,749 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5313174361043993\n",
            "2023-04-04 16:22:56,199 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5313360724734022\n",
            "2023-04-04 16:22:57,656 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5313746964258943\n",
            "2023-04-04 16:22:59,191 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5314439331010508\n",
            "2023-04-04 16:23:00,579 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5316085402070567\n",
            "2023-04-04 16:23:02,154 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5314873323040354\n",
            "2023-04-04 16:23:03,542 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.531366873240491\n",
            "2023-04-04 16:23:04,981 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5313650416519842\n",
            "2023-04-04 16:23:06,383 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.531348764473297\n",
            "2023-04-04 16:23:07,814 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.531329598228569\n",
            "2023-04-04 16:23:09,208 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313347157026405\n",
            "2023-04-04 16:23:10,601 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5312984092255872\n",
            "2023-04-04 16:23:11,935 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5314125278623805\n",
            "2023-04-04 16:23:13,405 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.531333325973898\n",
            "2023-04-04 16:23:14,816 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.531829698624985\n",
            "2023-04-04 16:23:16,142 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5316451210223059\n",
            "2023-04-04 16:23:17,590 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.531260745121553\n",
            "2023-04-04 16:23:18,996 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5316645284970771\n",
            "2023-04-04 16:23:20,507 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.531483911078473\n",
            "2023-04-04 16:23:21,900 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.53141312051836\n",
            "2023-04-04 16:23:23,281 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5314723322000736\n",
            "2023-04-04 16:23:24,639 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5313981425263712\n",
            "2023-04-04 16:23:26,064 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5316897600067312\n",
            "2023-04-04 16:23:27,427 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.531351905913169\n",
            "2023-04-04 16:23:28,758 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.531333175603016\n",
            "2023-04-04 16:23:30,164 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313456151319005\n",
            "2023-04-04 16:23:31,509 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.531312406479219\n",
            "2023-04-04 16:23:32,885 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5315102331752948\n",
            "2023-04-04 16:23:34,258 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5315088909964307\n",
            "2023-04-04 16:23:35,594 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5312532476405265\n",
            "2023-04-04 16:23:36,931 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5313474776081781\n",
            "2023-04-04 16:23:38,403 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5312811544149607\n",
            "2023-04-04 16:23:39,796 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5315167082821652\n",
            "2023-04-04 16:23:41,163 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5315097542025125\n",
            "2023-04-04 16:23:42,565 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.53140866715651\n",
            "2023-04-04 16:23:43,913 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5314710872832062\n",
            "2023-04-04 16:23:45,268 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.53143211066428\n",
            "2023-04-04 16:23:46,683 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5314242951274386\n",
            "2023-04-04 16:23:48,085 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5316917733530435\n",
            "2023-04-04 16:23:49,440 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5314816285947421\n",
            "2023-04-04 16:23:50,773 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313342148376607\n",
            "2023-04-04 16:23:52,296 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5314885877682722\n",
            "2023-04-04 16:23:53,845 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.531320212111178\n",
            "2023-04-04 16:23:55,254 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5314155249732837\n",
            "2023-04-04 16:23:57,027 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5313777934115225\n",
            "2023-04-04 16:23:58,537 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5313202975548976\n",
            "2023-04-04 16:23:59,862 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5313869538059892\n",
            "2023-04-04 16:24:01,235 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5314376746609175\n",
            "2023-04-04 16:24:02,620 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5316299256245325\n",
            "2023-04-04 16:24:03,996 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.531692886483697\n",
            "2023-04-04 16:24:05,378 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5318318715120751\n",
            "2023-04-04 16:24:06,734 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.531450434698084\n",
            "2023-04-04 16:24:08,112 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5313971432147537\n",
            "2023-04-04 16:24:09,449 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5313632735914675\n",
            "2023-04-04 16:24:10,784 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5315715462777006\n",
            "2023-04-04 16:24:12,221 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5313621864184046\n",
            "2023-04-04 16:24:13,725 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5313136098093199\n",
            "2023-04-04 16:24:15,125 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5313377011249847\n",
            "2023-04-04 16:24:16,460 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5313285483050756\n",
            "2023-04-04 16:24:17,914 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5312566717994585\n",
            "2023-04-04 16:24:19,245 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.531555818633244\n",
            "2023-04-04 16:24:20,600 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5319467277128451\n",
            "2023-04-04 16:24:21,955 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 0.00033333956995740266\n",
            "2023-04-04 16:24:21,957 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 5==================\n",
            "2023-04-04 16:24:21,969 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5322516919330484\n",
            "2023-04-04 16:24:23,389 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.532075843837978\n",
            "2023-04-04 16:24:24,735 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5317543695859355\n",
            "2023-04-04 16:24:26,175 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5321014989643882\n",
            "2023-04-04 16:24:27,601 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.532089858624745\n",
            "2023-04-04 16:24:29,234 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5317442443450588\n",
            "2023-04-04 16:24:30,823 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5316015614952956\n",
            "2023-04-04 16:24:32,478 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.531804818421444\n",
            "2023-04-04 16:24:33,942 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5320951597004293\n",
            "2023-04-04 16:24:35,368 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.531704458492748\n",
            "2023-04-04 16:24:36,789 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5316113977051589\n",
            "2023-04-04 16:24:38,228 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5322889129566668\n",
            "2023-04-04 16:24:39,607 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5337276150193437\n",
            "2023-04-04 16:24:41,039 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5321737313178376\n",
            "2023-04-04 16:24:42,445 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5318184059084956\n",
            "2023-04-04 16:24:43,796 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5316073905925451\n",
            "2023-04-04 16:24:45,218 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5317030722397393\n",
            "2023-04-04 16:24:46,670 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5318563360377557\n",
            "2023-04-04 16:24:48,154 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.533092809356464\n",
            "2023-04-04 16:24:49,496 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5314994630705998\n",
            "2023-04-04 16:24:50,864 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5319091898685853\n",
            "2023-04-04 16:24:52,182 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5316564348226671\n",
            "2023-04-04 16:24:53,581 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5315093580724874\n",
            "2023-04-04 16:24:54,923 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5327422505135642\n",
            "2023-04-04 16:24:56,311 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5314272671953941\n",
            "2023-04-04 16:24:57,704 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5315346133943204\n",
            "2023-04-04 16:24:59,076 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5314383081023657\n",
            "2023-04-04 16:25:00,434 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.531605578845449\n",
            "2023-04-04 16:25:01,782 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.532436303969734\n",
            "2023-04-04 16:25:03,279 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5320100302684774\n",
            "2023-04-04 16:25:04,894 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5319085501157663\n",
            "2023-04-04 16:25:06,308 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5323451563959238\n",
            "2023-04-04 16:25:07,827 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5317621842256712\n",
            "2023-04-04 16:25:09,161 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5316159355235972\n",
            "2023-04-04 16:25:10,758 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5317644494723082\n",
            "2023-04-04 16:25:12,306 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5316143079093831\n",
            "2023-04-04 16:25:14,173 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5316350692989742\n",
            "2023-04-04 16:25:16,013 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5314322522564427\n",
            "2023-04-04 16:25:17,657 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.531833065375074\n",
            "2023-04-04 16:25:19,244 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5317729250262768\n",
            "2023-04-04 16:25:20,711 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.532200677477599\n",
            "2023-04-04 16:25:22,267 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5313806380514083\n",
            "2023-04-04 16:25:23,881 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5333064544808466\n",
            "2023-04-04 16:25:25,475 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5314824461561716\n",
            "2023-04-04 16:25:27,069 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5317270487703523\n",
            "2023-04-04 16:25:28,653 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5320032555223846\n",
            "2023-04-04 16:25:30,113 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5328486101796146\n",
            "2023-04-04 16:25:31,563 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5316211165702633\n",
            "2023-04-04 16:25:33,026 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.531534678831522\n",
            "2023-04-04 16:25:34,537 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.532074916192492\n",
            "2023-04-04 16:25:36,168 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5315331092248576\n",
            "2023-04-04 16:25:37,938 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5318328739363347\n",
            "2023-04-04 16:25:39,475 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5316269744278943\n",
            "2023-04-04 16:25:40,979 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5316925703463224\n",
            "2023-04-04 16:25:42,821 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5314246401560505\n",
            "2023-04-04 16:25:44,361 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5314648053112514\n",
            "2023-04-04 16:25:45,823 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.532643620637573\n",
            "2023-04-04 16:25:47,334 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.531402526319905\n",
            "2023-04-04 16:25:48,897 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5321668829970996\n",
            "2023-04-04 16:25:50,278 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5313649384452612\n",
            "2023-04-04 16:25:51,626 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5318018757872265\n",
            "2023-04-04 16:25:52,997 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5317575291216308\n",
            "2023-04-04 16:25:54,356 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5315277052511465\n",
            "2023-04-04 16:25:55,794 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5316335418559879\n",
            "2023-04-04 16:25:57,230 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5318051374560175\n",
            "2023-04-04 16:25:58,635 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.531739071731904\n",
            "2023-04-04 16:25:59,954 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5363849953771092\n",
            "2023-04-04 16:26:01,687 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5316592330344476\n",
            "2023-04-04 16:26:03,117 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.532237661893058\n",
            "2023-04-04 16:26:04,469 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.531426379435717\n",
            "2023-04-04 16:26:05,846 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.531596424649343\n",
            "2023-04-04 16:26:07,192 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.531585995581914\n",
            "2023-04-04 16:26:08,584 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5315079545113857\n",
            "2023-04-04 16:26:09,940 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5314319631131785\n",
            "2023-04-04 16:26:11,291 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5315561823689816\n",
            "2023-04-04 16:26:12,653 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5334994932042736\n",
            "2023-04-04 16:26:14,092 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5340435492677635\n",
            "2023-04-04 16:26:15,414 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5327782345209693\n",
            "2023-04-04 16:26:16,748 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.532251346850486\n",
            "2023-04-04 16:26:18,257 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5337885606730226\n",
            "2023-04-04 16:26:19,611 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5321013699362624\n",
            "2023-04-04 16:26:20,968 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5316860490604671\n",
            "2023-04-04 16:26:22,349 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.531523443899534\n",
            "2023-04-04 16:26:23,771 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5342941728106723\n",
            "2023-04-04 16:26:25,187 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5327788734652963\n",
            "2023-04-04 16:26:26,548 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5322231832652005\n",
            "2023-04-04 16:26:27,911 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5342020239967404\n",
            "2023-04-04 16:26:29,351 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5314392158427397\n",
            "2023-04-04 16:26:30,750 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5314286054080364\n",
            "2023-04-04 16:26:32,172 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5317165882227777\n",
            "2023-04-04 16:26:33,587 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 7.028913140416648e-05\n",
            "2023-04-04 16:26:33,588 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 6==================\n",
            "2023-04-04 16:26:33,597 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5320742546031574\n",
            "2023-04-04 16:26:34,963 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5331650132268075\n",
            "2023-04-04 16:26:36,332 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5323337527138683\n",
            "2023-04-04 16:26:37,664 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5319786403753768\n",
            "2023-04-04 16:26:39,145 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.533166853663446\n",
            "2023-04-04 16:26:40,493 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5322045520686298\n",
            "2023-04-04 16:26:41,841 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5329646838759492\n",
            "2023-04-04 16:26:43,586 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.533925505238826\n",
            "2023-04-04 16:26:45,090 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5354664195339685\n",
            "2023-04-04 16:26:46,646 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5321230605598033\n",
            "2023-04-04 16:26:48,336 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5319167747793065\n",
            "2023-04-04 16:26:49,934 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5321430879954336\n",
            "2023-04-04 16:26:51,548 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5320654854468043\n",
            "2023-04-04 16:26:53,084 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5317712674940351\n",
            "2023-04-04 16:26:54,891 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5318581030345118\n",
            "2023-04-04 16:26:56,571 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.533191839066485\n",
            "2023-04-04 16:26:58,241 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.532959530417465\n",
            "2023-04-04 16:27:00,059 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5320448306374999\n",
            "2023-04-04 16:27:01,982 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5366277403235664\n",
            "2023-04-04 16:27:03,681 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5319651235212965\n",
            "2023-04-04 16:27:05,132 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5329605972071727\n",
            "2023-04-04 16:27:06,540 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.531852808763711\n",
            "2023-04-04 16:27:07,967 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5366840952080845\n",
            "2023-04-04 16:27:09,417 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5318048224562602\n",
            "2023-04-04 16:27:11,439 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5323325433598163\n",
            "2023-04-04 16:27:13,063 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5323210061551533\n",
            "2023-04-04 16:27:14,772 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5322468220955128\n",
            "2023-04-04 16:27:16,166 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5317719671605214\n",
            "2023-04-04 16:27:17,511 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5324149267797036\n",
            "2023-04-04 16:27:18,883 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5317215518000418\n",
            "2023-04-04 16:27:20,807 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5320829137375584\n",
            "2023-04-04 16:27:22,467 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5320146788707016\n",
            "2023-04-04 16:27:24,051 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5327655653717434\n",
            "2023-04-04 16:27:25,525 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5321793259448881\n",
            "2023-04-04 16:27:27,651 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.532128338504716\n",
            "2023-04-04 16:27:29,489 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.531961168946822\n",
            "2023-04-04 16:27:30,901 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5328883237912603\n",
            "2023-04-04 16:27:32,251 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5321210917302812\n",
            "2023-04-04 16:27:34,011 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5329629817709578\n",
            "2023-04-04 16:27:35,662 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5331017654193728\n",
            "2023-04-04 16:27:37,262 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5325053914362587\n",
            "2023-04-04 16:27:38,859 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5321159839459155\n",
            "2023-04-04 16:27:40,868 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5355265032847247\n",
            "2023-04-04 16:27:42,455 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5326749494545449\n",
            "2023-04-04 16:27:44,075 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5316856416973095\n",
            "2023-04-04 16:27:45,652 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5318974776794516\n",
            "2023-04-04 16:27:47,074 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5327654304605773\n",
            "2023-04-04 16:27:48,625 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5322922229643132\n",
            "2023-04-04 16:27:49,974 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5328787996527988\n",
            "2023-04-04 16:27:51,350 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5325640953873727\n",
            "2023-04-04 16:27:52,890 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5341825412785834\n",
            "2023-04-04 16:27:54,686 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5331061729351119\n",
            "2023-04-04 16:27:56,612 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5317954657372275\n",
            "2023-04-04 16:27:58,423 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5319488383430964\n",
            "2023-04-04 16:27:59,976 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.53197710153193\n",
            "2023-04-04 16:28:01,369 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5317249019401546\n",
            "2023-04-04 16:28:02,792 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.534979079938055\n",
            "2023-04-04 16:28:04,155 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5320095250013814\n",
            "2023-04-04 16:28:05,627 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.532070329779539\n",
            "2023-04-04 16:28:07,284 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5329160439244258\n",
            "2023-04-04 16:28:09,083 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5325452002100097\n",
            "2023-04-04 16:28:10,458 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5322924334747114\n",
            "2023-04-04 16:28:11,802 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5326105458905004\n",
            "2023-04-04 16:28:13,189 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.533517552050308\n",
            "2023-04-04 16:28:14,516 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5322360696424355\n",
            "2023-04-04 16:28:15,894 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5382821328710183\n",
            "2023-04-04 16:28:17,412 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5317559083491363\n",
            "2023-04-04 16:28:19,079 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5317758234407524\n",
            "2023-04-04 16:28:20,762 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5318139270701154\n",
            "2023-04-04 16:28:22,644 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5316991656939178\n",
            "2023-04-04 16:28:24,197 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5320095944111118\n",
            "2023-04-04 16:28:25,515 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5322434804099254\n",
            "2023-04-04 16:28:26,916 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5327763713404539\n",
            "2023-04-04 16:28:28,757 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5321877585937664\n",
            "2023-04-04 16:28:30,509 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.531838015818374\n",
            "2023-04-04 16:28:32,175 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5336123291715666\n",
            "2023-04-04 16:28:33,801 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5323858669603085\n",
            "2023-04-04 16:28:35,318 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5332445816034617\n",
            "2023-04-04 16:28:36,640 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5346587169748434\n",
            "2023-04-04 16:28:37,994 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.53243682223795\n",
            "2023-04-04 16:28:39,355 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5322155369964316\n",
            "2023-04-04 16:28:40,699 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.532201823902393\n",
            "2023-04-04 16:28:42,058 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5325489382551523\n",
            "2023-04-04 16:28:43,481 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.533618671903245\n",
            "2023-04-04 16:28:44,806 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5382374493829187\n",
            "2023-04-04 16:28:46,158 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5336240193163906\n",
            "2023-04-04 16:28:47,644 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5318584736392749\n",
            "2023-04-04 16:28:49,064 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5322878373487645\n",
            "2023-04-04 16:28:50,427 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.533238289508007\n",
            "2023-04-04 16:28:51,762 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.532811434925096\n",
            "2023-04-04 16:28:53,114 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 1.5520862755025178e-05\n",
            "2023-04-04 16:28:53,116 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 7==================\n",
            "2023-04-04 16:28:53,126 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.533297041807348\n",
            "2023-04-04 16:28:54,501 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5329394403479968\n",
            "2023-04-04 16:28:55,811 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.532645473871236\n",
            "2023-04-04 16:28:57,131 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5327744114019926\n",
            "2023-04-04 16:28:58,523 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.534433819797661\n",
            "2023-04-04 16:28:59,910 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5336527596832228\n",
            "2023-04-04 16:29:01,339 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.533122747954054\n",
            "2023-04-04 16:29:02,740 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5324547474049137\n",
            "2023-04-04 16:29:04,104 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5338306244649798\n",
            "2023-04-04 16:29:05,452 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5339813212286357\n",
            "2023-04-04 16:29:06,774 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5393644164054894\n",
            "2023-04-04 16:29:08,192 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5325115499856543\n",
            "2023-04-04 16:29:09,584 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5322888941327848\n",
            "2023-04-04 16:29:10,927 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5373734350277415\n",
            "2023-04-04 16:29:12,278 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.533108089818505\n",
            "2023-04-04 16:29:13,607 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.53263210400656\n",
            "2023-04-04 16:29:14,952 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5370779302445534\n",
            "2023-04-04 16:29:16,593 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5327337868533975\n",
            "2023-04-04 16:29:18,385 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5325399373851099\n",
            "2023-04-04 16:29:20,014 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5331639601093936\n",
            "2023-04-04 16:29:21,426 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.53449177928834\n",
            "2023-04-04 16:29:22,837 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5327578064916434\n",
            "2023-04-04 16:29:24,347 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5380231485536\n",
            "2023-04-04 16:29:25,702 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5333062025938866\n",
            "2023-04-04 16:29:27,466 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5352678699205908\n",
            "2023-04-04 16:29:29,175 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5329285581333014\n",
            "2023-04-04 16:29:30,589 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5361508323891542\n",
            "2023-04-04 16:29:32,235 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5335993578870846\n",
            "2023-04-04 16:29:33,823 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.532726297502897\n",
            "2023-04-04 16:29:35,322 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5360770251333098\n",
            "2023-04-04 16:29:36,935 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5408074755010723\n",
            "2023-04-04 16:29:38,494 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5379767953023318\n",
            "2023-04-04 16:29:40,260 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5358886649650538\n",
            "2023-04-04 16:29:42,081 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.533086587030641\n",
            "2023-04-04 16:29:43,512 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.534742445065524\n",
            "2023-04-04 16:29:45,248 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5391166883964116\n",
            "2023-04-04 16:29:47,243 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.532898661456307\n",
            "2023-04-04 16:29:49,183 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5365741647333728\n",
            "2023-04-04 16:29:51,109 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5324189811327147\n",
            "2023-04-04 16:29:52,831 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5331651706381542\n",
            "2023-04-04 16:29:54,340 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5325030564136495\n",
            "2023-04-04 16:29:55,777 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5322503622491865\n",
            "2023-04-04 16:29:57,236 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.532441317951989\n",
            "2023-04-04 16:29:58,826 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5334701393092594\n",
            "2023-04-04 16:30:00,293 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.532726884423974\n",
            "2023-04-04 16:30:01,978 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5340617810720292\n",
            "2023-04-04 16:30:03,320 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5357855630640267\n",
            "2023-04-04 16:30:04,737 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.533069580950614\n",
            "2023-04-04 16:30:06,096 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5358034645221315\n",
            "2023-04-04 16:30:07,634 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5327962340564447\n",
            "2023-04-04 16:30:09,442 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.532953569990738\n",
            "2023-04-04 16:30:11,288 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5324764553019639\n",
            "2023-04-04 16:30:13,346 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.533460377159372\n",
            "2023-04-04 16:30:15,290 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5330051991870621\n",
            "2023-04-04 16:30:17,077 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.53367225255917\n",
            "2023-04-04 16:30:18,556 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5323565713672347\n",
            "2023-04-04 16:30:19,969 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.532570446798135\n",
            "2023-04-04 16:30:21,533 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.533316573040283\n",
            "2023-04-04 16:30:23,142 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.53432501007661\n",
            "2023-04-04 16:30:24,582 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5403068532954187\n",
            "2023-04-04 16:30:26,251 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5381248223217594\n",
            "2023-04-04 16:30:27,851 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.534788660234695\n",
            "2023-04-04 16:30:29,287 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5333428263017612\n",
            "2023-04-04 16:30:30,631 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5325960385775383\n",
            "2023-04-04 16:30:32,030 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5350191490072898\n",
            "2023-04-04 16:30:33,426 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5336424672624234\n",
            "2023-04-04 16:30:34,769 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5326222701410401\n",
            "2023-04-04 16:30:36,133 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.532624572052386\n",
            "2023-04-04 16:30:37,509 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5330932486970947\n",
            "2023-04-04 16:30:38,901 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.534782704756194\n",
            "2023-04-04 16:30:40,263 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5335882200522075\n",
            "2023-04-04 16:30:41,605 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5341196960678785\n",
            "2023-04-04 16:30:42,991 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5324299031712207\n",
            "2023-04-04 16:30:44,362 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5352485116079533\n",
            "2023-04-04 16:30:45,760 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5333736784637786\n",
            "2023-04-04 16:30:47,281 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5338522831188544\n",
            "2023-04-04 16:30:48,755 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5361406932532655\n",
            "2023-04-04 16:30:50,558 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.533204460894136\n",
            "2023-04-04 16:30:52,716 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.532759627944969\n",
            "2023-04-04 16:30:54,642 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.53441454001918\n",
            "2023-04-04 16:30:56,725 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5383989057638854\n",
            "2023-04-04 16:30:58,289 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5357351488433504\n",
            "2023-04-04 16:30:59,877 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5328248985030166\n",
            "2023-04-04 16:31:01,669 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.535702096807164\n",
            "2023-04-04 16:31:03,339 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5332545964239739\n",
            "2023-04-04 16:31:05,321 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5416881745534532\n",
            "2023-04-04 16:31:07,024 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.532669115293529\n",
            "2023-04-04 16:31:08,779 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5334355390535006\n",
            "2023-04-04 16:31:10,507 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5331307411126245\n",
            "2023-04-04 16:31:12,150 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.53380935053741\n",
            "2023-04-04 16:31:13,620 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 3.565211883937991e-06\n",
            "2023-04-04 16:31:13,621 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 8==================\n",
            "2023-04-04 16:31:13,635 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.53854658067222\n",
            "2023-04-04 16:31:15,045 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.536375948575684\n",
            "2023-04-04 16:31:16,390 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5345687866372388\n",
            "2023-04-04 16:31:17,880 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5340604201431485\n",
            "2023-04-04 16:31:19,272 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5354150758380514\n",
            "2023-04-04 16:31:20,695 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5351608879784684\n",
            "2023-04-04 16:31:22,053 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5358823957367678\n",
            "2023-04-04 16:31:23,388 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5352998933620998\n",
            "2023-04-04 16:31:24,863 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5361725606130272\n",
            "2023-04-04 16:31:26,224 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5387736521197164\n",
            "2023-04-04 16:31:27,551 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.534819940859981\n",
            "2023-04-04 16:31:28,932 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.536127511994069\n",
            "2023-04-04 16:31:30,275 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5389633399301845\n",
            "2023-04-04 16:31:31,622 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5347176707975339\n",
            "2023-04-04 16:31:32,946 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5543142907675964\n",
            "2023-04-04 16:31:34,471 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5341961262776338\n",
            "2023-04-04 16:31:35,836 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5341562793379437\n",
            "2023-04-04 16:31:37,233 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5374861739946708\n",
            "2023-04-04 16:31:38,639 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5354779587478802\n",
            "2023-04-04 16:31:40,047 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5434479895055462\n",
            "2023-04-04 16:31:41,432 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5340239936017472\n",
            "2023-04-04 16:31:42,916 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5343329592220125\n",
            "2023-04-04 16:31:44,299 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.535221494391487\n",
            "2023-04-04 16:31:45,694 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5351556720158481\n",
            "2023-04-04 16:31:47,170 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5343888698616979\n",
            "2023-04-04 16:31:48,672 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5380253874968153\n",
            "2023-04-04 16:31:50,015 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5414033420854847\n",
            "2023-04-04 16:31:51,356 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5354386229635704\n",
            "2023-04-04 16:31:52,792 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.538194208713021\n",
            "2023-04-04 16:31:54,165 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.533106812052558\n",
            "2023-04-04 16:31:55,617 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5342141542384307\n",
            "2023-04-04 16:31:57,007 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5356503658895355\n",
            "2023-04-04 16:31:58,370 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5381198518891006\n",
            "2023-04-04 16:31:59,703 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5367323968758904\n",
            "2023-04-04 16:32:01,138 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.534794674227695\n",
            "2023-04-04 16:32:02,482 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.533878603876622\n",
            "2023-04-04 16:32:03,908 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5339435574805356\n",
            "2023-04-04 16:32:05,359 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.535389675893218\n",
            "2023-04-04 16:32:06,754 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.534357708591847\n",
            "2023-04-04 16:32:08,258 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.537398127447193\n",
            "2023-04-04 16:32:09,653 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5404500305924862\n",
            "2023-04-04 16:32:11,041 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.543495801445551\n",
            "2023-04-04 16:32:12,421 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5347701144446027\n",
            "2023-04-04 16:32:13,800 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5342271196517103\n",
            "2023-04-04 16:32:15,213 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5373707160719567\n",
            "2023-04-04 16:32:16,568 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5433715957652165\n",
            "2023-04-04 16:32:17,970 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5354961295418303\n",
            "2023-04-04 16:32:19,357 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.536297755015639\n",
            "2023-04-04 16:32:20,762 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5364053376056486\n",
            "2023-04-04 16:32:22,150 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5423329077078625\n",
            "2023-04-04 16:32:23,514 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.535063355289866\n",
            "2023-04-04 16:32:25,056 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5345269257659726\n",
            "2023-04-04 16:32:26,429 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.534062127787177\n",
            "2023-04-04 16:32:27,789 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5385806628174203\n",
            "2023-04-04 16:32:29,200 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.547485799647224\n",
            "2023-04-04 16:32:30,571 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5375105554473947\n",
            "2023-04-04 16:32:31,977 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5351867514836408\n",
            "2023-04-04 16:32:33,454 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5355567221708408\n",
            "2023-04-04 16:32:34,863 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5377638081711615\n",
            "2023-04-04 16:32:36,303 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5371088837180535\n",
            "2023-04-04 16:32:37,626 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5397128711265482\n",
            "2023-04-04 16:32:38,969 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5520852511395666\n",
            "2023-04-04 16:32:40,415 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5358371470018322\n",
            "2023-04-04 16:32:41,790 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.53750467123188\n",
            "2023-04-04 16:32:43,157 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.53633041040798\n",
            "2023-04-04 16:32:44,558 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5680278584327314\n",
            "2023-04-04 16:32:45,949 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5354988449658762\n",
            "2023-04-04 16:32:47,405 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5343074977247502\n",
            "2023-04-04 16:32:48,814 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5397304268147896\n",
            "2023-04-04 16:32:50,242 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5360681580772537\n",
            "2023-04-04 16:32:51,599 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5345638075327264\n",
            "2023-04-04 16:32:52,976 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5381687388463625\n",
            "2023-04-04 16:32:54,351 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.540793673353028\n",
            "2023-04-04 16:32:55,743 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5340451693046533\n",
            "2023-04-04 16:32:57,091 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5353340287233117\n",
            "2023-04-04 16:32:58,425 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5442660148109852\n",
            "2023-04-04 16:32:59,812 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.541191259327904\n",
            "2023-04-04 16:33:01,188 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5529238020309806\n",
            "2023-04-04 16:33:02,537 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5427406512713142\n",
            "2023-04-04 16:33:03,970 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.548635071260376\n",
            "2023-04-04 16:33:05,372 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5671623133655628\n",
            "2023-04-04 16:33:06,785 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5366655487398841\n",
            "2023-04-04 16:33:08,148 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5400504349495008\n",
            "2023-04-04 16:33:09,572 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5350906751057356\n",
            "2023-04-04 16:33:10,910 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.550469936259434\n",
            "2023-04-04 16:33:12,245 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5363852323931462\n",
            "2023-04-04 16:33:13,630 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5379578805504468\n",
            "2023-04-04 16:33:15,050 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5359092172517799\n",
            "2023-04-04 16:33:16,402 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5354191966173052\n",
            "2023-04-04 16:33:17,929 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.535723638153733\n",
            "2023-04-04 16:33:19,364 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 8.405983162873554e-07\n",
            "2023-04-04 16:33:19,367 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 9==================\n",
            "2023-04-04 16:33:19,376 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5402956641983023\n",
            "2023-04-04 16:33:20,773 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5721809841813215\n",
            "2023-04-04 16:33:22,136 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5370304596718372\n",
            "2023-04-04 16:33:23,538 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5371146725169713\n",
            "2023-04-04 16:33:25,007 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.563296118070381\n",
            "2023-04-04 16:33:26,412 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5421491194659995\n",
            "2023-04-04 16:33:27,750 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5436676727107257\n",
            "2023-04-04 16:33:29,147 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.539888509850112\n",
            "2023-04-04 16:33:30,570 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5474209176778726\n",
            "2023-04-04 16:33:32,014 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.536554518187823\n",
            "2023-04-04 16:33:33,354 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.540230767699723\n",
            "2023-04-04 16:33:34,770 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.539978399138891\n",
            "2023-04-04 16:33:36,171 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5449410973343007\n",
            "2023-04-04 16:33:37,522 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.537116083029812\n",
            "2023-04-04 16:33:38,921 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5514528772807163\n",
            "2023-04-04 16:33:40,609 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5363890972513703\n",
            "2023-04-04 16:33:42,146 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.53985078643325\n",
            "2023-04-04 16:33:43,878 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5414679214737994\n",
            "2023-04-04 16:33:45,880 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.542934919030881\n",
            "2023-04-04 16:33:47,759 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.539042761271442\n",
            "2023-04-04 16:33:49,658 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5393966191635646\n",
            "2023-04-04 16:33:51,251 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5362197996543208\n",
            "2023-04-04 16:33:52,767 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5443535733764253\n",
            "2023-04-04 16:33:54,396 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.541643660812811\n",
            "2023-04-04 16:33:55,851 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5501540808534344\n",
            "2023-04-04 16:33:57,188 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5518347367445244\n",
            "2023-04-04 16:33:58,691 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5363961769799044\n",
            "2023-04-04 16:34:00,078 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5452617600084815\n",
            "2023-04-04 16:34:01,538 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5384140226528542\n",
            "2023-04-04 16:34:02,934 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5484936440472752\n",
            "2023-04-04 16:34:04,362 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5381399277399008\n",
            "2023-04-04 16:34:05,805 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5376145328618263\n",
            "2023-04-04 16:34:07,183 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.544307484178192\n",
            "2023-04-04 16:34:08,527 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5452228718527292\n",
            "2023-04-04 16:34:09,919 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5364608059380211\n",
            "2023-04-04 16:34:11,293 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5373101178893174\n",
            "2023-04-04 16:34:12,654 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5388846473487319\n",
            "2023-04-04 16:34:14,032 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.535549452518286\n",
            "2023-04-04 16:34:15,419 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5501350413255628\n",
            "2023-04-04 16:34:16,769 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.537606189808993\n",
            "2023-04-04 16:34:18,227 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.543405153722114\n",
            "2023-04-04 16:34:19,591 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5382515707985767\n",
            "2023-04-04 16:34:20,951 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.537392459951508\n",
            "2023-04-04 16:34:22,334 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5425872125542632\n",
            "2023-04-04 16:34:23,678 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.538092704293417\n",
            "2023-04-04 16:34:25,107 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.538849184715224\n",
            "2023-04-04 16:34:26,507 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.539490080124924\n",
            "2023-04-04 16:34:27,904 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5412428845084327\n",
            "2023-04-04 16:34:29,261 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5456659219809508\n",
            "2023-04-04 16:34:30,612 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.542099832420046\n",
            "2023-04-04 16:34:32,011 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.548922933959274\n",
            "2023-04-04 16:34:33,379 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.546514582774627\n",
            "2023-04-04 16:34:34,765 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5365306411341675\n",
            "2023-04-04 16:34:36,283 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5569414431599697\n",
            "2023-04-04 16:34:37,686 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5352329011953352\n",
            "2023-04-04 16:34:39,078 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5364791073919717\n",
            "2023-04-04 16:34:40,558 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5379559209593356\n",
            "2023-04-04 16:34:41,908 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5644652941374273\n",
            "2023-04-04 16:34:43,306 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.535759837451068\n",
            "2023-04-04 16:34:44,699 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.536255410356386\n",
            "2023-04-04 16:34:46,058 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.547354496223524\n",
            "2023-04-04 16:34:47,481 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5637651536817205\n",
            "2023-04-04 16:34:48,915 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5402228645387166\n",
            "2023-04-04 16:34:50,328 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.542405050746454\n",
            "2023-04-04 16:34:51,680 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5537121552764803\n",
            "2023-04-04 16:34:53,127 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5370684407006487\n",
            "2023-04-04 16:34:54,551 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5446697012841981\n",
            "2023-04-04 16:34:55,896 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5405589666644135\n",
            "2023-04-04 16:34:57,227 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5391479992580124\n",
            "2023-04-04 16:34:58,627 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5597267512278832\n",
            "2023-04-04 16:35:00,117 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.555095526822133\n",
            "2023-04-04 16:35:01,551 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5372072726284745\n",
            "2023-04-04 16:35:02,924 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5447588327819832\n",
            "2023-04-04 16:35:04,380 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5525081642993948\n",
            "2023-04-04 16:35:05,774 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5399802248879988\n",
            "2023-04-04 16:35:07,136 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5422900982727277\n",
            "2023-04-04 16:35:08,546 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.539181945329822\n",
            "2023-04-04 16:35:09,971 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5438121448690707\n",
            "2023-04-04 16:35:11,360 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5495087080429182\n",
            "2023-04-04 16:35:12,755 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5481475093102168\n",
            "2023-04-04 16:35:14,090 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5442214159878451\n",
            "2023-04-04 16:35:15,438 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5412993706376956\n",
            "2023-04-04 16:35:16,888 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5432015508423402\n",
            "2023-04-04 16:35:18,351 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.539306491120408\n",
            "2023-04-04 16:35:19,733 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5365738700416056\n",
            "2023-04-04 16:35:21,093 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.54503069659679\n",
            "2023-04-04 16:35:22,443 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5359268181195374\n",
            "2023-04-04 16:35:23,816 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5478156974987483\n",
            "2023-04-04 16:35:25,349 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5604854375084047\n",
            "2023-04-04 16:35:26,748 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5354041420598041\n",
            "2023-04-04 16:35:28,117 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 2.0466819528053293e-07\n",
            "2023-04-04 16:35:28,119 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:58] - INFO: Current epoch: 10==================\n",
            "2023-04-04 16:35:28,128 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5494283392343025\n",
            "2023-04-04 16:35:29,509 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5637070787743301\n",
            "2023-04-04 16:35:30,892 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5566447548093\n",
            "2023-04-04 16:35:32,258 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.541500706110615\n",
            "2023-04-04 16:35:33,696 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5527626679744764\n",
            "2023-04-04 16:35:35,193 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5603455014761014\n",
            "2023-04-04 16:35:36,572 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5573486467331807\n",
            "2023-04-04 16:35:37,942 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.546454493776089\n",
            "2023-04-04 16:35:39,320 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5585398854524257\n",
            "2023-04-04 16:35:40,692 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.542191308629141\n",
            "2023-04-04 16:35:42,038 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5488303599105762\n",
            "2023-04-04 16:35:43,378 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5407567899653485\n",
            "2023-04-04 16:35:44,741 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5484015840640704\n",
            "2023-04-04 16:35:46,191 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5536796136487359\n",
            "2023-04-04 16:35:47,628 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5588648360087167\n",
            "2023-04-04 16:35:49,076 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5585015122473964\n",
            "2023-04-04 16:35:50,572 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5433349242898833\n",
            "2023-04-04 16:35:51,959 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.541666014608189\n",
            "2023-04-04 16:35:53,335 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5515993207636176\n",
            "2023-04-04 16:35:54,737 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.553303180053508\n",
            "2023-04-04 16:35:56,142 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5474497753579035\n",
            "2023-04-04 16:35:57,492 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5593079536903611\n",
            "2023-04-04 16:35:58,830 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.543497824534886\n",
            "2023-04-04 16:36:00,216 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5450718749736532\n",
            "2023-04-04 16:36:01,612 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.545467212612006\n",
            "2023-04-04 16:36:02,976 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5439835978770433\n",
            "2023-04-04 16:36:04,377 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5401401594336543\n",
            "2023-04-04 16:36:05,855 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.554100928188562\n",
            "2023-04-04 16:36:07,227 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5396003770392204\n",
            "2023-04-04 16:36:08,613 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5593111015514527\n",
            "2023-04-04 16:36:09,983 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5424525982000508\n",
            "2023-04-04 16:36:11,331 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5407280654859503\n",
            "2023-04-04 16:36:12,679 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.543001037280848\n",
            "2023-04-04 16:36:14,004 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5403120389144922\n",
            "2023-04-04 16:36:15,363 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.540134004732759\n",
            "2023-04-04 16:36:16,742 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5432768735113223\n",
            "2023-04-04 16:36:18,109 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5766346339674469\n",
            "2023-04-04 16:36:19,480 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5659849760688178\n",
            "2023-04-04 16:36:20,832 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5693218762360703\n",
            "2023-04-04 16:36:22,180 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5525414577793861\n",
            "2023-04-04 16:36:23,617 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.539572595397248\n",
            "2023-04-04 16:36:25,009 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5743604135154692\n",
            "2023-04-04 16:36:26,402 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5401163757807381\n",
            "2023-04-04 16:36:27,754 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5479466306723217\n",
            "2023-04-04 16:36:29,135 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5516190160355499\n",
            "2023-04-04 16:36:30,568 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5660776306302955\n",
            "2023-04-04 16:36:31,929 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.545066964908319\n",
            "2023-04-04 16:36:33,406 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.552763768409696\n",
            "2023-04-04 16:36:34,846 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5849706058438824\n",
            "2023-04-04 16:36:36,233 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5957745856703245\n",
            "2023-04-04 16:36:37,566 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5525219722648687\n",
            "2023-04-04 16:36:38,940 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5607542659595153\n",
            "2023-04-04 16:36:40,431 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5595817411559667\n",
            "2023-04-04 16:36:41,887 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5579855134204574\n",
            "2023-04-04 16:36:43,333 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.541203068122395\n",
            "2023-04-04 16:36:44,693 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.543042038266124\n",
            "2023-04-04 16:36:46,042 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5524112134382795\n",
            "2023-04-04 16:36:47,477 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5434551976571536\n",
            "2023-04-04 16:36:48,852 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5392800598665195\n",
            "2023-04-04 16:36:50,249 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.585595359091933\n",
            "2023-04-04 16:36:51,614 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5841651688337663\n",
            "2023-04-04 16:36:52,972 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5465819507221363\n",
            "2023-04-04 16:36:54,357 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.5425198236911186\n",
            "2023-04-04 16:36:55,714 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.549459786157249\n",
            "2023-04-04 16:36:57,057 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5615175936025962\n",
            "2023-04-04 16:36:58,499 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5670834209009388\n",
            "2023-04-04 16:36:59,885 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5431048717614075\n",
            "2023-04-04 16:37:01,332 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.559415032327813\n",
            "2023-04-04 16:37:02,711 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5394197725102545\n",
            "2023-04-04 16:37:04,082 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.5622954070813477\n",
            "2023-04-04 16:37:05,445 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5405747440218263\n",
            "2023-04-04 16:37:06,856 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.563040346532867\n",
            "2023-04-04 16:37:08,210 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.5418069206444065\n",
            "2023-04-04 16:37:09,574 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5389176194540513\n",
            "2023-04-04 16:37:10,960 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5459462556001313\n",
            "2023-04-04 16:37:12,359 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 0: 1.5515934522401396\n",
            "2023-04-04 16:37:13,731 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 200: 1.5491757083219075\n",
            "2023-04-04 16:37:15,123 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 400: 1.6025015644358813\n",
            "2023-04-04 16:37:16,462 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 600: 1.5735073575832212\n",
            "2023-04-04 16:37:17,895 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 800: 1.5463356837463333\n",
            "2023-04-04 16:37:19,276 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1000: 1.5453114805093837\n",
            "2023-04-04 16:37:20,699 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1200: 1.5446919280899363\n",
            "2023-04-04 16:37:22,045 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1400: 1.5734613383174718\n",
            "2023-04-04 16:37:23,427 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1600: 1.5473775760347417\n",
            "2023-04-04 16:37:24,784 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 1800: 1.54141532040848\n",
            "2023-04-04 16:37:26,249 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2000: 1.5453342336482923\n",
            "2023-04-04 16:37:27,631 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2200: 1.5433848753662716\n",
            "2023-04-04 16:37:28,948 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2400: 1.545444445577758\n",
            "2023-04-04 16:37:30,302 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2600: 1.5401996385314554\n",
            "2023-04-04 16:37:31,651 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:104] - INFO: Current loss in step 2800: 1.5834347198594152\n",
            "2023-04-04 16:37:33,102 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\gae\\torch\\trainers\\al_trainer.py[line:68] - INFO: Current        h: 4.949706244872232e-08\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'fdr': 0.1818, 'tpr': 0.45, 'fpr': 0.08, 'shd': 13, 'nnz': 11, 'precision': 0.6429, 'recall': 0.45, 'F1': 0.5294, 'gscore': 0.2}\n"
          ]
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import DAG, IIDSimulation\n",
        "from castle.algorithms import GAE\n",
        "\n",
        "\n",
        "#######################################\n",
        "# graph_auto_encoder used simulate data\n",
        "#######################################\n",
        "# simulate data for graph-auto-encoder\n",
        "weighted_random_dag = DAG.erdos_renyi(n_nodes=10, n_edges=20, weight_range=(0.5, 2.0), seed=1)\n",
        "dataset = IIDSimulation(W=weighted_random_dag, n=2000, method='linear', sem_type='gauss')\n",
        "true_dag, X = dataset.B, dataset.X\n",
        "\n",
        "ga = GAE(input_dim=10)\n",
        "ga.learn(X)\n",
        "\n",
        "# plot est_dag and true_dag\n",
        "GraphDAG(ga.causal_matrix, true_dag, save_name='Result_gae')\n",
        "\n",
        "# calculate accuracy\n",
        "met = MetricsDAG(ga.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_gae.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b8a754b",
      "metadata": {
        "id": "6b8a754b"
      },
      "source": [
        "##__M 15: RL*__\n",
        "* A RL-based algorithm that can work with flexible score functions (including non-smooth ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5cf703d",
      "metadata": {
        "id": "d5cf703d",
        "outputId": "abb0f8c3-16b5-4a90-841f-c19d7711c9e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-05 10:34:53,809 - C:\\Users\\notebook\\castle\\datasets\\simulator.py[line:270] - INFO: Finished synthetic dataset\n",
            "2023-04-05 10:34:53,814 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:222] - INFO: GPU is available.\n",
            "2023-04-05 10:34:53,846 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:264] - INFO: Python version is 3.9.13\n",
            "2023-04-05 10:34:54,101 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:289] - INFO: Original sl: 1.1201855398449987, su: 5.838377459109804, strue: -8.879814460155002\n",
            "2023-04-05 10:34:54,107 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:290] - INFO: Transfomed sl: 1.1201855398449987, su: 5.838377459109804, lambda2: 0.001, true: -10.59727981726336\n",
            "2023-04-05 10:34:54,178 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:334] - INFO: Finished creating training dataset and reward class\n",
            "2023-04-05 10:34:54,187 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:350] - INFO: Starting training.\n",
            "  0%|                                                                                        | 0/20000 [00:00<?, ?it/s]2023-04-05 10:34:54,271 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:362] - INFO: Shape of actor.input: torch.Size([64, 10, 64])\n",
            "2023-04-05 10:34:57,454 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:415] - INFO: [iter 1] reward_batch: -3.188, max_reward: -0.9772, max_reward_batch: -0.9772\n",
            "2023-04-05 10:34:57,459 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\rl\\torch\\rl.py[line:428] - INFO: [iter 1] lambda1 1.0, upper 5.0, lambda2 0.01, upper 0.01, score_min 0.8194, cyc_min 157.9\n",
            "  0%|                                                                             | 5/20000 [00:15<16:49:09,  3.03s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19988\\752710403.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'IID_Test'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\rl\\torch\\rl.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, dag, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mcausal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcausal_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\rl\\torch\\rl.py\u001b[0m in \u001b[0;36m_rl\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mreward_feed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallreward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcal_rewards\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraphs_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# np.array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m             \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_reward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\rl\\torch\\models\\actor_graph.py\u001b[0m in \u001b[0;36mbuild_reward\u001b[1;34m(self, reward_)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_optim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbuild_optim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\rl\\torch\\models\\actor_graph.py\u001b[0m in \u001b[0;36mbuild_optim\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;31m# Minimize step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from castle.algorithms import RL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = RL()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_RL')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_RL.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c26296f",
      "metadata": {
        "id": "2c26296f"
      },
      "source": [
        "##__M 16: CORL*__\n",
        "* A RL- and order-based algorithm that improves the efficiency and scalability of previous RL-based approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4c725b",
      "metadata": {
        "id": "ab4c725b",
        "outputId": "28add662-57e9-44a4-dcd8-dc1ec10374b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-06 13:08:50,671 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:173] - INFO: GPU is available.\n",
            "2023-04-06 13:08:50,682 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:233] - INFO: Python version is 3.9.13\n",
            "2023-04-06 13:08:50,722 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:277] - INFO: Shape of input batch: 64, 3, 100\n",
            "2023-04-06 13:08:50,723 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:279] - INFO: Shape of input batch: 64, 3, 256\n",
            "2023-04-06 13:08:50,724 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:281] - INFO: Starting training.\n",
            "  0%|                                                                                        | 0/10000 [00:00<?, ?it/s]C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py:334: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
            "  td_target=torch.tensor(td_target),\n",
            "2023-04-06 13:08:51,103 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 1] max_reward: -17.77, max_reward_batch: -17.77\n",
            "2023-04-06 13:08:51,104 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 1] score_min 17.77\n",
            "  2%|█▌                                                                          | 199/10000 [01:19<1:11:06,  2.30it/s]2023-04-06 13:10:10,121 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 200] max_reward: -17.77, max_reward_batch: -17.77\n",
            "  4%|███                                                                         | 399/10000 [02:37<1:09:29,  2.30it/s]2023-04-06 13:11:28,917 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 400] max_reward: -17.77, max_reward_batch: -17.77\n",
            "  5%|███▊                                                                        | 499/10000 [03:17<1:05:36,  2.41it/s]2023-04-06 13:12:08,859 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 500] score_min 17.77\n",
            "  6%|████▋                                                                         | 599/10000 [03:56<58:25,  2.68it/s]2023-04-06 13:12:47,769 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 600] max_reward: -17.77, max_reward_batch: -17.77\n",
            "  8%|██████▏                                                                       | 799/10000 [05:14<56:46,  2.70it/s]2023-04-06 13:14:05,623 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 800] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 10%|███████▊                                                                      | 999/10000 [06:32<57:02,  2.63it/s]2023-04-06 13:15:23,840 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 1000] max_reward: -17.77, max_reward_batch: -17.77\n",
            "2023-04-06 13:15:23,841 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 1000] score_min 17.77\n",
            " 12%|█████████▏                                                                   | 1199/10000 [07:49<55:04,  2.66it/s]2023-04-06 13:16:40,912 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 1200] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 14%|██████████▍                                                                | 1399/10000 [09:05<1:07:49,  2.11it/s]2023-04-06 13:17:56,929 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 1400] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 15%|███████████▏                                                               | 1499/10000 [09:48<1:05:02,  2.18it/s]2023-04-06 13:18:40,095 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 1500] score_min 17.77\n",
            " 16%|███████████▉                                                               | 1599/10000 [10:31<1:02:06,  2.25it/s]2023-04-06 13:19:22,250 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 1600] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 18%|█████████████▊                                                               | 1799/10000 [11:58<58:42,  2.33it/s]2023-04-06 13:20:49,529 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 1800] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 20%|███████████████▍                                                             | 1999/10000 [13:25<52:43,  2.53it/s]2023-04-06 13:22:16,462 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 2000] max_reward: -17.77, max_reward_batch: -17.77\n",
            "2023-04-06 13:22:16,463 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 2000] score_min 17.77\n",
            " 22%|████████████████▉                                                            | 2199/10000 [14:52<56:20,  2.31it/s]2023-04-06 13:23:43,603 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 2200] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 24%|█████████████████▉                                                         | 2399/10000 [1:05:35<55:27,  2.28it/s]2023-04-06 14:14:27,014 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 2400] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 25%|██████████████████▋                                                        | 2499/10000 [1:06:18<55:14,  2.26it/s]2023-04-06 14:15:09,846 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 2500] score_min 17.77\n",
            " 26%|███████████████████▍                                                       | 2599/10000 [1:07:02<52:32,  2.35it/s]2023-04-06 14:15:53,440 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 2600] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 28%|████████████████████▉                                                      | 2799/10000 [1:08:30<51:24,  2.33it/s]2023-04-06 14:17:21,167 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 2800] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 30%|██████████████████████▍                                                    | 2999/10000 [1:09:56<53:47,  2.17it/s]2023-04-06 14:18:47,461 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 3000] max_reward: -17.77, max_reward_batch: -17.77\n",
            "2023-04-06 14:18:47,462 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 3000] score_min 17.77\n",
            " 32%|███████████████████████▉                                                   | 3199/10000 [1:11:21<48:39,  2.33it/s]2023-04-06 14:20:12,829 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 3200] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 34%|█████████████████████████▍                                                 | 3399/10000 [1:12:47<45:28,  2.42it/s]2023-04-06 14:21:38,360 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 3400] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 35%|██████████████████████████▏                                                | 3499/10000 [1:13:28<45:46,  2.37it/s]2023-04-06 14:22:20,172 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 3500] score_min 17.77\n",
            " 36%|██████████████████████████▉                                                | 3599/10000 [1:14:11<48:44,  2.19it/s]2023-04-06 14:23:02,976 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 3600] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 38%|████████████████████████████▍                                              | 3799/10000 [1:15:38<42:49,  2.41it/s]2023-04-06 14:24:29,578 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 3800] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 40%|█████████████████████████████▉                                             | 3999/10000 [1:17:05<39:38,  2.52it/s]2023-04-06 14:25:56,220 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 4000] max_reward: -17.77, max_reward_batch: -17.77\n",
            "2023-04-06 14:25:56,221 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 4000] score_min 17.77\n",
            " 42%|███████████████████████████████▍                                           | 4199/10000 [1:18:29<40:59,  2.36it/s]2023-04-06 14:27:20,280 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 4200] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 44%|████████████████████████████████▉                                          | 4399/10000 [1:19:54<39:41,  2.35it/s]2023-04-06 14:28:45,194 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 4400] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 45%|█████████████████████████████████▋                                         | 4499/10000 [1:20:36<35:31,  2.58it/s]2023-04-06 14:29:27,740 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 4500] score_min 17.77\n",
            " 46%|██████████████████████████████████▍                                        | 4599/10000 [1:21:18<32:49,  2.74it/s]2023-04-06 14:30:09,659 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 4600] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 48%|███████████████████████████████████▉                                       | 4799/10000 [2:22:50<55:35,  1.56it/s]2023-04-06 15:31:41,347 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 4800] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 50%|█████████████████████████████████████▍                                     | 4999/10000 [2:23:42<20:54,  3.99it/s]2023-04-06 15:32:33,012 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 5000] max_reward: -17.77, max_reward_batch: -17.77\n",
            "2023-04-06 15:32:33,013 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 5000] score_min 17.77\n",
            " 52%|██████████████████████████████████████▉                                    | 5199/10000 [2:24:34<24:30,  3.27it/s]2023-04-06 15:33:25,647 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 5200] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 54%|████████████████████████████████████████▍                                  | 5399/10000 [2:25:29<20:12,  3.79it/s]2023-04-06 15:34:20,827 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 5400] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 55%|█████████████████████████████████████████▏                                 | 5499/10000 [2:25:56<19:35,  3.83it/s]2023-04-06 15:34:47,357 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 5500] score_min 17.77\n",
            " 56%|█████████████████████████████████████████▉                                 | 5599/10000 [2:26:23<19:51,  3.69it/s]2023-04-06 15:35:14,334 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 5600] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 58%|███████████████████████████████████████████▍                               | 5799/10000 [2:27:17<19:03,  3.67it/s]2023-04-06 15:36:08,445 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 5800] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 60%|████████████████████████████████████████████▉                              | 5999/10000 [2:28:15<18:16,  3.65it/s]2023-04-06 15:37:06,924 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 6000] max_reward: -17.77, max_reward_batch: -17.77\n",
            "2023-04-06 15:37:06,925 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 6000] score_min 17.77\n",
            " 62%|██████████████████████████████████████████████▍                            | 6199/10000 [2:29:26<23:00,  2.75it/s]2023-04-06 15:38:17,231 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 6200] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 64%|███████████████████████████████████████████████▉                           | 6399/10000 [2:30:40<23:23,  2.56it/s]2023-04-06 15:39:31,832 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 6400] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 65%|████████████████████████████████████████████████▋                          | 6499/10000 [2:31:18<29:42,  1.96it/s]2023-04-06 15:40:10,799 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:377] - INFO: [iter 6500] score_min 17.77\n",
            " 66%|█████████████████████████████████████████████████▍                         | 6599/10000 [2:31:58<21:34,  2.63it/s]2023-04-06 15:40:49,375 - C:\\Users\\notebook\\castle\\algorithms\\gradient\\corl\\torch\\corl.py[line:371] - INFO: [iter 6600] max_reward: -17.77, max_reward_batch: -17.77\n",
            " 67%|█████████████████████████████████████████████████▏                       | 6741/10000 [2:32:50<1:13:53,  1.36s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23144\\736542736.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# X, true_dag, _ = load_dataset('IID_Test')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCORL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mGraphDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMetricsDAG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcausal_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_dag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\corl.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, data, columns, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m                              f'n_nodes: {self.seq_length}.')\n\u001b[0;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdag_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dag_mask'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mcausal_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rl_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         self.causal_matrix = Tensor(causal_matrix,\n\u001b[0;32m    219\u001b[0m                                     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\corl.py\u001b[0m in \u001b[0;36m_rl_search\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# (batch_size, n_nodes, input_dim)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mencoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[0mdecoder_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\frame\\_actor.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\models\\decoders.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0ms_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0ms_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhi_ci\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhi_ci\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0maction_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\castle\\algorithms\\gradient\\corl\\torch\\models\\_base_network.py\u001b[0m in \u001b[0;36mstep_decode\u001b[1;34m(self, input, state)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'LSTMDecoder'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;31m# Run the cell on a combination of the previous input and state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_batched\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m         ret = _VF.lstm_cell(\n\u001b[0m\u001b[0;32m   1238\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from castle.algorithms.gradient.corl.torch import CORL\n",
        "from castle.datasets import load_dataset\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "# X, true_dag, _ = load_dataset('IID_Test')\n",
        "n = CORL()\n",
        "n.learn(X)\n",
        "GraphDAG(n.causal_matrix, true_dag, save_name='Result_CORL')\n",
        "met = MetricsDAG(n.causal_matrix, true_dag)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_CORL.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EventSequence/Function-based"
      ],
      "metadata": {
        "id": "uIfigZAgWUIP"
      },
      "id": "uIfigZAgWUIP"
    },
    {
      "cell_type": "markdown",
      "id": "c4079bb8",
      "metadata": {
        "id": "c4079bb8"
      },
      "source": [
        "##__M 17: TTPM__\n",
        "*\tA causal structure learning algorithm based on Topological Hawkes process for spatio-temporal event sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "927b495d",
      "metadata": {
        "id": "927b495d",
        "outputId": "5e7dd189-ddd9-460d-95c6-91ce92f3ed89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-06 12:44:18,769 - C:\\Users\\notebook\\castle\\backend\\__init__.py[line:36] - INFO: You can use `os.environ['CASTLE_BACKEND'] = backend` to set the backend(`pytorch` or `mindspore`).\n",
            "2023-04-06 12:44:19,079 - C:\\Users\\notebook\\castle\\algorithms\\__init__.py[line:36] - INFO: You are using ``pytorch`` as the backend.\n",
            "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 583.14it/s]\n",
            "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 2196.43it/s]\n",
            "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 4373.62it/s]\n",
            "100%|████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 5063.14it/s]\n",
            "2023-04-06 12:44:19,526 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 0]: likelihood_score = -5957.134209880932\n",
            "2023-04-06 12:44:19,862 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 1]: likelihood_score = -5897.55306114358\n",
            "2023-04-06 12:44:20,279 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 2]: likelihood_score = -5841.039885736854\n",
            "2023-04-06 12:44:20,641 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 3]: likelihood_score = -5790.7452051866385\n",
            "2023-04-06 12:44:20,970 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 4]: likelihood_score = -5743.925521150004\n",
            "2023-04-06 12:44:21,384 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 5]: likelihood_score = -5704.304908025697\n",
            "2023-04-06 12:44:21,799 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 6]: likelihood_score = -5670.410069126243\n",
            "2023-04-06 12:44:22,270 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 7]: likelihood_score = -5638.422004527301\n",
            "2023-04-06 12:44:22,770 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 8]: likelihood_score = -5615.361020208201\n",
            "2023-04-06 12:44:23,248 - C:\\Users\\notebook\\castle\\algorithms\\ttpm\\ttpm.py[line:225] - INFO: [iter 9]: likelihood_score = -5610.196280633188\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x300 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'fdr': 0.0,\n",
              " 'tpr': 1.0,\n",
              " 'fpr': 0.0,\n",
              " 'shd': 0,\n",
              " 'nnz': 9,\n",
              " 'precision': 1.0,\n",
              " 'recall': 1.0,\n",
              " 'F1': 1.0,\n",
              " 'gscore': 1.0}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "from castle.datasets import load_dataset\n",
        "from castle.algorithms import TTPM\n",
        "# Data Simulation for TTPM\n",
        "X, true_causal_matrix, topology_matrix = load_dataset('THP_Test')\n",
        "ttpm = TTPM(topology_matrix, max_hop=2)\n",
        "ttpm.learn(X)\n",
        "causal_matrix = ttpm.causal_matrix\n",
        "# plot est_dag and true_dag\n",
        "GraphDAG(ttpm.causal_matrix, true_causal_matrix, save_name='Result_ttpm')\n",
        "    # calculate accuracy\n",
        "met = MetricsDAG(ttpm.causal_matrix, true_causal_matrix)\n",
        "print(met.metrics)\n",
        "df = pd.DataFrame([met.metrics])\n",
        "df.to_csv('Scores_ttpm.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b619da6c",
      "metadata": {
        "id": "b619da6c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "原始单元格格式",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}