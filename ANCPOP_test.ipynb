{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po_zr02dCTVJ"
      },
      "source": [
        "# Get start_Step1\n",
        "\n",
        "Define a function for solving the NCPO problems with given standard deviations of process noise and observtion noise, length of  estimation data and required relaxation level.\n",
        "\n",
        "* mount drive\n",
        "* set envirment\n",
        "* install packages"
      ],
      "id": "Po_zr02dCTVJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHf74wQIVx2v",
        "outputId": "91b2ab56-ee28-4d2a-e9ee-fd0504d12257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/Colab Notebooks/Causality_NotesTest/\")"
      ],
      "id": "oHf74wQIVx2v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M9j88nhBZwF"
      },
      "outputs": [],
      "source": [
        "!export PYTHONPATH=\"$PYTHONPATH:/content/drive/MyDrive/Colab Notebooks/Causality_NotesTest\"\n",
        "import os\n",
        "os.environ['MOSEKLM_LICENSE_FILE']=\"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "id": "2M9j88nhBZwF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a349920",
        "outputId": "473fd21f-38c2-41bb-d88e-77af098af630"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mosek\n",
            "  Downloading Mosek-10.1.21-cp37-abi3-manylinux2014_x86_64.whl (14.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mosek) (1.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: mosek\n",
            "Successfully installed mosek-10.1.21\n",
            "Collecting ncpol2sdpa\n",
            "  Downloading ncpol2sdpa-1.12.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sympy>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from ncpol2sdpa) (1.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ncpol2sdpa) (1.23.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=0.7.2->ncpol2sdpa) (1.3.0)\n",
            "Building wheels for collected packages: ncpol2sdpa\n",
            "  Building wheel for ncpol2sdpa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ncpol2sdpa: filename=ncpol2sdpa-1.12.2-py3-none-any.whl size=70796 sha256=4e094c64ab7f22636fc5ef05e64e6529a38d6f3920c7ab1e54d3bb7d4fd31f2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/d7/fe/ab61f3bf30a350feab3bb4dccd63932d56cbbd32b9ec0d94fa\n",
            "Successfully built ncpol2sdpa\n",
            "Installing collected packages: ncpol2sdpa\n",
            "Successfully installed ncpol2sdpa-1.12.2\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting gcastle==1.0.3rc2\n",
            "  Downloading gcastle-1.0.3rc2-py3-none-any.whl (187 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (3.7.1)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.19.1 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (1.11.4)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from gcastle==1.0.3rc2) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.2->gcastle==1.0.3rc2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.22.0->gcastle==1.0.3rc2) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.1->gcastle==1.0.3rc2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.1->gcastle==1.0.3rc2) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.2->gcastle==1.0.3rc2) (1.16.0)\n",
            "Installing collected packages: gcastle\n",
            "Successfully installed gcastle-1.0.3rc2\n"
          ]
        }
      ],
      "source": [
        "### for colab ###\n",
        "# To execute the notebook directly in colab make sure your MOSEK license file is in one the locations\n",
        "#\n",
        "# /content/mosek.lic   or   /root/mosek/mosek.lic\n",
        "#\n",
        "# inside this notebook's internal filesystem.\n",
        "# Install MOSEK and ncpol2sdpa if not already installed\n",
        "!pip install mosek torch\n",
        "!pip install ncpol2sdpa\n",
        "!pip install networkx\n",
        "!pip install gcastle==1.0.3rc2"
      ],
      "id": "8a349920"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b276ZutL8fmp"
      },
      "source": [
        "# NCPOLR_Step2 ##\n",
        "* NCPOLR (estimate_1hidden, estimate_2hidden)\n",
        "* ANM_NCPO (learn, anmNCPO_estimate)"
      ],
      "id": "b276ZutL8fmp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3370fed0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import scale\n",
        "from itertools import combinations\n",
        "from castle.common import BaseLearner, Tensor\n",
        "from castle.common.independence_tests import hsic_test\n",
        "# from inputlds import*\n",
        "# from functions import*\n",
        "from ncpol2sdpa import*\n",
        "from math import sqrt\n",
        "\n",
        "\n",
        "class NCPOLR(object):\n",
        "    \"\"\"Estimator based on NCPOP Regressor\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Quan Zhou https://github.com/Quan-Zhou/Proper-Learning-of-LDS/blob/master/ncpop/functions.py\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(NCPOLR, self).__init__()\n",
        "\n",
        "    def estimate_1hidden(self, X, Y):\n",
        "        \"\"\"Fit Estimator based on NCPOP Regressor model and predict y or produce residuals.\n",
        "        The module converts a noncommutative optimization problem provided in SymPy\n",
        "        format to an SDPA semidefinite programming problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array\n",
        "            Variable seen as cause\n",
        "        Y: array\n",
        "            Variable seen as effect\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predict: array\n",
        "            regression predict values of y or residuals\n",
        "        \"\"\"\n",
        "\n",
        "        T = len(Y)\n",
        "        level = 1\n",
        "\n",
        "        # Decision Variables\n",
        "        G = generate_operators(\"G\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        f = generate_operators(\"f\", n_vars=T, hermitian=True, commutative=False)\n",
        "        n = generate_operators(\"m\", n_vars=T, hermitian=True, commutative=False)\n",
        "        p = generate_operators(\"p\", n_vars=T, hermitian=True, commutative=False)\n",
        "\n",
        "        # Objective\n",
        "        obj = sum((Y[i]-f[i])**2 for i in range(T)) + 0.5*sum(p[i] for i in range(T))\n",
        "\n",
        "        # Constraints\n",
        "        ine1 = [f[i] - G*X[i] - n[i] for i in range(T)]\n",
        "        ine2 = [-f[i] + G*X[i] + n[i] for i in range(T)]\n",
        "        ine3 = [p[i]-n[i] for i in range(T)]\n",
        "        ine4 = [p[i]+n[i] for i in range(T)]\n",
        "        ines = ine1+ine2+ine3+ine4\n",
        "\n",
        "        # Solve the NCPO\n",
        "        sdp = SdpRelaxation(variables = flatten([G,f,n,p]),verbose = 1)\n",
        "        sdp.get_relaxation(level, objective=obj, inequalities=ines)\n",
        "        sdp.solve(solver='mosek')\n",
        "        #sdp.solve(solver='sdpa', solverparameters={\"executable\":\"sdpa_gmp\",\"executable\": \"C:/Users/zhouq/Documents/sdpa7-windows/sdpa.exe\"})\n",
        "        print(sdp.primal, sdp.dual, sdp.status)\n",
        "\n",
        "        if(sdp.status != 'infeasible'):\n",
        "            print('ok.')\n",
        "            est_noise = []\n",
        "            for i in range(T):\n",
        "                est_noise.append(sdp[n[i]])\n",
        "            print(est_noise)\n",
        "            return est_noise\n",
        "        else:\n",
        "            print('Cannot find feasible solution.')\n",
        "            return\n",
        "\n",
        "    def estimate_2hidden(self, X, Y):\n",
        "        \"\"\"Fit Estimator based on NCPOP Regressor model with 2*2 hidden state matrix and predict y or produce residuals.\n",
        "        The module converts a noncommutative optimization problem provided in SymPy\n",
        "        format to an SDPA semidefinite programming problem.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array\n",
        "            Variable seen as cause\n",
        "        Y: array\n",
        "            Variable seen as effect\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_predict: array\n",
        "            regression predict values of y or residuals\n",
        "        \"\"\"\n",
        "\n",
        "        T = len(Y)\n",
        "        level = 1\n",
        "\n",
        "        # Decision Variables\n",
        "        G = generate_operators(\"G\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        H = generate_operators(\"H\", n_vars=1, hermitian=True, commutative=False)[0]\n",
        "        Fdash = generate_operators(\"Fdash\", n_vars=2, hermitian=True, commutative=False)\n",
        "        m = generate_operators(\"m\", n_vars=T, hermitian=True, commutative=False)\n",
        "        q = generate_operators(\"q\", n_vars=T, hermitian=True, commutative=False)\n",
        "        p = generate_operators(\"p\", n_vars=T, hermitian=True, commutative=True)\n",
        "        f = generate_operators(\"f\", n_vars=T, hermitian=True, commutative=True)\n",
        "        n = generate_operators(\"n\", n_vars=T, hermitian=True, commutative=False)\n",
        "        u = generate_operators(\"u\", n_vars=T, hermitian=True, commutative=False)\n",
        "\n",
        "        # Objective\n",
        "        obj = sum((Y[i]-f[i])**2 for i in range(T)) + 0.0005*sum(p[i]**2 for i in range(T)) + 0.001*sum(q[i]**2 for i in range(T))\n",
        "\n",
        "        # Constraints\n",
        "        ine1 = [ f[i] - Fdash[0]*m[i]  - Fdash[1]*m[i-1] - p[i] for i in range(1,T)]\n",
        "        ine2 = [-f[i] + Fdash[0]*m[i] +Fdash[1]*m[i-1] + p[i] for i in range(1,T)]\n",
        "        ine3 = [ m[i] - G*m[i-1]  - H*m[i-2] - q[i] for i in range(2,T)]\n",
        "        ine4 = [-m[i] + G*m[i-1] + H*m[i-2] + q[i] for i in range(2,T)]\n",
        "        ine5 = [p[i] + n[i] for i in range(T)]\n",
        "        ine6 = [p[i] - n[i] for i in range(T)]\n",
        "        ine7 = [q[i] + u[i] for i in range(T)]\n",
        "        ine8 = [q[i] - u[i] for i in range(T)]\n",
        "\n",
        "        ines = ine1+ine2+ine3+ine4\n",
        "\n",
        "        # Solve the NCPO\n",
        "        sdp = SdpRelaxation(variables = flatten([G,H,Fdash,f,p,m,q]),verbose = 1)\n",
        "        sdp.get_relaxation(level, objective=obj, inequalities=ines)\n",
        "        sdp.solve(solver='mosek')\n",
        "        #sdp.solve(solver='sdpa', solverparameters={\"executable\":\"sdpa_gmp\",\"executable\": \"C:/Users/zhouq/Documents/sdpa7-windows/sdpa.exe\"})\n",
        "        print(sdp.primal, sdp.dual, sdp.status)\n",
        "\n",
        "        if(sdp.status != 'infeasible'):\n",
        "            print('ok.')\n",
        "            est_noise = []\n",
        "            for i in range(T):\n",
        "                est_noise.append(sdp[Y[i]-f[i]+q[i]])\n",
        "            print(est_noise)\n",
        "            return est_noise\n",
        "        else:\n",
        "            print('Cannot find feasible solution.')\n",
        "            return\n",
        "\n",
        "\n",
        "\n",
        "class ANM_NCPO(BaseLearner):\n",
        "    \"\"\"\n",
        "    Nonlinear causal discovery with additive noise models\n",
        "\n",
        "    Use Estimator based on NCPOP Regressor and independent Gaussian noise,\n",
        "    For the independence test, we implemented the HSIC with a Gaussian kernel,\n",
        "    where we used the gamma distribution as an approximation for the\n",
        "    distribution of the HSIC under the null hypothesis of independence\n",
        "    in order to calculate the p-value of the test result.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Hoyer, Patrik O and Janzing, Dominik and Mooij, Joris M and Peters,\n",
        "    Jonas and Schölkopf, Bernhard,\n",
        "    \"Nonlinear causal discovery with additive noise models\", NIPS 2009\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha : float, default 0.05\n",
        "        significance level be used to compute threshold\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    causal_matrix : array like shape of (n_features, n_features)\n",
        "        Learned causal structure matrix.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=0.05):\n",
        "        super(ANM_NCPO, self).__init__()\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def learn(self, data,causalmodelling, columns=None, regressor=NCPOLR(),test_method=hsic_test, **kwargs):\n",
        "        \"\"\"Set up and run the ANM_NCPOP algorithm.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: numpy.ndarray or Tensor\n",
        "            Training data.\n",
        "        causalmodelling: Modelling type(hidden_state1,hidden_state2,ARMA,Discrete)\n",
        "        columns : Index or array-like\n",
        "            Column labels to use for resulting tensor. Will default to\n",
        "            RangeIndex (0, 1, 2, ..., n) if no column labels are provided.\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is NCPOLR.\n",
        "            If user defined, must implement `estimate` self.method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test self.method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "        \"\"\"\n",
        "\n",
        "        self.regressor = regressor\n",
        "\n",
        "        # create learning model and ground truth model\n",
        "        data = Tensor(data, columns=columns)\n",
        "\n",
        "        node_num = data.shape[1]\n",
        "        self.causal_matrix = Tensor(np.zeros((node_num, node_num)),\n",
        "                                    index=data.columns,\n",
        "                                    columns=data.columns)\n",
        "\n",
        "        for i, j in combinations(range(node_num), 2):\n",
        "            x = data[:, i]\n",
        "            y = data[:, j]\n",
        "            xx = x.reshape((-1, 1))\n",
        "            yy = y.reshape((-1, 1))\n",
        "\n",
        "            flag = test_method(xx, yy, alpha=self.alpha)\n",
        "            if flag == 1:\n",
        "                continue\n",
        "            # test x-->y\n",
        "            flag = self.anmNCPO_estimate(x, y, causalmodelling, regressor = regressor, test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[i, j] = 1\n",
        "            # test y-->x\n",
        "            flag = self.anmNCPO_estimate(y, x, causalmodelling, regressor = regressor, test_method=test_method)\n",
        "            if flag:\n",
        "                self.causal_matrix[j, i] = 1\n",
        "\n",
        "    def anmNCPO_estimate(self, x, y, causalmodelling, regressor=NCPOLR(), test_method=hsic_test):\n",
        "        \"\"\"Compute the fitness score of the ANM_NCPOP Regression model in the x->y direction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: array\n",
        "            Variable seen as cause\n",
        "        y: array\n",
        "            Variable seen as effect\n",
        "        causalmodelling: Modelling type(hidden_state1,hidden_state2,ARMA,Discrete)\n",
        "        regressor: Class\n",
        "            Nonlinear regression estimator, if not provided, it is NCPOP.\n",
        "            If user defined, must implement `estimate` self.method. such as :\n",
        "                `regressor.estimate(x, y)`\n",
        "        test_method: callable, default test_method\n",
        "            independence test self.method, if not provided, it is HSIC.\n",
        "            If user defined, must accept three arguments--x, y and keyword\n",
        "            argument--alpha. such as :\n",
        "                `test_method(x, y, alpha=0.05)`\n",
        "        Returns\n",
        "        -------\n",
        "        out: int, 0 or 1\n",
        "            If 1, residuals n is independent of x, then accept x --> y\n",
        "            If 0, residuals n is not independent of x, then reject x --> y\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        x = scale(x)\n",
        "        y = scale(y)\n",
        "        if causalmodelling == 'hidden_state1':\n",
        "            y_res = regressor.estimate_1hidden(x, y)\n",
        "        elif causalmodelling == 'hidden_state2':\n",
        "            y_res = regressor.estimate_2hidden(x, y)\n",
        "        else:\n",
        "            print('WRONG MODELLING TYPE')\n",
        "        #if causalmodelling='ARMA':\n",
        "        #if causalmodelling='Discrete':\n",
        "\n",
        "\n",
        "        flag = test_method(np.asarray(y_res).reshape((-1, 1)), np.asarray(x).reshape((-1, 1)), alpha=self.alpha)\n",
        "        print(flag)\n",
        "\n",
        "        return flag\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "3370fed0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test_BuiltinDataSet_Step3\n",
        "* erdos_renyi function\n",
        "Set Gauss_linear/Nonlinear, nodes, edges and DataSize"
      ],
      "metadata": {
        "id": "2IMxmQH_C_0U"
      },
      "id": "2IMxmQH_C_0U"
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# 2020.12 added (1) low rank DAG generations;\n",
        "#               (2) quad functons for causal functions;\n",
        "#               (3) event-type data\n",
        "# 2021.08 deleted (1) condition: sem_type == 'poisson'\n",
        "# Huawei Technologies Co., Ltd.\n",
        "#\n",
        "# Copyright (C) 2021. Huawei Technologies Co., Ltd. All rights reserved.\n",
        "#\n",
        "# Copyright (c) Xun Zheng (https://github.com/xunzheng/notears)\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import logging\n",
        "import random\n",
        "from random import sample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite\n",
        "from tqdm import tqdm\n",
        "from copy import deepcopy\n",
        "from itertools import combinations\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "\n",
        "class DAG(object):\n",
        "    '''\n",
        "    A class for simulating random (causal) DAG, where any DAG generator\n",
        "    method would return the weighed/binary adjacency matrix of a DAG.\n",
        "    Besides, we recommend using the python package \"NetworkX\"\n",
        "    to create more structures types.\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_permutation(M):\n",
        "        # np.random.permutation permutes first axis only\n",
        "        P = np.random.permutation(np.eye(M.shape[0]))\n",
        "        return P.T @ M @ P\n",
        "\n",
        "    @staticmethod\n",
        "    def _random_acyclic_orientation(B_und):\n",
        "        B = np.tril(DAG._random_permutation(B_und), k=-1)\n",
        "        B_perm = DAG._random_permutation(B)\n",
        "        return B_perm\n",
        "\n",
        "    @staticmethod\n",
        "    def _graph_to_adjmat(G):\n",
        "        return nx.to_numpy_array(G)\n",
        "\n",
        "    @staticmethod\n",
        "    def _BtoW(B, d, w_range):\n",
        "        U = np.random.uniform(low=w_range[0], high=w_range[1], size=[d, d])\n",
        "        U[np.random.rand(d, d) < 0.5] *= -1\n",
        "        W = (B != 0).astype(float) * U\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def _low_rank_dag(d, degree, rank):\n",
        "        \"\"\"\n",
        "        Simulate random low rank DAG with some expected degree.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        d: int\n",
        "            Number of nodes.\n",
        "        degree: int\n",
        "            Expected node degree, in + out.\n",
        "        rank: int\n",
        "            Maximum rank (rank < d-1).\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        B: np.nparray\n",
        "            Initialize DAG.\n",
        "        \"\"\"\n",
        "        prob = float(degree) / (d - 1)\n",
        "        B = np.triu((np.random.rand(d, d) < prob).astype(float), k=1)\n",
        "        total_edge_num = np.sum(B == 1)\n",
        "        sampled_pa = sample(range(d - 1), rank)\n",
        "        sampled_pa.sort(reverse=True)\n",
        "        sampled_ch = []\n",
        "        for i in sampled_pa:\n",
        "            candidate = set(range(i + 1, d))\n",
        "            candidate = candidate - set(sampled_ch)\n",
        "            sampled_ch.append(sample(candidate, 1)[0])\n",
        "            B[i, sampled_ch[-1]] = 1\n",
        "        remaining_pa = list(set(range(d)) - set(sampled_pa))\n",
        "        remaining_ch = list(set(range(d)) - set(sampled_ch))\n",
        "        B[np.ix_(remaining_pa, remaining_ch)] = 0\n",
        "        after_matching_edge_num = np.sum(B == 1)\n",
        "\n",
        "        # delta = total_edge_num - after_matching_edge_num\n",
        "        # mask B\n",
        "        maskedB = B + np.tril(np.ones((d, d)))\n",
        "        maskedB[np.ix_(remaining_pa, remaining_ch)] = 1\n",
        "        B[maskedB == 0] = 1\n",
        "\n",
        "        remaining_ch_set = set([i + d for i in remaining_ch])\n",
        "        sampled_ch_set = set([i + d for i in sampled_ch])\n",
        "        remaining_pa_set = set(remaining_pa)\n",
        "        sampled_pa_set = set(sampled_pa)\n",
        "\n",
        "        edges = np.transpose(np.nonzero(B))\n",
        "        edges[:, 1] += d\n",
        "        bigraph = nx.Graph()\n",
        "        bigraph.add_nodes_from(range(2 * d))\n",
        "        bigraph.add_edges_from(edges)\n",
        "        M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "        while len(M) > 2 * rank:\n",
        "            keys = set(M.keys())\n",
        "            rmv_cand = keys & (remaining_pa_set | remaining_ch_set)\n",
        "            p = sample(rmv_cand, 1)[0]\n",
        "            c = M[p]\n",
        "            # destroy p-c\n",
        "            bigraph.remove_edge(p, c)\n",
        "            M = nx.bipartite.maximum_matching(bigraph, top_nodes=range(d))\n",
        "\n",
        "        new_edges = np.array(bigraph.edges)\n",
        "        for i in range(len(new_edges)):\n",
        "            new_edges[i,].sort()\n",
        "        new_edges[:, 1] -= d\n",
        "\n",
        "        BB = np.zeros((d, d))\n",
        "        B = np.zeros((d, d))\n",
        "        BB[new_edges[:, 0], new_edges[:, 1]] = 1\n",
        "\n",
        "        if np.sum(BB == 1) > total_edge_num:\n",
        "            delta = total_edge_num - rank\n",
        "            BB[sampled_pa, sampled_ch] = 0\n",
        "            rmv_cand_edges = np.transpose(np.nonzero(BB))\n",
        "            if delta <= 0:\n",
        "                raise RuntimeError(r'Number of edges is below the rank, please \\\n",
        "                                   set a larger edge or degree \\\n",
        "                                   (you can change seed or increase degree).')\n",
        "            selected = np.array(sample(rmv_cand_edges.tolist(), delta))\n",
        "            B[selected[:, 0], selected[:, 1]] = 1\n",
        "            B[sampled_pa, sampled_ch] = 1\n",
        "        else:\n",
        "            B = deepcopy(BB)\n",
        "\n",
        "        B = B.transpose()\n",
        "        return B\n",
        "\n",
        "    @staticmethod\n",
        "    def erdos_renyi(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Erdos-Renyi\n",
        "        creation_prob = (2 * n_edges) / (n_nodes ** 2)\n",
        "        G_und = nx.erdos_renyi_graph(n=n_nodes, p=creation_prob, seed=seed)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_free(n_nodes, n_edges, weight_range=None, seed=None):\n",
        "\n",
        "        assert (n_nodes > 0 and n_edges >= n_nodes and n_edges < n_nodes * n_nodes)\n",
        "        set_random_seed(seed)\n",
        "        # Scale-free, Barabasi-Albert\n",
        "        m = int(round(n_edges / n_nodes))\n",
        "        G_und = nx.barabasi_albert_graph(n=n_nodes, m=m)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def bipartite(n_nodes, n_edges, split_ratio = 0.2, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        # Bipartite, Sec 4.1 of (Gu, Fu, Zhou, 2018)\n",
        "        n_top = int(split_ratio * n_nodes)\n",
        "        n_bottom = n_nodes -  n_top\n",
        "        creation_prob = n_edges/(n_top*n_bottom)\n",
        "        G_und = bipartite.random_graph(n_top, n_bottom, p=creation_prob, directed=True)\n",
        "        B_und = DAG._graph_to_adjmat(G_und)\n",
        "        B = DAG._random_acyclic_orientation(B_und)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def hierarchical(n_nodes, degree=5, graph_level=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 1\n",
        "        set_random_seed(seed)\n",
        "        prob = float(degree) / (n_nodes - 1)\n",
        "        B = np.tril((np.random.rand(n_nodes, n_nodes) < prob).astype(float), k=-1)\n",
        "        point = sample(range(n_nodes - 1), graph_level - 1)\n",
        "        point.sort()\n",
        "        point = [0] + [x + 1 for x in point] + [n_nodes]\n",
        "        for i in range(graph_level):\n",
        "            B[point[i]:point[i + 1], point[i]:point[i + 1]] = 0\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "    @staticmethod\n",
        "    def low_rank(n_nodes, degree=1, rank=5, weight_range=None, seed=None):\n",
        "\n",
        "        assert n_nodes > 0\n",
        "        set_random_seed(seed)\n",
        "        B = DAG._low_rank_dag(n_nodes, degree, rank)\n",
        "        if weight_range is None:\n",
        "            return B\n",
        "        else:\n",
        "            W = DAG._BtoW(B, n_nodes, weight_range)\n",
        "        return W\n",
        "\n",
        "\n",
        "class IIDSimulation(object):\n",
        "    '''\n",
        "    Simulate IID datasets for causal structure learning.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    W: np.ndarray\n",
        "        Weighted adjacency matrix for the target causal graph.\n",
        "    n: int\n",
        "        Number of samples for standard trainning dataset.\n",
        "    method: str, (linear or nonlinear), default='linear'\n",
        "        Distribution for standard trainning dataset.\n",
        "    sem_type: str\n",
        "        gauss, exp, gumbel, uniform, logistic (linear);\n",
        "        mlp, mim, gp, gp-add, quadratic (nonlinear).\n",
        "    noise_scale: float\n",
        "        Scale parameter of noise distribution in linear SEM.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, W, n=1000, method='linear',\n",
        "                 sem_type='gauss', noise_scale=1.0):\n",
        "\n",
        "        self.B = (W != 0).astype(int)\n",
        "        if method == 'linear':\n",
        "            self.X = IIDSimulation._simulate_linear_sem(\n",
        "                    W, n, sem_type, noise_scale)\n",
        "        elif method == 'nonlinear':\n",
        "            self.X = IIDSimulation._simulate_nonlinear_sem(\n",
        "                    W, n, sem_type, noise_scale)\n",
        "        logging.info('Finished synthetic dataset')\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_linear_sem(W, n, sem_type, noise_scale):\n",
        "        \"\"\"\n",
        "        Simulate samples from linear SEM with specified type of noise.\n",
        "        For uniform, noise z ~ uniform(-a, a), where a = noise_scale.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        W: np.ndarray\n",
        "            [d, d] weighted adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples, n=inf mimics population risk.\n",
        "        sem_type: str\n",
        "            gauss, exp, gumbel, uniform, logistic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        X: np.ndarray\n",
        "            [n, d] sample matrix, [d, d] if n=inf\n",
        "        \"\"\"\n",
        "        def _simulate_single_equation(X, w, scale):\n",
        "            \"\"\"X: [n, num of parents], w: [num of parents], x: [n]\"\"\"\n",
        "            if sem_type == 'gauss':\n",
        "                z = np.random.normal(scale=scale, size=n)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'exp':\n",
        "                z = np.random.exponential(scale=scale, size=n)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'gumbel':\n",
        "                z = np.random.gumbel(scale=scale, size=n)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'uniform':\n",
        "                z = np.random.uniform(low=-scale, high=scale, size=n)\n",
        "                x = X @ w + z\n",
        "            elif sem_type == 'logistic':\n",
        "                x = np.random.binomial(1, sigmoid(X @ w)) * 1.0\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a linear model, \\\n",
        "                                 the options are as follows: gauss, exp, \\\n",
        "                                 gumbel, uniform, logistic.')\n",
        "            return x\n",
        "\n",
        "        d = W.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "        G_nx =  nx.from_numpy_array(W, create_using=nx.DiGraph)\n",
        "        if not nx.is_directed_acyclic_graph(G_nx):\n",
        "            raise ValueError('W must be a DAG')\n",
        "        if np.isinf(n):  # population risk for linear gauss SEM\n",
        "            if sem_type == 'gauss':\n",
        "                # make 1/d X'X = true cov\n",
        "                X = np.sqrt(d) * np.diag(scale_vec) @ np.linalg.inv(np.eye(d) - W)\n",
        "                return X\n",
        "            else:\n",
        "                raise ValueError('population risk not available')\n",
        "        # empirical risk\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        X = np.zeros([n, d])\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], W[parents, j], scale_vec[j])\n",
        "        return X\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_nonlinear_sem(W, n, sem_type, noise_scale):\n",
        "        \"\"\"\n",
        "        Simulate samples from nonlinear SEM.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        B: np.ndarray\n",
        "            [d, d] binary adj matrix of DAG.\n",
        "        n: int\n",
        "            Number of samples.\n",
        "        sem_type: str\n",
        "            mlp, mim, gp, gp-add, or quadratic.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        X: np.ndarray\n",
        "            [n, d] sample matrix\n",
        "        \"\"\"\n",
        "        if sem_type == 'quadratic':\n",
        "            return IIDSimulation._simulate_quad_sem(W, n, noise_scale)\n",
        "\n",
        "        def _simulate_single_equation(X, scale):\n",
        "            \"\"\"X: [n, num of parents], x: [n]\"\"\"\n",
        "            z = np.random.normal(scale=scale, size=n)\n",
        "            pa_size = X.shape[1]\n",
        "            if pa_size == 0:\n",
        "                return z\n",
        "            if sem_type == 'mlp':\n",
        "                hidden = 100\n",
        "                W1 = np.random.uniform(low=0.5, high=2.0, size=[pa_size, hidden])\n",
        "                W1[np.random.rand(*W1.shape) < 0.5] *= -1\n",
        "                W2 = np.random.uniform(low=0.5, high=2.0, size=hidden)\n",
        "                W2[np.random.rand(hidden) < 0.5] *= -1\n",
        "                x = sigmoid(X @ W1) @ W2 + z\n",
        "            elif sem_type == 'mim':\n",
        "                w1 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w1[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w2 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w2[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                w3 = np.random.uniform(low=0.5, high=2.0, size=pa_size)\n",
        "                w3[np.random.rand(pa_size) < 0.5] *= -1\n",
        "                x = np.tanh(X @ w1) + np.cos(X @ w2) + np.sin(X @ w3) + z\n",
        "            elif sem_type == 'gp':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = gp.sample_y(X, random_state=None).flatten() + z\n",
        "            elif sem_type == 'gp-add':\n",
        "                from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "                gp = GaussianProcessRegressor()\n",
        "                x = sum([gp.sample_y(X[:, i, None], random_state=None).flatten()\n",
        "                        for i in range(X.shape[1])]) + z\n",
        "            else:\n",
        "                raise ValueError('Unknown sem type. In a nonlinear model, \\\n",
        "                                 the options are as follows: mlp, mim, \\\n",
        "                                 gp, gp-add, or quadratic.')\n",
        "            return x\n",
        "\n",
        "        B = (W != 0).astype(int)\n",
        "        d = B.shape[0]\n",
        "        if noise_scale is None:\n",
        "            scale_vec = np.ones(d)\n",
        "        elif np.isscalar(noise_scale):\n",
        "            scale_vec = noise_scale * np.ones(d)\n",
        "        else:\n",
        "            if len(noise_scale) != d:\n",
        "                raise ValueError('noise scale must be a scalar or has length d')\n",
        "            scale_vec = noise_scale\n",
        "\n",
        "        X = np.zeros([n, d])\n",
        "        G_nx =  nx.from_numpy_array(B, create_using=nx.DiGraph)\n",
        "        ordered_vertices = list(nx.topological_sort(G_nx))\n",
        "        assert len(ordered_vertices) == d\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G_nx.predecessors(j))\n",
        "            X[:, j] = _simulate_single_equation(X[:, parents], scale_vec[j])\n",
        "        return X\n",
        "\n",
        "    @staticmethod\n",
        "    def _simulate_quad_sem(W, n, noise_scale):\n",
        "        \"\"\"\n",
        "        Simulate samples from SEM with specified type of noise.\n",
        "        Coefficient is randomly drawn but specifically designed\n",
        "        to avoid overflow issues.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        W: np.ndarray\n",
        "            weigthed DAG.\n",
        "        n: int\n",
        "            Number of samples.\n",
        "        noise_scale: float\n",
        "            Scale parameter of noise distribution in linear SEM.\n",
        "\n",
        "        Return\n",
        "        ------\n",
        "        X: np.ndarray\n",
        "            [n,d] sample matrix\n",
        "        \"\"\"\n",
        "        def generate_quadratic_coef(random_zero=True):\n",
        "            if random_zero and np.random.randint(low=0, high=2):\n",
        "                return 0\n",
        "            else:\n",
        "                coef = np.random.uniform(low=0.5, high=1)\n",
        "                if np.random.randint(low=0, high=2):\n",
        "                    coef *= -1\n",
        "                return coef\n",
        "\n",
        "        G = nx.DiGraph(W)\n",
        "        d = W.shape[0]\n",
        "        X = np.zeros([n, d])\n",
        "        ordered_vertices = list(nx.topological_sort(G))\n",
        "        assert len(ordered_vertices) == d\n",
        "        for j in ordered_vertices:\n",
        "            parents = list(G.predecessors(j))\n",
        "\n",
        "            if len(parents) == 0:\n",
        "                eta = np.zeros([n])\n",
        "            elif len(parents) == 1:\n",
        "                # We don't generate random zero coefficient if there is only one parent\n",
        "                eta = np.zeros([n])\n",
        "                used_parents = set()\n",
        "                p = parents[0]\n",
        "                num_terms = 0\n",
        "\n",
        "                # Linear term\n",
        "                coef = generate_quadratic_coef(random_zero=False)\n",
        "                if coef != 0:\n",
        "                    eta += coef * X[:, p]\n",
        "                    used_parents.add(p)\n",
        "                    num_terms += 1\n",
        "\n",
        "                # Squared term\n",
        "                coef = generate_quadratic_coef(random_zero=False)\n",
        "                if coef != 0:\n",
        "                    eta += coef * np.square(X[:, p])\n",
        "                    used_parents.add(p)\n",
        "                    num_terms += 1\n",
        "\n",
        "                if num_terms > 0:\n",
        "                    eta /= num_terms    # Compute average\n",
        "\n",
        "                # Remove parent if both coef is zero\n",
        "                if p not in used_parents:\n",
        "                    W[p, j] = 0\n",
        "            else:    # More than 1 parent\n",
        "                eta = np.zeros([n])\n",
        "                used_parents = set()\n",
        "                num_terms = 0\n",
        "\n",
        "                for p in parents:\n",
        "                    # Linear terms\n",
        "                    coef = generate_quadratic_coef(random_zero=True)\n",
        "                    if coef > 0:\n",
        "                        eta += coef * X[:, p]\n",
        "                        used_parents.add(p)\n",
        "                        num_terms += 1\n",
        "\n",
        "                    # Squared terms\n",
        "                    coef = generate_quadratic_coef(random_zero=True)\n",
        "                    if coef > 0:\n",
        "                        eta += coef * np.square(X[:, p])\n",
        "                        used_parents.add(p)\n",
        "                        num_terms += 1\n",
        "\n",
        "                # Cross terms\n",
        "                for p1, p2 in combinations(parents, 2):\n",
        "                    coef = generate_quadratic_coef(random_zero=True)\n",
        "                    if coef > 0:\n",
        "                        eta += coef * X[:, p1] * X[:, p2]\n",
        "                        used_parents.add(p1)\n",
        "                        used_parents.add(p2)\n",
        "                        num_terms += 1\n",
        "\n",
        "                if num_terms > 0:\n",
        "                    eta /= num_terms    # Compute average\n",
        "\n",
        "                # Remove parent if both coef is zero\n",
        "                unused_parents = set(parents) - used_parents\n",
        "                if p in unused_parents:\n",
        "                    W[p, j] = 0\n",
        "\n",
        "            X[:, j] = eta + np.random.normal(scale=noise_scale, size=n)\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class Topology(object):\n",
        "    \"\"\"\n",
        "    A class for generating some classical (undirected) network structures,\n",
        "    in which any graph generator method would return the adjacency matrix of\n",
        "    a network structure.\n",
        "    In fact, we recommend to directly use the python package \"NetworkX\"\n",
        "    to create various structures you need.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def erdos_renyi(n_nodes, n_edges, seed=None):\n",
        "        \"\"\"\n",
        "        Generate topology matrix\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes : int, greater than 0\n",
        "            The number of nodes.\n",
        "        n_edges : int, greater than 0\n",
        "            Use to calculate probability for edge creation.\n",
        "        seed : integer, random_state, or None (default)\n",
        "            Indicator of random number generation state.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        B: np.matrix\n",
        "        \"\"\"\n",
        "        assert n_nodes > 0, 'The number of nodes must be greater than 0.'\n",
        "        creation_prob = (2*n_edges)/(n_nodes**2)\n",
        "        G = nx.erdos_renyi_graph(n=n_nodes, p=creation_prob, seed=seed)\n",
        "        B = nx.to_numpy_array(G)\n",
        "        return B\n",
        "\n",
        "\n",
        "class THPSimulation(object):\n",
        "    \"\"\"\n",
        "    A class for simulating event sequences with\n",
        "    THP (Topological Hawkes Process) setting.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    causal_matrix: np.matrix\n",
        "        The casual matrix.\n",
        "    topology_matrix: np.matrix\n",
        "        Interpreted as an adjacency matrix to generate graph.\n",
        "        Has two dimension, should be square.\n",
        "    mu_range: tuple, default=(0.00005, 0.0001)\n",
        "    alpha_range: tuple, default=(0.005, 0.007)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, causal_matrix, topology_matrix,\n",
        "                 mu_range=(0.00005, 0.0001), alpha_range=(0.005, 0.007)):\n",
        "\n",
        "        assert (isinstance(causal_matrix, np.ndarray) and\n",
        "                causal_matrix.ndim == 2 and\n",
        "                causal_matrix.shape[0] == causal_matrix.shape[1]),\\\n",
        "            'casual_matrix should be np.matrix object, two dimension, square.'\n",
        "        assert (isinstance(topology_matrix, np.ndarray) and\n",
        "                topology_matrix.ndim == 2 and\n",
        "                topology_matrix.shape[0] == topology_matrix.shape[1]),\\\n",
        "            'topology_matrix should be np.matrix object, two dimension, square.'\n",
        "\n",
        "        self._causal_matrix = (causal_matrix != 0).astype(int)\n",
        "\n",
        "        self._topo = nx.from_numpy_array(topology_matrix,\n",
        "                                          create_using=nx.Graph)\n",
        "\n",
        "        self._mu_range = mu_range\n",
        "        self._alpha_range = alpha_range\n",
        "\n",
        "    def simulate(self, T, max_hop=1, beta=10):\n",
        "        \"\"\"\n",
        "        Generate simulation data.\n",
        "        \"\"\"\n",
        "        N = self._causal_matrix.shape[0]\n",
        "\n",
        "        mu = np.random.uniform(*self._mu_range, N)\n",
        "\n",
        "        alpha = np.random.uniform(*self._alpha_range, [N, N])\n",
        "        alpha = alpha * self._causal_matrix\n",
        "        alpha = np.ones([max_hop+1, N, N]) * alpha\n",
        "\n",
        "        immigrant_events = dict()\n",
        "        for node in self._topo.nodes:\n",
        "            immigrant_events[node] = self._trigger_events(mu, 0, T, beta)\n",
        "\n",
        "        base_events = immigrant_events.copy()\n",
        "        events = immigrant_events.copy()\n",
        "        while sum(map(len, base_events.values())) != 0:\n",
        "            offspring_events = dict()\n",
        "            for node in tqdm(self._topo.nodes):\n",
        "                offspring_events[node] = []\n",
        "                for k in range(max_hop+1):\n",
        "                    k_base_events = []\n",
        "                    for neighbor in self._get_k_hop_neighbors(\n",
        "                            self._topo, node, k):\n",
        "                        k_base_events += base_events[neighbor]\n",
        "                    k_new_events = [self._trigger_events(\n",
        "                        alpha[k, i], start_time, duration, beta)\n",
        "                        for (i, start_time, duration) in k_base_events]\n",
        "                    for event_group in k_new_events:\n",
        "                        offspring_events[node] += event_group\n",
        "                events[node] += offspring_events[node]\n",
        "            base_events = offspring_events\n",
        "\n",
        "        Xn_list = []\n",
        "        for node, event_group in events.items():\n",
        "            Xn = pd.DataFrame(event_group,\n",
        "                              columns=['event', 'timestamp', 'duration'])\n",
        "            Xn.insert(0, 'node', node)\n",
        "            Xn_list.append(Xn.reindex(columns=['event', 'timestamp', 'node']))\n",
        "        X = pd.concat(Xn_list, sort=False, ignore_index=True)\n",
        "        return X\n",
        "\n",
        "    @staticmethod\n",
        "    def _trigger_events(intensity_vec, start_time, duration, beta):\n",
        "\n",
        "        events = []\n",
        "        for i, intensity in enumerate(intensity_vec):\n",
        "            if intensity:\n",
        "                trigger_time = start_time\n",
        "                while True:\n",
        "                    trigger_time = round(trigger_time + np.random.exponential(\n",
        "                        1 / intensity))\n",
        "                    if trigger_time > start_time + duration:\n",
        "                        break\n",
        "                    sub_duration = (np.max((0, np.random.exponential(beta)))).round()\n",
        "                    events.append((i, trigger_time, sub_duration))\n",
        "        return events\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_k_hop_neighbors(G, node, k):\n",
        "        if k == 0:\n",
        "            return {node}\n",
        "        else:\n",
        "            return (set(nx.single_source_dijkstra_path_length(G, node, k).keys())\n",
        "                    - set(nx.single_source_dijkstra_path_length(\n",
        "                        G, node, k - 1).keys()))"
      ],
      "metadata": {
        "id": "J8KFenscDDwE"
      },
      "id": "J8KFenscDDwE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and plots_Step4\n",
        "* Test datasets\n"
      ],
      "metadata": {
        "id": "7onEpv2XDemx"
      },
      "id": "7onEpv2XDemx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilIw6Mrx-bAS"
      },
      "source": [
        "# ANM_NCPOP_All\n"
      ],
      "id": "ilIw6Mrx-bAS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "245i_O68pkoJ",
        "outputId": "ebec23bb-4eed-46e3-b0df-a12ff0ba10f2"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-21b45e8e6b46>\u001b[0m in \u001b[0;36m<cell line: 501>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mANM_NCPOP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFile_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Real'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAncpop_Test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: ANM_NCPOP.Ancpop_Test() missing 1 required positional argument: 'self'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from castle.common import GraphDAG\n",
        "from castle.metrics import MetricsDAG\n",
        "\n",
        "\n",
        "class ANM_NCPOP(object):\n",
        "    '''\n",
        "    A class for simulating random (causal) DAG from real or synthetic datasets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_type: str, (Real or Synthetic), default='Real'\n",
        "    file_name: str, name of data,\n",
        "        Setting file_name = 0 if data_type = 'Synthetic'\n",
        "    method: str, (linear or nonlinear), default='linear'\n",
        "        Distribution for standard trainning dataset.\n",
        "    File_PATH: str\n",
        "        Save file path\n",
        "    start: int\n",
        "        Start number of samples for standard trainning dataset\n",
        "    stop: int\n",
        "        stop number of samples for standard trainning dataset\n",
        "    step: int\n",
        "        step number of samples for standard trainning dataset\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, data_type, file_name, File_PATH, start, stop, step):\n",
        "        self.data_type = data_type\n",
        "        self.file_name = file_name\n",
        "        self.File_PATH = File_PATH\n",
        "        self.start = start\n",
        "        self.stop = stop\n",
        "        self.step = step\n",
        "\n",
        "    @staticmethod\n",
        "    def Ancpop_Test(self):\n",
        "        ##################################################### Read Dataset ##########################################\n",
        "        if self.data_type=='Real':\n",
        "            if not os.path.exists(self.File_PATH):\n",
        "                print('Dataset does not exist!')\n",
        "            td_results=os.listdir(self.File_PATH)\n",
        "            if len(td_results)==0:\n",
        "                print('INFO: Empty data file!')\n",
        "            else:\n",
        "                print('INFO: Reading Data in File!')\n",
        "                for file_name in td_results:\n",
        "                    rt = Ancpop_Real(self.File_PATH, self.file_name, self.start, self.stop, self.step)\n",
        "                    rt.Ancpop_real_Test()\n",
        "\n",
        "        else:\n",
        "            for method in ['linear','nonlinear']:\n",
        "                if method == 'linear':\n",
        "                    st = ['gauss', 'exp', 'gumbel', 'uniform', 'logistic']\n",
        "                else:\n",
        "                    st = ['mlp', 'mim','gp', 'gp-add', 'quadratic']\n",
        "                for sem_type in st:\n",
        "                    nodes = range(6,14,3)\n",
        "                    edges = range(10,21,5)\n",
        "                    st = Ancpop_Simulation(method, self.File_PATH, sem_type, nodes, edges, self.start, self.stop, self.step)\n",
        "                    st.Ancpop_simulation_Test()\n",
        "\n",
        "class Ancpop_Real(object):\n",
        "    '''\n",
        "    A class for simulating random (causal) DAG, where any DAG generator\n",
        "    self.method would return the weighed/binary adjacency matrix of a DAG.\n",
        "    Besides, we recommend using the python package \"NetworkX\"\n",
        "    to create more structures types.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    File_PATH: str\n",
        "        Save file path\n",
        "    File_NAME: str\n",
        "        Read data name\n",
        "    start: int\n",
        "        Start number of samples for standard trainning dataset\n",
        "    stop: int\n",
        "        stop number of samples for standard trainning dataset\n",
        "    step: int\n",
        "        step number of samples for standard trainning dataset\n",
        "\n",
        "    '''\n",
        "    def __init__(self, File_PATH,File_NAME, start, stop, step):\n",
        "        self.File_PATH = File_PATH\n",
        "        self.File_NAME = File_NAME\n",
        "        self.start = start\n",
        "        self.stop = stop\n",
        "        self.step = step\n",
        "        self.datasize = range(self.start, self.stop, self.step)\n",
        "        self.datasize_num = len(self.datasize)\n",
        "        self.sname = re.split(\"\\.\", self.File_NAME)[0]\n",
        "\n",
        "\n",
        "    def Ancpop_real_Test(self):\n",
        "        ################################################  Test Data #############################################\n",
        "        self.File_PATH_Heatmaps = self.File_PATH + 'Results_'+self.sname+ '/'\n",
        "        self.Table_PATH_Summary = self.File_PATH_Heatmaps + 'Summary_'+self.sname+'.csv'\n",
        "        if not os.path.exists(self.Table_PATH_Summary):\n",
        "            print('INFO: Testing '+ self.sname+'!')\n",
        "            self.File_PATH_Base = self.File_PATH + 'Details_'+self.sname+ '/'\n",
        "            self.Ancpop_real_estimate(self)\n",
        "        else:\n",
        "            print('INFO: Finished '+ self.sname+'Sampling!')\n",
        "\n",
        "        ################################################  Create And Plot #############################################\n",
        "        #self.Plots_ANCPOP_Real(self)\n",
        "        print('INFO: Finished plotting '+ self.sname + '!')\n",
        "\n",
        "    @staticmethod\n",
        "    def Ancpop_real_estimate(self):\n",
        "        read_Dir=os.listdir(self.File_PATH)\n",
        "        if len(read_Dir) == 1:\n",
        "            self.File_TYPE = re.split(\"\\.\", self.File_NAME)[1]\n",
        "            if self.File_TYPE =='npz':\n",
        "                print(self.File_TYPE)\n",
        "                Tests_data = np.load(self.File_PATH+self.File_NAME, allow_pickle=True)\n",
        "                Raw_data = Tests_data['x']\n",
        "                true_dag = Tests_data['y']\n",
        "                #print(Tests_data['x'][:20], Raw_data[:20],true_dag)\n",
        "                #self.nodes_num = len(causal_matrix)\n",
        "                #self.edges_num = len(causal_matrix[causal_matrix == 1])\n",
        "            elif self.File_TYPE =='tar':\n",
        "                Tests_data = np.load(self.File_NAME+'.tar', allow_pickle=True)\n",
        "            else:\n",
        "                print('INFO: Cannot Read '+self.File_TYPE+' Data!')\n",
        "        else:\n",
        "            Raw_data_path = self.File_PATH+read_Dir[0]\n",
        "            Raw_data = pd.read_csv(Raw_data_path, header=0, index_col=0)\n",
        "            DAG_data_path = self.File_PATH+read_Dir[1]\n",
        "            DAG_data = pd.read_csv(DAG_data_path, header=0, index_col=0)\n",
        "\n",
        "\n",
        "        ############################################## Create Files ###################################\n",
        "        if not os.path.exists(self.File_PATH_Base):\n",
        "            os.makedirs(self.File_PATH_Base)\n",
        "        self.File_PATH_MetricsDAG = self.File_PATH_Base +'MetricsDAG/'\n",
        "        if not os.path.exists(self.File_PATH_MetricsDAG):\n",
        "            os.makedirs(self.File_PATH_MetricsDAG)\n",
        "        if not os.path.exists(self.File_PATH_Heatmaps):\n",
        "            os.makedirs(self.File_PATH_Heatmaps)\n",
        "\n",
        "        duration_anm_ncpop = []\n",
        "        f1_anm_ncpop = []\n",
        "        df = pd.DataFrame(columns=['fdr', 'tpr', 'fpr', 'shd', 'nnz', 'precision', 'recall', 'F1', 'gscore'])\n",
        "\n",
        "        #df = pd.DataFrame(columns=['DataSize','False_Discovery_Rate', 'True_Positive_Rate', 'False_Positive_Rate', 'SHD', 'NNZ', 'Precision', 'Recall', 'F1_Score', 'G_score'])\n",
        "        for i in self.datasize:\n",
        "            data = Raw_data[:i]\n",
        "            t_start = time.time()\n",
        "            anmNCPO = ANM_NCPO(alpha=0.05)\n",
        "            anmNCPO.learn(data = data, causalmodelling = 'hidden_state1')\n",
        "            # plot predict_dag and true_dag\n",
        "            GraphDAG(anmNCPO.causal_matrix, true_dag, show=False, save_name = self.File_PATH_MetricsDAG+self.sname+'_' + str(i) + 'Datasize.png')\n",
        "            met = MetricsDAG(anmNCPO.causal_matrix, true_dag)\n",
        "            #f1_result.to_csv(self.File_PATH_Heatmaps + 'F1_'+self.sname+'.csv',index=False)\n",
        "            if math.isnan(float(met.metrics['F1'])):\n",
        "              f1_anm_ncpop.append(0.2)\n",
        "            else:\n",
        "              print(math.isnan(met.metrics['F1']),met.metrics['F1']==np.nan)\n",
        "              f1_anm_ncpop.append(met.metrics['F1'])\n",
        "            duration_anm_ncpop.append(time.time()-t_start)\n",
        "            print(self.sname+'_' + str(i) + 'Datasize is done!'+'F1 Score is'+ str(met.metrics['F1'])+'.')\n",
        "            #'Time Duration is'+ str(time.time()-t_start))\n",
        "            df = pd.concat([df, pd.DataFrame([met.metrics])])\n",
        "        df.to_csv(self.Table_PATH_Summary, index=False)\n",
        "        #df = pd.concat({\"DataSize\":[self.datasize]},{df.loc[:, ['F1']]})\n",
        "        df_F1 = pd.DataFrame({\"DataSize\":self.datasize,'F1_Score':f1_anm_ncpop,'Duration':duration_anm_ncpop})\n",
        "        #f1_result = df.loc[:, ['DataSize','F1_Score']]\n",
        "        df_F1.to_csv(self.File_PATH_Heatmaps + 'F1_'+self.sname+'.csv',index=False)\n",
        "        return df_F1\n",
        "\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def Summary_Results(self):\n",
        "        f1_anm_ncpop = pd.DataFrame()\n",
        "        tqdm=os.listdir(self.File_PATH_Details)\n",
        "        for i in range(0,len(tqdm)):\n",
        "            File_PATH = os.path.join(self.File_PATH_Details,tqdm[i])\n",
        "            #ds = re.split(\"D\",re.split(\"edges_\",re.split(\"nodes_\", re.split(\"\\.\", tqdm[i])[0])[1])[1])[0]\n",
        "            df = pd.read_csv(File_PATH)\n",
        "            f1_anm_ncpop_nan = df.loc[:,'F1']\n",
        "            if len([f1_anm_ncpop_nan == 0]) == len(f1_anm_ncpop_nan):\n",
        "              f1_anm_ncpop_mean = 0.2\n",
        "            else:\n",
        "              f1_anm_ncpop_mean = round(np.nanmean(f1_anm_ncpop_nan), 3)\n",
        "            f1_anm_ncpop = pd.concat((f1_anm_ncpop, pd.DataFrame([ds, f1_anm_ncpop_mean])), axis=1)\n",
        "        f1_result = pd.DataFrame(np.array(f1_anm_ncpop.T), columns=['Linear','Gauss','Nodes','Edges','DataSize','F1_Score'])\n",
        "        f1_result.to_csv(self.Table_PATH_Summary,index=False)\n",
        "        return f1_result'''\n",
        "\n",
        "    @staticmethod\n",
        "    def Plots_ANCPOP_Real(self):\n",
        "        self.pro_rang = np.arange(self.start, self.stop, self.step)\n",
        "        self.obs_rang = np.arange(self.start, self.stop, self.step)\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(20,20))\n",
        "        z =[]\n",
        "        zz = pd.read_csv(self.File_PATH_Heatmaps + 'F1_'+self.sname+'.csv', header=[1], index_col=0)\n",
        "        z = zz.to_numpy()\n",
        "        z_min=np.min(z)\n",
        "        z_max=np.max(z)\n",
        "        c = axes.imshow(z, cmap =plt.cm.bone_r, vmin = z_min, vmax = z_max,\n",
        "                        interpolation ='nearest', origin ='upper')\n",
        "        axes.set_xlabel('length of time windows '+r'$T$',fontsize=10)\n",
        "        #axes.set_ylabel('F1 score ',fontsize=10)\n",
        "        positions = range(9)\n",
        "        labels=self.datasize\n",
        "        axes.yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
        "        axes.yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
        "        for ytick in axes.get_yticklabels():\n",
        "            ytick.set_fontsize(4)\n",
        "        axes.xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
        "        axes.xaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
        "        for xtick in axes.get_xticklabels():\n",
        "            xtick.set_fontsize(4)\n",
        "\n",
        "        fig.colorbar(c, ax=axes.ravel().tolist())\n",
        "        plt.savefig(self.File_PATH_Heatmaps +'Heatmap_ '+self.sname+'.pdf', bbox_inches='tight')\n",
        "\n",
        "\n",
        "class Ancpop_Simulation(object):\n",
        "    '''\n",
        "    A class for simulating random (causal) DAG, where any DAG generator\n",
        "    self.method would return the weighed/binary adjacency matrix of a DAG.\n",
        "    Besides, we recommend using the python package \"NetworkX\"\n",
        "    to create more structures types.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    method: str, (linear or nonlinear), default='linear'\n",
        "        Distribution for standard trainning dataset.\n",
        "    File_PATH_Base: str\n",
        "        Save file path\n",
        "    sem_type: str\n",
        "        gauss, exp, gumbel, uniform, logistic (linear);\n",
        "        mlp, mim, gp, gp-add, quadratic (nonlinear).\n",
        "    nodes: series\n",
        "        Notes of samples for standard trainning dataset\n",
        "    edges: series\n",
        "        Edges of samples for standard trainning dataset\n",
        "    start: int\n",
        "        Start number of samples for standard trainning dataset\n",
        "    stop: int\n",
        "        stop number of samples for standard trainning dataset\n",
        "    step: int\n",
        "        step number of samples for standard trainning dataset\n",
        "\n",
        "    '''\n",
        "\n",
        "    def __init__(self, method, File_PATH, sem_type, nodes, edges, start, stop, step):\n",
        "        self.method = method\n",
        "        self.File_PATH = File_PATH\n",
        "        self.printname = self.method.capitalize()+' SEM Samples with ' + self.sem_type.capitalize() +' Noise'\n",
        "        self.filename = self.method.capitalize()+'SEM_' + self.sem_type.capitalize() +'Noise'\n",
        "        self.File_PATH_Base = self.File_PATH +'Result_'+ self.filename +'/'\n",
        "        self.sem_type = sem_type\n",
        "        self.nodes = nodes\n",
        "        self.edges = edges\n",
        "        self.start = start\n",
        "        self.stop = stop\n",
        "        self.step = step\n",
        "        self.nodes_num = len(self.nodes)\n",
        "        self.edges_num = len(self.edges)\n",
        "        self.pro_rang = np.arange(self.start, self.stop, self.step)\n",
        "        self.obs_rang = np.arange(self.start, self.stop, self.step)\n",
        "        self.datasize = range(self.start, self.stop, self.step)\n",
        "        self.datasize_num = len(self.datasize)\n",
        "\n",
        "    @staticmethod\n",
        "    def Ancpop_simulation_Test(self):\n",
        "        ############################################## Create And Download Dataset ###################################\n",
        "        self.File_PATH_Details = self.File_PATH_Base + 'Results_Details/'\n",
        "        if not os.path.exists(self.File_PATH_Details):\n",
        "            os.makedirs(self.File_PATH_Details)\n",
        "        print('INFO: Created Result_'+self.method.capitalize()+' File!')\n",
        "        tqdm_csv=os.listdir(self.File_PATH_Details)\n",
        "        if len(tqdm_csv) != self.nodes_num*self.edges_num* self.datasize_num:\n",
        "            print('INFO: Simulating '+self.printname +'!')\n",
        "            self.Ancpop_simulation_estimate(self, sem_type)\n",
        "        else:\n",
        "            print('INFO: Finished '+ self.printname+' simulation!')\n",
        "\n",
        "        ################################################# Summary Dataset ############################################\n",
        "        ################################## Insert/Download Dataset And Create Summary Table############################\n",
        "        self.File_PATH_Heatmaps = self.File_PATH + 'Results_'+self.filename+ '/'\n",
        "        if not os.path.exists(self.File_PATH_Heatmaps):\n",
        "            os.makedirs(self.File_PATH_Heatmaps )\n",
        "        self.Table_PATH_Summary = self.File_PATH_Heatmaps + 'Summary_'+self.filename+'.csv'\n",
        "        if not os.path.exists(self.Table_PATH_Summary):\n",
        "            print('INFO: Summarizing samples from '+'!')\n",
        "            self.Summary_Results(self)\n",
        "            # Table_Summary = self.Summary_Results()\n",
        "\n",
        "        else:\n",
        "            print('INFO: Finished '+ self.printname+' summary!')\n",
        "            # Table_Summary = pd.read_csv(Table_PATH_Summary,header=0,index_col=0)\n",
        "\n",
        "        ######################################### Create And Combine Dataset Summary #################################\n",
        "        self.File_PATH_Results = self.File_PATH_Base + 'Results/'\n",
        "        if not os.path.exists(self.File_PATH_Results):\n",
        "            os.makedirs(self.File_PATH_Results)\n",
        "        tqdm_results=os.listdir(self.File_PATH_Results)\n",
        "        if len(tqdm_results) != (self.nodes_num+self.edges_num)*2:\n",
        "            print('INFO: Preparing plot!')\n",
        "            self.Type_Results(self, 'Nodes')\n",
        "            self.Type_Results(self, 'Edges')\n",
        "\n",
        "        ################################################  Create And Plot #############################################\n",
        "        self.Plots_Type_ANCPOP(self,'Nodes')\n",
        "        print('INFO: Finished plotting '+ self.printname + ' on nodes!')\n",
        "\n",
        "        self.Plots_Type_ANCPOP(self,'Edges')\n",
        "        print('INFO: Finished plotting '+ self.printname + ' on edges!')\n",
        "\n",
        "        self.Plots_ANCPOP(self)\n",
        "        print('INFO: Finished plotting '+ self.printname + '!')\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def Ancpop_simulation_estimate(self, sem_type):\n",
        "        duration_anm_ncpop = []\n",
        "        f1_anm_ncpop = []\n",
        "        df = pd.DataFrame(columns=['fdr', 'tpr', 'fpr', 'shd', 'nnz', 'precision', 'recall', 'F1', 'gscore'])\n",
        "        for nn in range(6,14,3):\n",
        "            for ne in range(10,21,5):\n",
        "                weighted_random_dag = DAG.erdos_renyi(n_nodes=nn, n_edges=ne, seed=1)\n",
        "                for ds in range(5,40,5):\n",
        "                    sname = self.method+ '_gauss_'+str(nn)+'nodes_'+str(ne)+'edges_'+str(ds)+'DataSize'\n",
        "                    path_check = os.path.exists(self.File_PATH_Base + 'Results_Details/' +sname+'.csv')\n",
        "                    if not path_check:\n",
        "                        dataset = IIDSimulation(W=weighted_random_dag, n=ds, method=self.method, sem_type=sem_type)\n",
        "                        true_dag, data = dataset.B, dataset.X\n",
        "\n",
        "                        t_start = time.time()\n",
        "                        anmNCPO = ANM_NCPO(alpha=0.05)\n",
        "                        anmNCPO.learn(data=data,causalmodelling='hidden_state1')\n",
        "\n",
        "                        # plot predict_dag and true_dag\n",
        "                        self.File_PATH_MetricsDAG = self.File_PATH_Base +'MetricsDAG/'\n",
        "                        if not os.path.exists(self.File_PATH_MetricsDAG):\n",
        "                          os.makedirs(self.File_PATH_MetricsDAG)\n",
        "                        GraphDAG(anmNCPO.causal_matrix, true_dag, show=False, save_name = self.File_PATH_MetricsDAG +'Result_' +sname)\n",
        "                        met = MetricsDAG(anmNCPO.causal_matrix, true_dag)\n",
        "                        duration_anm_ncpop.append(time.time()-t_start)\n",
        "                        f1_anm_ncpop.append(met.metrics['F1'])\n",
        "                        print(sname+ 'is done!'+'F1 Score is'+ str(met.metrics['F1'])+'.' 'Time Duration is'+ str(time.time()-t_start))\n",
        "                        #mm = pd.DataFrame(pd.DataFrame([mm]).append([met.metrics]).dropna(axis = 0, how ='any'))#.drop_duplicates(inplace= True)\n",
        "                        df = pd.concat([df, pd.DataFrame([met.metrics])])\n",
        "                        df.to_csv(self.File_PATH_Details +sname+'.csv', index=False)\n",
        "                df1=df.assign(DataSize = ds)\n",
        "                df1.to_csv(self.File_PATH_Base + 'summary_' + self.method+ '_gauss_'+str(nn)+'nodes_'+str(ne)+'edges' +'.csv', index=False)\n",
        "        f1_anm_ncpop = np.array(f1_anm_ncpop)\n",
        "        np.savetxt(self.File_PATH_Base + 'Summary_F1_' +self.filename+'.csv', f1_anm_ncpop, delimiter=',')\n",
        "        #f1_anm_ncpop.to_csv(self.File_PATH_Base + 'Summary_F1_' + self.method +'_Gauss.csv', index=False)\n",
        "        np.savetxt(self.File_PATH_Base + 'Duration_' +self.filename+'.csv', duration_anm_ncpop, delimiter=',')\n",
        "        #duration_anm_ncpop.to_csv(self.File_PATH_Base + 'Duration_' + self.method + '_Gauss.csv', index=False)\n",
        "        #f1_if_lds_mean = np.mean(f1_if_lds, axis=1)\n",
        "        #f1_if_lds_std = np.std(f1_if_lds, axis=1)\n",
        "        print(df1,f1_anm_ncpop, duration_anm_ncpop)\n",
        "\n",
        "    @staticmethod\n",
        "    def Summary_Results(self):\n",
        "        f1_anm_ncpop = pd.DataFrame()\n",
        "        tqdm=os.listdir(self.File_PATH_Details)\n",
        "        for i in range(0,len(tqdm)):\n",
        "            File_PATH = os.path.join(self.File_PATH_Details,tqdm[i])\n",
        "            #entries = re.split(\"_\", re.split(\"\\.\", tqdm[i])[0])\n",
        "            self.method_nn = re.split(\"_\",re.split(\"nodes_\", re.split(\"\\.\", tqdm[i])[0])[0])\n",
        "            ne = re.split(\"edges_\",re.split(\"nodes_\", re.split(\"\\.\", tqdm[i])[0])[1])[0]\n",
        "            ds = re.split(\"D\",re.split(\"edges_\",re.split(\"nodes_\", re.split(\"\\.\", tqdm[i])[0])[1])[1])[0]\n",
        "            df = pd.read_csv(File_PATH)\n",
        "            f1_anm_ncpop_nan = df.loc[:,'F1']\n",
        "            if len([f1_anm_ncpop_nan == 0]) == len(f1_anm_ncpop_nan):\n",
        "              f1_anm_ncpop_mean = 0.2\n",
        "            else:\n",
        "              f1_anm_ncpop_mean = round(np.nanmean(f1_anm_ncpop_nan), 3)\n",
        "            f1_anm_ncpop = pd.concat((f1_anm_ncpop, pd.DataFrame(self.method_nn+[ne, ds, f1_anm_ncpop_mean])), axis=1)\n",
        "        f1_result = pd.DataFrame(np.array(f1_anm_ncpop.T), columns=['Linear','Gauss','Nodes','Edges','DataSize','F1_Score'])\n",
        "        f1_result.to_csv(self.Table_PATH_Summary,index=False)\n",
        "        return f1_result\n",
        "\n",
        "    @staticmethod\n",
        "    def Type_Results(self, type):\n",
        "        if type == 'Nodes':\n",
        "            pivot = 'Edges'\n",
        "        else:\n",
        "            pivot = 'Nodes'\n",
        "        f1_result =pd.read_csv(self.Table_PATH_Summary, header=0, index_col=0)\n",
        "        group_obj = f1_result.groupby(type)#.agg('mean')\n",
        "        for i in group_obj:\n",
        "            print(i[0])\n",
        "            f1_anm_ncpop_ = i[1].pivot(index=pivot,columns='DataSize',values='F1_Score')\n",
        "            f1_anm_ncpop_result = f1_anm_ncpop_.reset_index()\n",
        "            #print(f1_anm_ncpop_result)\n",
        "            self.Table_PATH_Results = self.File_PATH_Results + 'Summary_'+type+'_'+str(i[0])\n",
        "            f1_anm_ncpop_result.to_csv(self.Table_PATH_Results+\".csv\",index=False)\n",
        "            np.save(self.Table_PATH_Results+\".npy\", f1_anm_ncpop_result)\n",
        "\n",
        "    @staticmethod\n",
        "    def Plots_Type_ANCPOP(self, type):\n",
        "        self.pro_rang = np.arange(self.start, self.stop, self.step)\n",
        "        self.obs_rang = np.arange(self.start, self.stop, self.step)\n",
        "        if type == 'Nodes':\n",
        "            type_num = self.nodes_num\n",
        "            size = self.nodes\n",
        "            labels = self.edges\n",
        "            yaxes = 'Number of Edges'\n",
        "        else:\n",
        "            type_num = self.edges_num\n",
        "            size = self.edges\n",
        "            labels = self.nodes\n",
        "            yaxes = 'Number of Variables'\n",
        "        z = [[] for i in range(type_num)]\n",
        "        pro_rang = self.pro_rang\n",
        "        obs_rang = self.obs_rang\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=type_num,figsize=(20,4))\n",
        "        for i in range(type_num):\n",
        "            read_path = self.File_PATH_Base + 'Results/Summary_'+ type +'_'+str(size[i]) +'.csv'\n",
        "            zz = pd.read_csv(read_path,header=0,index_col=0)\n",
        "            z[i] = zz.to_numpy()\n",
        "        z_min=np.min(z)\n",
        "        z_max=np.max(z)\n",
        "        for i in range(type_num):\n",
        "            c = axes[i].imshow(z[i], cmap =plt.cm.bone_r, vmin = z_min, vmax = z_max, interpolation ='nearest', origin ='upper')\n",
        "            axes[i].set_xlabel('length of time windows '+r'$T$',fontsize=10)\n",
        "            axes[i].set_ylabel(yaxes,fontsize=10)\n",
        "            axes[i].title.set_text('F1 score of '+ type + str(size[i]))\n",
        "            positions = range(8)\n",
        "            axes[i].yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
        "            axes[i].yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
        "            for ytick in axes[i].get_yticklabels():\n",
        "                ytick.set_fontsize(4)\n",
        "                #ytick.set_rotation(45)\n",
        "            xlabels = self.datasize\n",
        "            axes[i].xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
        "            axes[i].xaxis.set_major_formatter(ticker.FixedFormatter(xlabels))\n",
        "            for xtick in axes[i].get_xticklabels():\n",
        "                xtick.set_fontsize(4)\n",
        "                #xtick.set_rotation(45)\n",
        "\n",
        "        fig.colorbar(c, ax=axes.ravel().tolist())\n",
        "        plt.savefig(self.File_PATH_Heatmaps +'Heatmap_ '+self.filename+'_'+type+'.pdf', bbox_inches='tight')\n",
        "\n",
        "    @staticmethod\n",
        "    def Plots_ANCPOP(self):\n",
        "        self.pro_rang = np.arange(self.start, self.stop, self.step)\n",
        "        self.obs_rang = np.arange(self.start, self.stop, self.step)\n",
        "        max_num = np.max([self.nodes_num, self.edges_num])\n",
        "        z = [[[] for i in range(max_num)]for j in range(2)]\n",
        "        # z[1][2] [[[],[],[]],[[],[],[]]]\n",
        "        pro_rang = self.pro_rang\n",
        "        obs_rang = self.obs_rang\n",
        "        fig, axes = plt.subplots(nrows=2, ncols=len(z[0]),figsize=(20,8))\n",
        "        for j in range(2):\n",
        "            if j == 0:\n",
        "                type = 'Nodes'\n",
        "                size = self.nodes\n",
        "                labels = self.edges\n",
        "                yaxes = 'Number of Edges'\n",
        "            else:\n",
        "                type = 'Edges'\n",
        "                size = self.edges\n",
        "                labels = self.nodes\n",
        "                yaxes = 'Number of Variables'\n",
        "            for i in range(max_num):\n",
        "                read_path = self.File_PATH_Base + 'Results/Summary_'+ type +'_'+str(size[i]) +'.csv'\n",
        "                zz = pd.read_csv(read_path,header=0,index_col=0)\n",
        "                z[j][i] = zz.to_numpy()\n",
        "            z_min=np.min(z[j])\n",
        "            z_max=np.max(z[j])\n",
        "            for i in range(max_num):\n",
        "                c = axes[j][i].imshow(z[j][i], cmap =plt.cm.bone_r, vmin = z_min, vmax = z_max, interpolation ='nearest', origin ='upper')\n",
        "                axes[j][i].set_xlabel('length of time windows '+r'$T$',fontsize=10)\n",
        "                axes[j][i].set_ylabel(yaxes,fontsize=10)\n",
        "                axes[j][i].title.set_text('F1 score of '+ type + str(size[i]))\n",
        "                positions = range(8)\n",
        "                axes[j][i].yaxis.set_major_locator(ticker.FixedLocator(positions))\n",
        "                axes[j][i].yaxis.set_major_formatter(ticker.FixedFormatter(labels))\n",
        "                for ytick in axes[j][i].get_yticklabels():\n",
        "                    ytick.set_fontsize(4)\n",
        "                    #ytick.set_rotation(45)\n",
        "                xlabels = self.datasize\n",
        "                axes[j][i].xaxis.set_major_locator(ticker.FixedLocator(positions))\n",
        "                axes[j][i].xaxis.set_major_formatter(ticker.FixedFormatter(xlabels))\n",
        "                for xtick in axes[j][i].get_xticklabels():\n",
        "                    xtick.set_fontsize(4)\n",
        "                    #xtick.set_rotation(45)\n",
        "        title = 'Performance of '+ self.printname\n",
        "        fig.colorbar(c, ax=axes.ravel().tolist())\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "        #plt.title(title, fontdict=None, loc='center', pad=None)\n",
        "        plt.savefig(self.File_PATH_Heatmaps +'Heatmap_ '+self.filename+'.pdf', bbox_inches='tight')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ######################################## SETTING GAUSS_TYPE, self.File_PATH ######################################\n",
        "    ########################### SETTING GAUSS_TYPE, self.File_PATH_Base, nodes, edges, and datasize ##########################\n",
        "    #File_PATH = '/content/drive/MyDrive/Colab Notebooks/Causality_NotesTest/Test_Causality_Datasets/Real_data/Telephone/'\n",
        "    File_PATH = '/content/drive/MyDrive/Colab Notebooks/Causality_NotesTest/Test_Causality_Datasets/Synthetic datasets/'\n",
        "\n",
        "    start = 5\n",
        "    stop = 40\n",
        "    step = 5\n",
        "    file_name = 'linearGauss_6_15.npz'\n",
        "    result = ANM_NCPOP('Real', 0 , File_PATH, start, stop, step)\n",
        "    result.Ancpop_Test()"
      ],
      "id": "245i_O68pkoJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUIpPPKyzoRI"
      },
      "source": [
        "# ANM_NCPOP_Realdata\n",
        "* Make sure there is only one table in file"
      ],
      "id": "XUIpPPKyzoRI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6zeQv8trqgD"
      },
      "source": [
        "# Test data"
      ],
      "id": "w6zeQv8trqgD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "5gfOYcIqiLOW",
        "outputId": "bfd6fd15-c9db-491a-ef3f-21425d52af8a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-857e94b64963>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mRaw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRaw_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#22589 rows × 54 columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mRaw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3778\u001b[0m         \u001b[0;31m# Do we have a slicer (on rows)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3779\u001b[0;31m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_index_sliceable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3781\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mconvert_to_index_sliceable\u001b[0;34m(obj, key)\u001b[0m\n\u001b[1;32m   2492\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2494\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_convert_slice_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;31m# We always treat __getitem__ slicing as label-based\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;31m# translate to locations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_indexer\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   6557\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deprecated_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kind\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"slice_indexer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6559\u001b[0;31m         \u001b[0mstart_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6561\u001b[0m         \u001b[0;31m# return a slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_locs\u001b[0;34m(self, start, end, step, kind)\u001b[0m\n\u001b[1;32m   6771\u001b[0m         \u001b[0mend_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6773\u001b[0;31m             \u001b[0mend_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"right\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6774\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend_slice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6775\u001b[0m             \u001b[0mend_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_slice_bound\u001b[0;34m(self, label, side, kind)\u001b[0m\n\u001b[1;32m   6692\u001b[0m             \u001b[0mslc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_booleans_to_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"u1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6693\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6694\u001b[0;31m                 raise KeyError(\n\u001b[0m\u001b[1;32m   6695\u001b[0m                     \u001b[0;34mf\"Cannot get {side} slice bound for non-unique \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6696\u001b[0m                     \u001b[0;34mf\"label: {repr(original_label)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Cannot get right slice bound for non-unique label: 10'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "File_PATH = '/content/drive/MyDrive/Colab Notebooks/Causality_NotesTest/Test_Causality_Datasets/Real_data/Telephone/'\n",
        "Raw_data_path = File_PATH+'real_dataset_processed.csv'\n",
        "Raw_dag = File_PATH+'true_graph.csv'\n",
        "\n",
        "Raw_data = pd.read_csv(Raw_data_path, header=0, index_col=0)\n",
        "#22589 rows × 54 columns\n",
        "Raw_data[:10]"
      ],
      "id": "5gfOYcIqiLOW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "990afb68"
      },
      "outputs": [],
      "source": [],
      "id": "990afb68"
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
